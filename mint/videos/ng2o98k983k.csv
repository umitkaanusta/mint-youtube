,video_id,commenter_channel_url,commenter_channel_id,commenter_channel_display_name,comment_id,comment_like_count,comment_publish_date,text,commenter_rating,comment_parent_id,collection_date,reply_count
0,ng2o98k983k,http://www.youtube.com/channel/UCTxJBm8hH9t_YAr4DGH-JjA,UCTxJBm8hH9t_YAr4DGH-JjA,Preeti Arawal,Ugz-5fxN9aqDA2c0g5x4AaABAg,0,1594094053.0,"Hi All,

I am using pycharm to run python code,when I installed requests,bs4,beautifulsoup4 in my windows system it installed successfully,But when I import it is giving error.Please let me know  how can I resolve the issue.",none,,2020-07-07 23:38:13.039144,0.0
1,ng2o98k983k,http://www.youtube.com/channel/UC2vlcuYgS7MUOPPGhDrH-uA,UC2vlcuYgS7MUOPPGhDrH-uA,Allister Graham,UgxLv_Ejv0vB4iq7IKh4AaABAg,1,1594052859.0,"Very informative video, you're an excellent teacher! While other YouTubers ramble off topic, you get straight to the point, and I really appreciate that.",none,,2020-07-07 23:38:13.039144,0.0
2,ng2o98k983k,http://www.youtube.com/channel/UC76J82vv9gqlNdN-Yv_jfnQ,UC76J82vv9gqlNdN-Yv_jfnQ,Abdelmalek Benkouider,Ugx4vxv7StX4WcT7kVV4AaABAg,1,1593983787.0,"thank you so much, for me you're the BEST python teacher. I really enjoy watching your videos and learn complex staff in very easy way!",none,,2020-07-07 23:38:13.039144,0.0
3,ng2o98k983k,http://www.youtube.com/channel/UCap5SXw3jPXMHgiW6dlVQEA,UCap5SXw3jPXMHgiW6dlVQEA,Aman Chauhan,Ugw4cTe0RI4s_h9u0zR4AaABAg,0,1593850003.0,Guys! Make sure files are stored in ANSI.,none,,2020-07-07 23:38:13.039144,0.0
4,ng2o98k983k,http://www.youtube.com/channel/UCUTtf2387S4bKu4Oh_QMEXg,UCUTtf2387S4bKu4Oh_QMEXg,adarsh tiwari,UgwHfvLJUaHa4_DGGMt4AaABAg,1,1593772170.0,I truly appreciate your compassionate teaching and dedication to the tutorials. Thank you for all the teaching!!,none,,2020-07-07 23:38:13.039144,0.0
5,ng2o98k983k,http://www.youtube.com/channel/UC6pJbgLdqpo00F3MnA-5btg,UC6pJbgLdqpo00F3MnA-5btg,Bùi Thanh Lâm,UgzdpTytRdKCk0P1aEt4AaABAg,1,1593741972.0,Thanks! This helps me a lot!,none,,2020-07-07 23:38:13.039144,0.0
6,ng2o98k983k,http://www.youtube.com/channel/UCfafYyXCSqeycN8uKSa_f_g,UCfafYyXCSqeycN8uKSa_f_g,String Guo,Ugzav6tXjuP5FnWblZN4AaABAg,0,1593609491.0,This video really builds up my confidence in programming!,none,,2020-07-07 23:38:13.040131,0.0
7,ng2o98k983k,http://www.youtube.com/channel/UC63c-b4_9DmJAGrC51XYzog,UC63c-b4_9DmJAGrC51XYzog,TeachingComputing,Ugzvi3Ib-cS9n6tj8V54AaABAg,0,1593540767.0,"@Corey - but how do you then display this dynamically on a webpage? As in, what are the suggested ways to access that CSV file (dynamically) and display the data on your own webpage?",none,,2020-07-07 23:38:13.040131,0.0
8,ng2o98k983k,http://www.youtube.com/channel/UCDiVYERGY0WwrS8od1TQVYA,UCDiVYERGY0WwrS8od1TQVYA,Jackson Sophat,UgwAhC4NoAeMiX83Nc14AaABAg,0,1593486322.0,Thank you for sharing,none,,2020-07-07 23:38:13.040131,0.0
9,ng2o98k983k,http://www.youtube.com/channel/UCpg8eExde6_4nU2H7813lEA,UCpg8eExde6_4nU2H7813lEA,Melisa Liu,UgyjQsgw9nh0_QeNsFF4AaABAg,1,1593325847.0,La combinación de BeautifulSoup y Octoparse https://www.octoparse.es/  es el mejor posicionamiento preciso y la extracción rápida de grandes cantidades de datos web.,none,,2020-07-07 23:38:13.040131,0.0
10,ng2o98k983k,http://www.youtube.com/channel/UC9rsgD6ySq0tjxiRKmTmwkA,UC9rsgD6ySq0tjxiRKmTmwkA,zafer bağdu,Ugx82hFE1AQClsOi5Ut4AaABAg,0,1593259269.0,hi i am writing  class name  corectly and result is none what is the problem,none,,2020-07-07 23:38:13.040131,0.0
11,ng2o98k983k,http://www.youtube.com/channel/UCcmY6W3yGJQGUNwB8WWrB-Q,UCcmY6W3yGJQGUNwB8WWrB-Q,Daniel Worgan,UgwSPlxKcBSu-TGDbh14AaABAg,0,1593160641.0,Thanks again. I think a Selenium tutorial would be useful as well!,none,,2020-07-07 23:38:13.040131,0.0
12,ng2o98k983k,http://www.youtube.com/channel/UCIPWcUyPCUcZPCYXMLhd21w,UCIPWcUyPCUcZPCYXMLhd21w,Alieu Badjie,UgzmrVS-XLcpxwPi3iF4AaABAg,0,1593087697.0,I will love you to make a video of web scarping  for quotes.toscrape.com and books.toscrape.com using beautifulsoup4 and selenium and adding user menu for login if possible,none,,2020-07-07 23:38:13.040131,0.0
13,ng2o98k983k,http://www.youtube.com/channel/UCqBEgEO96w-J-Z0sYsvxx4w,UCqBEgEO96w-J-Z0sYsvxx4w,kinshuk vats,UgxPtu_odR-cNXHlWwF4AaABAg,0,1593016991.0,Amazing. The way you teach the most complex things in the most simplest way is commendable 👍🏻👍🏻👍🏻❤️❤️❤️,none,,2020-07-07 23:38:13.040131,0.0
14,ng2o98k983k,http://www.youtube.com/channel/UCXJKVuFoKrrY4uNULNqCwrQ,UCXJKVuFoKrrY4uNULNqCwrQ,Muzahidur Rahman,UgypC8M4TeQw-be-ElJ4AaABAg,0,1592957755.0,Thank you. Can't believe I have been building scrapers for months still got so much deep understanding from a single video.,none,,2020-07-07 23:38:13.040131,0.0
15,ng2o98k983k,http://www.youtube.com/channel/UCgzh4rn5KzRoohwAM5O4iMw,UCgzh4rn5KzRoohwAM5O4iMw,Howard Xian,UgzL82Fq4B_16JlPOCV4AaABAg,0,1592693122.0,"Please also create a tutorial on Scrapy, thanks!",none,,2020-07-07 23:38:13.040131,0.0
16,ng2o98k983k,http://www.youtube.com/channel/UCmrpylmqCVETvIoQz243wUA,UCmrpylmqCVETvIoQz243wUA,Shariq Ahmed,UgyxDkRnSHb7Q2DvUb14AaABAg,0,1592574820.0,One of the finest web scraping with BS4 tutorials on YouTube! Thank you so much sir!,none,,2020-07-07 23:38:13.040131,0.0
17,ng2o98k983k,http://www.youtube.com/channel/UCxdISuEBJ9Eg06sW8BuvbjA,UCxdISuEBJ9Eg06sW8BuvbjA,Ab Cd,UgyZID6R_jfr_x_L-wJ4AaABAg,0,1592538793.0,"great solution and tips bro!! thank you for your video., I will definitely try to improve my scraping skills with your tutor. but as non-tech user  Also I have found good alternative to scrape amazon reviews https://e-scraper.com/useful-articles/a-hassle-free-method-to-scrape-amazon-reviews/. maybe it helps to somebody too.",none,,2020-07-07 23:38:13.040131,0.0
18,ng2o98k983k,http://www.youtube.com/channel/UCE0NQpMq6J0OKRoN1MVw3GA,UCE0NQpMq6J0OKRoN1MVw3GA,Tyler Prometheus,Ugzy7BNtqScypxEQKhh4AaABAg,0,1592526073.0,"what are the key pairs to comment out and indent (manipulate) multiple lines at once while highlighting them?
I've tried every combo I can think of and I see it all the time but nobody ever says how they do it.
thanks.",none,,2020-07-07 23:38:13.040131,0.0
19,ng2o98k983k,http://www.youtube.com/channel/UCEqIeKfCjW4e4_S6rsG233g,UCEqIeKfCjW4e4_S6rsG233g,Jared Crenshaw,Ugx6eaNVWlM1B8VkFph4AaABAg,0,1592504009.0,"Phenomenal lesson, will be referencing later. Thank you so much!",none,,2020-07-07 23:38:13.040131,0.0
20,ng2o98k983k,http://www.youtube.com/channel/UCgvLZ8y2XD0son0-7tD_HwQ,UCgvLZ8y2XD0son0-7tD_HwQ,Rishabh Kumar,UgzyQltX62oiy7oh9cZ4AaABAg,0,1592466380.0,"His teaching is so good ,i had a orgasm in the mid video.(no homo).Joker aside Great video.",none,,2020-07-07 23:38:13.041130,0.0
21,ng2o98k983k,http://www.youtube.com/channel/UCx-paX8btcGTrzVKgcBnj2Q,UCx-paX8btcGTrzVKgcBnj2Q,Binx Coder,UgzrjSCPfSf0bOLbRgF4AaABAg,0,1592388239.0,Navigating with CSS selectors? BTW great content!,none,,2020-07-07 23:38:13.041271,0.0
22,ng2o98k983k,http://www.youtube.com/channel/UCL-3-zsC4xZud50guyUjQDg,UCL-3-zsC4xZud50guyUjQDg,TNT Captain,UgzMZrETAPoLi6BYpeN4AaABAg,1,1592316407.0,"This is how to teach in a proper way, in some videos a code is already given and they just read it out which makes it difficult to understand.
Thanks Corey, keep up the great work.",none,,2020-07-07 23:38:13.041271,0.0
23,ng2o98k983k,http://www.youtube.com/channel/UCIu6dlLnha1XHiESu3khyLg,UCIu6dlLnha1XHiESu3khyLg,Programmer Nikhil,UgxCzuaupY6nILQJq2R4AaABAg,1,1592119283.0,Thank you very much sir .,none,,2020-07-07 23:38:13.041271,0.0
24,ng2o98k983k,http://www.youtube.com/channel/UCMPMtB6vDXu650-u4VA0cjw,UCMPMtB6vDXu650-u4VA0cjw,Guru prasad,Ugwzvqj9KoTf-Efaq_J4AaABAg,1,1592077076.0,"Amazing video! the content and the way of explanation was very crisp and clear, i personally feel this is one of the best tutorial for Web scraping, thanks Corey.",none,,2020-07-07 23:38:13.041271,0.0
25,ng2o98k983k,http://www.youtube.com/channel/UCJ3FxqzhU3qEny-gwwxXhVg,UCJ3FxqzhU3qEny-gwwxXhVg,Ants,Ugx13bqwk-u-1KjmvBx4AaABAg,0,1592046379.0,Do data scientists use this?,none,,2020-07-07 23:38:13.041271,0.0
26,ng2o98k983k,http://www.youtube.com/channel/UCqlrAM4RubzxH3QAm9ZDZ5A,UCqlrAM4RubzxH3QAm9ZDZ5A,Ammar A Hasan,UgyV7a-B9sujpWejorx4AaABAg,1,1592030048.0,"Amazing, Thanks alot",none,,2020-07-07 23:38:13.041271,0.0
27,ng2o98k983k,http://www.youtube.com/channel/UCps7Z5AY-AgRzykl003Rqvw,UCps7Z5AY-AgRzykl003Rqvw,Rohan Pednekar,Ugz0N0PUX5Av7N8F0cd4AaABAg,1,1592020179.0,Thanks :),none,,2020-07-07 23:38:13.041271,0.0
28,ng2o98k983k,http://www.youtube.com/channel/UCe7hnDdXLYZ9O-PYkaUjDHw,UCe7hnDdXLYZ9O-PYkaUjDHw,Pangeran,Ugyj4NwvOahREDrMXyR4AaABAg,0,1591722706.0,"Hey please help, when i did the requests function, python did not show the html tags and classes but not the text inside them",none,,2020-07-07 23:38:13.041271,0.0
29,ng2o98k983k,http://www.youtube.com/channel/UC6581WT9-Xy9oHoljV_EOew,UC6581WT9-Xy9oHoljV_EOew,umar,Ugwv8f2hSJIT2uDX3Mp4AaABAg,0,1591676187.0,can anyone pls send me cheet sheet (notes) for this video?,none,,2020-07-07 23:38:13.041271,0.0
30,ng2o98k983k,http://www.youtube.com/channel/UCsyrhcpY615tAJGRkwIZ02Q,UCsyrhcpY615tAJGRkwIZ02Q,Jonatan Søgaard,UgzhB6twLhpzPFJCzzB4AaABAg,1,1591625092.0,"Wow, your Sublime doesn't say 'unregistered'! I don't think I have seen that in a video before...",none,,2020-07-07 23:38:13.041271,0.0
31,ng2o98k983k,http://www.youtube.com/channel/UCgDV6rF4HdQ7N65SQAvAhnw,UCgDV6rF4HdQ7N65SQAvAhnw,Amit Shukla,UgzxcCQyzvrklARWxOd4AaABAg,1,1591539488.0,Great video 🙌🙌,none,,2020-07-07 23:38:13.041271,0.0
32,ng2o98k983k,http://www.youtube.com/channel/UCnVrQSVlOMK7R5UOJpCpAiQ,UCnVrQSVlOMK7R5UOJpCpAiQ,Ujjawal Sharma,UgwEufvYmgSONARzHEh4AaABAg,1,1591491429.0,Thanks for such a great video ❤️,none,,2020-07-07 23:38:13.041271,0.0
33,ng2o98k983k,http://www.youtube.com/channel/UCPWVYKUJsSwM9aXoFpCkYyQ,UCPWVYKUJsSwM9aXoFpCkYyQ,akhil kn,UgyEBXg3eohK5MgrKr14AaABAg,0,1591266392.0,The one and only helpful video for beginners,none,,2020-07-07 23:38:13.042135,0.0
34,ng2o98k983k,http://www.youtube.com/channel/UCbswJS54a8QOx8Mh4dFKJsw,UCbswJS54a8QOx8Mh4dFKJsw,Dhairya Shah,UgyuBLfXjZK1bsWzFUd4AaABAg,0,1591256980.0,how can we read data from csv file and use some of it in my own html page??,none,,2020-07-07 23:38:13.042135,0.0
35,ng2o98k983k,http://www.youtube.com/channel/UC2-porxlUi9uh5vIzH4SqVw,UC2-porxlUi9uh5vIzH4SqVw,All The Mikeys Are Taken,UgzUTEom80OX6eFSyUR4AaABAg,0,1591158845.0,Can you cover the same topic using selenium plz?,none,,2020-07-07 23:38:13.042135,0.0
36,ng2o98k983k,http://www.youtube.com/channel/UC5xFcb4jjmZL0N6CWEiMoUw,UC5xFcb4jjmZL0N6CWEiMoUw,pratik joshi,UgxzCRuDBWTKTA9s1rB4AaABAg,0,1591111968.0,"I'm Getting 0 while printing the len function:
Here is my code
Thanks in advance
from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as soup
my_url = 'https://www.daraz.com.np/catalog/?_keyori=ss&from=input&page=3&q=headphones&spm=a2a0e.11779170.search.go.15bb2d2btaSWVj'
uClient = uReq(my_url)
page_html = uClient.read()
uClient.close()
page_soup = soup(page_html, ""html.parser"")
containers = page_soup.findAll(""div"", {""class"":""c2prKC""})
print(len(containers))",none,,2020-07-07 23:38:13.042135,0.0
37,ng2o98k983k,http://www.youtube.com/channel/UCNZb_b4TpNmfn_6Y9ulA-Ng,UCNZb_b4TpNmfn_6Y9ulA-Ng,rakesh rayudu,UgybrxfjeAA1DDOCJ9J4AaABAg,0,1590983900.0,"Hi Corey.. Thanks for the tutorials.. I'm not able to parse the text directly, I have to use encode method which is converting the text into byte code.. If I don't use encode, it is throwing UnicodeEncodeError: ascii code can't encode character '\u2013'.. can you please help me with this? I'm using python 3.6.9.. thanks..",none,,2020-07-07 23:38:13.042135,0.0
38,ng2o98k983k,http://www.youtube.com/channel/UCX-tWsKPmJPdP77vouuib1w,UCX-tWsKPmJPdP77vouuib1w,Ck Lam,UgxmgETkIcLp-LeyB_Z4AaABAg,0,1590932617.0,wonder how to deal with the error : 'cp950' codec can't encode character,none,,2020-07-07 23:38:13.042135,1.0
39,ng2o98k983k,http://www.youtube.com/channel/UCqR-_hTAksRM3v27U-OlQQg,UCqR-_hTAksRM3v27U-OlQQg,Chandra Morgan,Ugxcj3t2dDCApak_15p4AaABAg,0,1590824177.0,"great and awesome video, i wonder how to parse CSS elements from website for scraping?",none,,2020-07-07 23:38:13.042135,0.0
40,ng2o98k983k,http://www.youtube.com/channel/UC4E30od8C6RJRxC-PDEATBQ,UC4E30od8C6RJRxC-PDEATBQ,Mayar Mohsen,Ugz96iaNZNK_3_euy0V4AaABAg,1,1590750854.0,Easiest Tutorial on youtube thank you,none,,2020-07-07 23:38:13.042135,0.0
41,ng2o98k983k,http://www.youtube.com/channel/UCZIU4YpipaZ7XWZ7tm5-4Ug,UCZIU4YpipaZ7XWZ7tm5-4Ug,Divya Gurumoorthy,Ugy3eeS3wkjNnWkqe5F4AaABAg,0,1590480277.0,"Thank you so much sir :) This was so clearly explained and included some possibilities of error, which I actually did face, and you also told us how they could be solved as well.

I just have a question. In the 'find' method, only the first occurrence of that particular tag/class is retrieved. Is there a way to extract the 'nth' div/ nth occurrence of a particular class instead of using find all? Thanks a ton :)",none,,2020-07-07 23:38:13.042135,0.0
42,ng2o98k983k,http://www.youtube.com/channel/UC-Zmooe59zX3pfXWpFSYpEQ,UC-Zmooe59zX3pfXWpFSYpEQ,Vish,Ugzv6e1lQ43qF-cVN494AaABAg,0,1590412623.0,Thank you once more for the step-by-step real life examples. Can you please do a video for xml processing?,none,,2020-07-07 23:38:13.042135,0.0
43,ng2o98k983k,http://www.youtube.com/channel/UCGggrS0hT8fcBBhRbf6tasw,UCGggrS0hT8fcBBhRbf6tasw,Joshua Harvey,Ugznzqa8Z2SpWvrStBh4AaABAg,0,1590339217.0,"Great video, thank you Corey!",none,,2020-07-07 23:38:13.042135,0.0
44,ng2o98k983k,http://www.youtube.com/channel/UCbR62LpAMNjkJ6itQONv40w,UCbR62LpAMNjkJ6itQONv40w,Hamza el bouti,UgxXtC80U14y4DKAbGd4AaABAg,0,1590332157.0,"i cant  get the script from script tag """"""as string""""""",none,,2020-07-07 23:38:13.042135,0.0
45,ng2o98k983k,http://www.youtube.com/channel/UC1ECDJk4pkAJ0Q0G4QMoeSw,UC1ECDJk4pkAJ0Q0G4QMoeSw,Javed Ahmad,UgzOSdAWZMeT2wgU7jB4AaABAg,0,1590324023.0,Wonderful,none,,2020-07-07 23:38:13.042135,0.0
46,ng2o98k983k,http://www.youtube.com/channel/UC03vYw8G_BXSkA4ltQjUSOw,UC03vYw8G_BXSkA4ltQjUSOw,Joy Sharma,Ugy2FLZ-g7wV7nA1A_14AaABAg,0,1590301271.0,"Hey Corey, Please create some videos on numpy!",none,,2020-07-07 23:38:13.043131,0.0
47,ng2o98k983k,http://www.youtube.com/channel/UC9FdSSmNV7ttIRLMhS7kzEw,UC9FdSSmNV7ttIRLMhS7kzEw,Ajay K S,UgyoztxLbFMN_L0UPvB4AaABAg,1,1590230857.0,What an amazing tutorial!! Thank you Corey! :),none,,2020-07-07 23:38:13.043131,0.0
48,ng2o98k983k,http://www.youtube.com/channel/UChr8g3vryQHZ5aa1_jLsI_g,UChr8g3vryQHZ5aa1_jLsI_g,Abi D Vu,UgyBbKjfsv_Jr32arBJ4AaABAg,1,1590100742.0,So clear. Very easy to understand. Thank you.,none,,2020-07-07 23:38:13.043131,0.0
49,ng2o98k983k,http://www.youtube.com/channel/UCIY6ZTKcoUXtuoRsU613ZvA,UCIY6ZTKcoUXtuoRsU613ZvA,Naman kumar,UgyxcHOqy79GRnzRUMd4AaABAg,0,1590026890.0,"sir i  am getting an error 	 ImportError: cannot import name 'beautifulsoup' from 'bs4'   plzz help",none,,2020-07-07 23:38:13.043131,3.0
50,ng2o98k983k,http://www.youtube.com/channel/UCREPxaFjg0ea7e-KHrSX5ag,UCREPxaFjg0ea7e-KHrSX5ag,Connor Hayes,UgyA7nFF9eV_fziFdht4AaABAg,0,1589922986.0,"Note for Windows users: When using open() on a windows machine, you have to specify utf-8 encoding with open('file', encoding='utf-8'). Windows does not default to utf-8 and fails to encode some common bytes",none,,2020-07-07 23:38:13.043131,0.0
51,ng2o98k983k,http://www.youtube.com/channel/UCnhppW7PfYQRSjVSNwTuiAA,UCnhppW7PfYQRSjVSNwTuiAA,Henrique Junqueira,Ugw4OZpzfub3VqQ76fV4AaABAg,1,1589644366.0,"Corey, this is just THE BEST explanation about web scrapping with python. Just notice that you could explore a little bit more about the page response with the get method. Sometimes I have trouble trying to access one specific site, and I have to manage it in different forms. And some more detailed explanations about when it's possible and not possible to scrape data from websites.

Only this detail that I've missed. All the rest it's just perfect.",none,,2020-07-07 23:38:13.043131,0.0
52,ng2o98k983k,http://www.youtube.com/channel/UCC33cE0axinJEJfZoxEyXIw,UCC33cE0axinJEJfZoxEyXIw,Courtney,Ugyg6tbLBKtnYRwrxCt4AaABAg,0,1589627040.0,Python holy grail!,none,,2020-07-07 23:38:13.043131,0.0
53,ng2o98k983k,http://www.youtube.com/channel/UC53eVK3q6TxOrK6PloJB-xA,UC53eVK3q6TxOrK6PloJB-xA,Joost Wildenborg,UgwG7E5BeqSAyIvuPmN4AaABAg,1,1589579691.0,"Thanks, Corey. This was really helpful! Excellent video.",none,,2020-07-07 23:38:13.043131,0.0
54,ng2o98k983k,http://www.youtube.com/channel/UClRH--NY6qebbMDMkXzxOBQ,UClRH--NY6qebbMDMkXzxOBQ,Arpit Kaushal,UgwlELkSb3oofiluvPV4AaABAg,0,1589557069.0,"Getting these question marks ����  in place of  - ' "" _  when scraping the said website. Please help.",none,,2020-07-07 23:38:13.043131,0.0
55,ng2o98k983k,http://www.youtube.com/channel/UClRH--NY6qebbMDMkXzxOBQ,UClRH--NY6qebbMDMkXzxOBQ,Arpit Kaushal,Ugx-Fippd_Ega5TE8Q94AaABAg,0,1589556864.0,"Hi, thanks for the video! It was a good mini-project! :) 
I have an issue, I hope you (someone) can help out. 
 Getting � in place of ', '', - and some other characters. 
Here's my code - 
https://repl.it/talk/share/replacement-characterreplacement-characterreplacement-character-Characters-are-not-getting-printed-instead-getting-replacement-character/37693",none,,2020-07-07 23:38:13.043131,0.0
56,ng2o98k983k,http://www.youtube.com/channel/UChrWOdjEoiWSkrfHSFQEImA,UChrWOdjEoiWSkrfHSFQEImA,Matthew Birkholtz,UgwqNmlmHVi_VIVSvJx4AaABAg,1,1589435558.0,VERY NICE!,none,,2020-07-07 23:38:13.043131,0.0
57,ng2o98k983k,http://www.youtube.com/channel/UCSEBKnzEMPHCC_BvhMG5q8g,UCSEBKnzEMPHCC_BvhMG5q8g,Thaung Than,Ugy3B8Yk-0h0Pc_eu6F4AaABAg,1,1589373403.0,"Very helpful! Thank you, Corey.",none,,2020-07-07 23:38:13.043131,0.0
58,ng2o98k983k,http://www.youtube.com/channel/UCVWO1H6p6woxlNtT8zx1YJw,UCVWO1H6p6woxlNtT8zx1YJw,Gwanghyeon Gim,Ugwwk0NWHOl9HgNPkTB4AaABAg,1,1589239518.0,Thank you Corey. I can personally assure that your work doesn't go for nothing. I hope I can donate more to your patreon in the future.,none,,2020-07-07 23:38:13.043131,0.0
59,ng2o98k983k,http://www.youtube.com/channel/UCDI0q1918OiT8JaKSBvX-Dg,UCDI0q1918OiT8JaKSBvX-Dg,Jason Ahn,UgzIGZhDpabhQiJTxZ54AaABAg,0,1589173198.0,very helpful,none,,2020-07-07 23:38:13.044131,0.0
60,ng2o98k983k,http://www.youtube.com/channel/UC_kFGUICUgspn1N3lyhTO0Q,UC_kFGUICUgspn1N3lyhTO0Q,Solar System Sports Pridiction,UgwdH7oFDsi40a3Iy714AaABAg,0,1589110248.0,"how to kicked sports data, by time and loop calculation help me out",none,,2020-07-07 23:38:13.044131,0.0
61,ng2o98k983k,http://www.youtube.com/channel/UC4cUSBEvTXmgx6CiV_AaN_Q,UC4cUSBEvTXmgx6CiV_AaN_Q,Boris,UgzkJ7b0nCwR7CnYtZR4AaABAg,0,1589103411.0,"Whoever gets weird symbols when opening the .csv in *Excel*, change this line:
csv_file = open('cms_scrape.csv', 'w', *encoding='utf-8-sig'*)
Thanks once again Corey for brilliant content 💎",none,,2020-07-07 23:38:13.044131,0.0
62,ng2o98k983k,http://www.youtube.com/channel/UCE4SO4weAe7wbchTVN8VbCA,UCE4SO4weAe7wbchTVN8VbCA,Ll Wk,UgyAjUlBKNT0UfqEXPF4AaABAg,1,1589092644.0,This was really clear and easy to understand. Thanks!,none,,2020-07-07 23:38:13.044131,0.0
63,ng2o98k983k,http://www.youtube.com/channel/UCWdI7HbHh8W2PcxO-T3xpdg,UCWdI7HbHh8W2PcxO-T3xpdg,Xintong Liu,UgzpRKgrReDYuJ80rMR4AaABAg,0,1588986255.0,"Brief conclusion:
This video is some basic sytax for the BeautifulSoup.
1. How to get access to first element.
2. How to get access to the specific ones. 
3. How to parse by using split.
4. Deal with missing information.
5. Create and write csv file.",none,,2020-07-07 23:38:13.044131,0.0
64,ng2o98k983k,http://www.youtube.com/channel/UCjI42jCGFRbfGpXnQYkDy4w,UCjI42jCGFRbfGpXnQYkDy4w,Lucas Toccheton,UgyjjKCF-LSD0ec0E0x4AaABAg,1,1588956558.0,Absolutely the best. Didn't need anything else for my first web crawler!!! Thanks you!,none,,2020-07-07 23:38:13.044131,0.0
65,ng2o98k983k,http://www.youtube.com/channel/UCBXxDYSLRkCvSUiib53-bVg,UCBXxDYSLRkCvSUiib53-bVg,indratej reddy,UgydN4tvvbqTsSSUKK54AaABAg,0,1588954238.0,sir how to do xml parsing ?,none,,2020-07-07 23:38:13.044131,0.0
66,ng2o98k983k,http://www.youtube.com/channel/UCeIUHEXnk-0jAsVTKwpjBwg,UCeIUHEXnk-0jAsVTKwpjBwg,RAJ MANI,Ugzm4n3GFhfP2doSs-14AaABAg,0,1588853641.0,Hey ...Can there be a python code where i would like to append a html file into an another html file into a particular div,none,,2020-07-07 23:38:13.044131,0.0
67,ng2o98k983k,http://www.youtube.com/channel/UCB_JHgwALfOXIwFwPqw6hdQ,UCB_JHgwALfOXIwFwPqw6hdQ,Lucas Pinsdorf,UgyxTRLmC1talHPjsft4AaABAg,0,1588696543.0,"Hello! Perfect video! Would just like to know if you have some tips to getting the same kind of information (same structure of the data) from different url's of the same website. The url's are: ""www.yyyy.com/id=1"" I just have to change the 1 to 50.000. Thank you!",none,,2020-07-07 23:38:13.044131,0.0
68,ng2o98k983k,http://www.youtube.com/channel/UC4rT5SbxXahMC_dZpO2nYew,UC4rT5SbxXahMC_dZpO2nYew,Abdullah Al Nibir,UgwCjVfc2VYEMpVKWQt4AaABAg,0,1588664509.0,"10: 32 when I run i get,
attributeerror: module 'html5lib.treebuilders' has no attribute '_base'
Can anyone help?",none,,2020-07-07 23:38:13.044131,1.0
69,ng2o98k983k,http://www.youtube.com/channel/UCuPzc7vKd1aA4NMU89SEdHQ,UCuPzc7vKd1aA4NMU89SEdHQ,Raj Tiwari,UgzFKTlDNoZek4Z7zud4AaABAg,0,1588454574.0,thanks a ton...,none,,2020-07-07 23:38:13.044131,0.0
70,ng2o98k983k,http://www.youtube.com/channel/UCk-i1yj2zbhpOvMs1kDCzoA,UCk-i1yj2zbhpOvMs1kDCzoA,J L,Ugx_fGHChaWjdEgmDiZ4AaABAg,0,1588247098.0,I was looking for 20:14 at the beginning damn man hahah but still nice video,none,,2020-07-07 23:38:13.044131,0.0
71,ng2o98k983k,http://www.youtube.com/channel/UCuIGkqt1bQ7iiZnZrgzzwMw,UCuIGkqt1bQ7iiZnZrgzzwMw,Farhad Hossain,UgxJcxh4VwyJEJxOy_Z4AaABAg,0,1588055444.0,"Scraper API handles proxies, browsers, and CAPTCHAs, so you can get the HTML from any web page with a simple API call!
Scraper API offers 1000 free API calls without Any credit card required.
One of the most frustrating parts of automated web scraping is constantly dealing with IP blocks and CAPTCHAs. Scraper API rotates IP addresses with each request, from a pool of millions of proxies across over a dozen ISPs, and automatically retries failed requests, so you will never be blocked. Scraper API also handles CAPTCHAs for you, so you can concentrate on turning websites into actionable data.
If you wish to try Scraper API here is a 10% coupon for you to give it a try — apiscraping10",none,,2020-07-07 23:38:13.044131,0.0
72,ng2o98k983k,http://www.youtube.com/channel/UCGb9zl_bXHY98VmOTJqLs2A,UCGb9zl_bXHY98VmOTJqLs2A,Jingyi Wang,UgxHni7rCU8Wt82gnXN4AaABAg,0,1588000959.0,"Hi Corey, I have a question about grabbing the website URL. Why can't we just use ""src"" as our video link? As the one you get at 29:43 ? If I click what we get from vid_src, it will be directed to the video page. In this case, why do we even need to get the video ID and then create the link? Thank you so much. Have a great week!",none,,2020-07-07 23:38:13.044131,1.0
73,ng2o98k983k,http://www.youtube.com/channel/UCVEtRO7k39cJdsIeYL7u8IQ,UCVEtRO7k39cJdsIeYL7u8IQ,I FLY English,Ugzov0RtChwTvGKeyzF4AaABAg,0,1587961446.0,Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you Thank you,none,,2020-07-07 23:38:13.045126,0.0
74,ng2o98k983k,http://www.youtube.com/channel/UC6JEXqMI9E-wubMAJI8LDtQ,UC6JEXqMI9E-wubMAJI8LDtQ,Jorge Álvarez,UgxbEw_27_-akj5GFXB4AaABAg,0,1587905177.0,"Great tutorial, thanks man",none,,2020-07-07 23:38:13.045126,0.0
75,ng2o98k983k,http://www.youtube.com/channel/UCFdjZUZf38xNewBsOgVAdnA,UCFdjZUZf38xNewBsOgVAdnA,Kevin Patel,UgyaRz_qof-mBP8d4Tl4AaABAg,0,1587883114.0,Which IDE Corey is using? that gives output in same as coding window,none,,2020-07-07 23:38:13.045126,0.0
76,ng2o98k983k,http://www.youtube.com/channel/UCT11Z6a3L_DpLTvp7Xf46gQ,UCT11Z6a3L_DpLTvp7Xf46gQ,Augustin Joseph,UgzDruaS3GlFpimJe2R4AaABAg,0,1587804523.0,"Hello,sir I am augustin. When I tried to scrape my own website (http://augustinblogger.epizy.com/) I got an error stating ""enable JavaScript in your browser"" I tried to scrape on pycharm, QPython, Pydroid, Termux. I also referred internet but I could not find a solution that satisfies my problem. Please help me out sir.",none,,2020-07-07 23:38:13.045126,0.0
77,ng2o98k983k,http://www.youtube.com/channel/UCHZEhBVeRtb8R6cqo9pCCKA,UCHZEhBVeRtb8R6cqo9pCCKA,Hobby Programmers,UgxbCt6cdq9ffsrCCeZ4AaABAg,4,1587778008.0,"No where I have seen advanced topics explained in an easy way like this. Even in paid courses
THANKS Mr.Corey. THANKS A LOT!

I'd learnt much from your videos in a easy way.",none,,2020-07-07 23:38:13.045126,0.0
78,ng2o98k983k,http://www.youtube.com/channel/UCcko9S3eeJUTCR3CMo0HnyQ,UCcko9S3eeJUTCR3CMo0HnyQ,Vinay Agarwal,UgzKGDlX2bJUd4VAVBl4AaABAg,0,1587774953.0,"Hey Corey, could you please make a video about dynamic web scraping using selenium. I was having trouble with a website that changes the position of buttons after some clicks. I love your in-depth explanation about topics, so i know your video will clear all my doubts. I searched for a solution everywhere, i couldn't find anything helpful.",none,,2020-07-07 23:38:13.045126,0.0
79,ng2o98k983k,http://www.youtube.com/channel/UCUlswbVqCYsNr57_kLVsy0Q,UCUlswbVqCYsNr57_kLVsy0Q,abdulhamid khorajiya,UgxnvVnViOAb1c_Ma8B4AaABAg,1,1587745016.0,excellent 👍,none,,2020-07-07 23:38:13.045126,0.0
80,ng2o98k983k,http://www.youtube.com/channel/UCp9c5Bgs_iSMLkB69baolQg,UCp9c5Bgs_iSMLkB69baolQg,Sohan Soharab,UgzeOo9zSnl5UlhixbV4AaABAg,0,1587458602.0,How do I catch dynamically generated source attribute from image tag?,none,,2020-07-07 23:38:13.045126,0.0
81,ng2o98k983k,http://www.youtube.com/channel/UCsfvLpnvn3S7hZ3ZuuUTOCA,UCsfvLpnvn3S7hZ3ZuuUTOCA,Martin Kaspar,UgzQD-jOQZ7qhfUiMfN4AaABAg,0,1587413783.0,awesome - this is just awesome - many thanks for this great rundown in BeautifuSoup. it is just great: keep up the superb job!,none,,2020-07-07 23:38:13.045126,0.0
82,ng2o98k983k,http://www.youtube.com/channel/UCvaL29TkKFqkPQJSjGeedzA,UCvaL29TkKFqkPQJSjGeedzA,Mike Celia,UgxUJYi932oqnxMXHmp4AaABAg,1,1587249761.0,Great video,none,,2020-07-07 23:38:13.045126,0.0
83,ng2o98k983k,http://www.youtube.com/channel/UCkB5TygDs7J35DJvCvcLETQ,UCkB5TygDs7J35DJvCvcLETQ,Abdulmalik Giwa,Ugy0tb_TJMOLvZ1EKBJ4AaABAg,1,1587219649.0,"This is a very good and explanatory video Corey, been having issues understanding beautifulsoup but this just broke it down perfectly. I was referred here from coursera, great one 👍🏽",none,,2020-07-07 23:38:13.045126,0.0
84,ng2o98k983k,http://www.youtube.com/channel/UCfIiaKhpXv_5WyzBtz77Dlg,UCfIiaKhpXv_5WyzBtz77Dlg,Nevil Holmes,Ugzz_qKVP_WKoVW7r1Z4AaABAg,1,1587219214.0,thanks,none,,2020-07-07 23:38:13.045126,0.0
85,ng2o98k983k,http://www.youtube.com/channel/UCH2TeIvnfLJ3-Q_kaBujCRg,UCH2TeIvnfLJ3-Q_kaBujCRg,Александр Милко,UgwwYtfPRm4NGK6Wx9B4AaABAg,1,1587214612.0,"Corey, thank you so much for such amazing videos!",none,,2020-07-07 23:38:13.045126,0.0
86,ng2o98k983k,http://www.youtube.com/channel/UC16w0LwoNTItiqaRA792htA,UC16w0LwoNTItiqaRA792htA,hyboui CHANG,UgyW16EjMblDlJOibbx4AaABAg,1,1587214509.0,"Thnaks a lot, very clear and good to discover BeautifulSoup !",none,,2020-07-07 23:38:13.046123,0.0
87,ng2o98k983k,http://www.youtube.com/channel/UChMnvzk5HoQTJat6xGQlcsg,UChMnvzk5HoQTJat6xGQlcsg,Johan Jim,UgxceTv0SIoa8-U_HxB4AaABAg,0,1587195448.0,*_hello_*,none,,2020-07-07 23:38:13.046123,0.0
88,ng2o98k983k,http://www.youtube.com/channel/UCmNZCO_IPl6z-ShPUSEvJKA,UCmNZCO_IPl6z-ShPUSEvJKA,albert walker,UgyS7v2tkI71imezcP54AaABAg,1,1587139405.0,"Excellent, I found this video after trying to follow some other vids with continual syntax error. I haven't touched Python for several years (retired) and found this video an excellent guide to getting reacquainted
Thanks for your efforts. I have subscribed to your channel.",none,,2020-07-07 23:38:13.046123,0.0
89,ng2o98k983k,http://www.youtube.com/channel/UCSNTKyrdSlWItX9ND3x2OoA,UCSNTKyrdSlWItX9ND3x2OoA,Samik Mehta,UgzYE6RihNgcAsWAihF4AaABAg,0,1587109916.0,"When i tried to parse instagram page, i was unable to parse that page using above tools. Can you please help me out?",none,,2020-07-07 23:38:13.046123,0.0
90,ng2o98k983k,http://www.youtube.com/channel/UC-6GUqqY6UVew_atclFjKzA,UC-6GUqqY6UVew_atclFjKzA,TARUN SUKHPALANI,Ugz5lpORqGOP4g2SnkR4AaABAg,3,1587087867.0,"Master Corey, you are a magician. I just mesmerized.",none,,2020-07-07 23:38:13.046123,0.0
91,ng2o98k983k,http://www.youtube.com/channel/UCkE6KE3owczOhKTFoGhTFKw,UCkE6KE3owczOhKTFoGhTFKw,Baibx :U,UgzcN8PmdZr7S9ai52J4AaABAg,1,1586907823.0,This. ! Video!,none,,2020-07-07 23:38:13.046123,0.0
92,ng2o98k983k,http://www.youtube.com/channel/UC51AsmBMUuJHm6B31nh822g,UC51AsmBMUuJHm6B31nh822g,G Dunken,UgySd4E25fCpkSN6f1t4AaABAg,2,1586837678.0,Thanks a lot Corey. It tremendously helped me to understand BeautifulSoup and quickly use it for a small web scrapper script that I needed. This 45min learning was so value adding for me.,none,,2020-07-07 23:38:13.046123,0.0
93,ng2o98k983k,http://www.youtube.com/channel/UCT6qL5CGjDS0wMpS1HC4AUg,UCT6qL5CGjDS0wMpS1HC4AUg,Ita Lutfiana,Ugx_tzjRo8OIXzEWUyN4AaABAg,1,1586749999.0,Well done. What a fantastic tutorial video and fantastic YouTube Channel. Thank you so much for your crystal clear explanation.,none,,2020-07-07 23:38:13.046123,1.0
94,ng2o98k983k,http://www.youtube.com/channel/UCkvLuNQbCNPL9lSSuFDSrcw,UCkvLuNQbCNPL9lSSuFDSrcw,Fu Brian,UgzjWY2Iu2NEke2DPC14AaABAg,0,1586735139.0,"Should rename this video to: Python Tutorial: Web Scraping with BeautifulSoup, Requests and HTML",none,,2020-07-07 23:38:13.046123,0.0
95,ng2o98k983k,http://www.youtube.com/channel/UCxF3WGM_xTMwg5waQ4y8a9w,UCxF3WGM_xTMwg5waQ4y8a9w,Liang Jian Zhang,UgzT_GWU2YwhfVJR8B94AaABAg,0,1586721472.0,"Hi Corey,
is it possible to show how to scrap a website with 'load more' button? 

I tried to with loop yet the href does not show the web page number",none,,2020-07-07 23:38:13.046123,0.0
96,ng2o98k983k,http://www.youtube.com/channel/UCQSZ-VF2O92_P2qYfeeNEgw,UCQSZ-VF2O92_P2qYfeeNEgw,charles stanton,UgzodVr11tcxr38CciV4AaABAg,1,1586642142.0,"Excellent video Corey, thanks.  I finally got to see it because we're all quarantined to our houses due to the virus.  Videos like this make it bearable.",none,,2020-07-07 23:38:13.046123,0.0
97,ng2o98k983k,http://www.youtube.com/channel/UCnKQNOny8iBC1cg-lTqznSA,UCnKQNOny8iBC1cg-lTqznSA,sai charan,Ugy6Oj7bqY2InJay_yl4AaABAg,0,1586575540.0,what exactly is parser ?,none,,2020-07-07 23:38:13.046123,0.0
98,ng2o98k983k,http://www.youtube.com/channel/UCQIkCQccICw9xx5zZPO7s8g,UCQIkCQccICw9xx5zZPO7s8g,mohamed abdo,UgxErbMbDfIyaNAx_r94AaABAg,1,1586520375.0,"I don't know why some people disliked this video? I paid a money to learn this and the content of the class was way bellow this level, thank you for the great Tutorial!",none,,2020-07-07 23:38:13.047063,0.0
99,ng2o98k983k,http://www.youtube.com/channel/UC3dMqhwFpdl6u75Ix4U1wXw,UC3dMqhwFpdl6u75Ix4U1wXw,Humayun Ahmad Rajib,UgzG_rvfrijrpUDkjyZ4AaABAg,1,1586449118.0,Thank you sir for great video.,none,,2020-07-07 23:38:13.047063,0.0
100,ng2o98k983k,http://www.youtube.com/channel/UCAkS-LIMsYwtPVy3uDk_2AA,UCAkS-LIMsYwtPVy3uDk_2AA,Joshua Kindhart,UgzERSk12ewtBoh3H4V4AaABAg,0,1586388607.0,"@coreyschafer How would I iterate through table data on finviz.com to scrape Top Gainers? When I try {""class"": ""table-light-cp-row""}, bs4 can't find it.",none,,2020-07-07 23:38:13.597391,0.0
101,ng2o98k983k,http://www.youtube.com/channel/UCJIKVQhPY5e9NgyQQJ_OCfg,UCJIKVQhPY5e9NgyQQJ_OCfg,Salman B,UgweFMBIscLsbSJgpBR4AaABAg,0,1586261267.0,"hey great video, just wanted to ask, my CSV doesn't automatically open up with a table and the height of all the rows are too big, is there any way to fix this? Thanks again",none,,2020-07-07 23:38:13.597391,0.0
102,ng2o98k983k,http://www.youtube.com/channel/UCsFhkS61viiyoVIcOFbCSAw,UCsFhkS61viiyoVIcOFbCSAw,Robert Fredrick Chestnutt,UgwQg-Rz4AL6OOmncN54AaABAg,1,1586201961.0,"Thank you, thank you again. If your ReGex tutorials were not amazing - this is fantastic too. I really appreciate that there are loads of youtube tutorials, also having done a number of Udemy and Datacamp courses - you have an unbelievable ability to communicate the material making it accessible",none,,2020-07-07 23:38:13.597391,0.0
103,ng2o98k983k,http://www.youtube.com/channel/UC-9muy_2nfQj-dhsgzGwl0A,UC-9muy_2nfQj-dhsgzGwl0A,MakerMotion,UgwP19lpruV9c7Yemn94AaABAg,0,1586190705.0,here is web scraping for dataset creation => https://www.youtube.com/watch?v=30dRI2nvGNE,none,,2020-07-07 23:38:13.597391,0.0
104,ng2o98k983k,http://www.youtube.com/channel/UCVgEl0jRFRg8zUiAqIi20Rw,UCVgEl0jRFRg8zUiAqIi20Rw,Neptune's Beard,UgwIiDJAbwG6W3EORuF4AaABAg,1,1586142023.0,good stuff,none,,2020-07-07 23:38:13.597391,0.0
105,ng2o98k983k,http://www.youtube.com/channel/UCwffl14rSln4AaWp4tY_3_A,UCwffl14rSln4AaWp4tY_3_A,Soumya Kumar,UgyHD5oMFer-szbjl214AaABAg,0,1586095449.0,why is it important to do the csv_file.close()?,none,,2020-07-07 23:38:13.597391,0.0
106,ng2o98k983k,http://www.youtube.com/channel/UCxhEfD5t-EIuihxp5mTti5g,UCxhEfD5t-EIuihxp5mTti5g,Proud Nepali,UgxQVX-90OlzFr5TpCR4AaABAg,0,1586060684.0,"Are you from Georgia, Sir?",none,,2020-07-07 23:38:13.597391,1.0
107,ng2o98k983k,http://www.youtube.com/channel/UCxhEfD5t-EIuihxp5mTti5g,UCxhEfD5t-EIuihxp5mTti5g,Proud Nepali,Ugx1sSuhjSzovkbKCSx4AaABAg,0,1585971243.0,10:00 why not import lxml?,none,,2020-07-07 23:38:13.597391,0.0
108,ng2o98k983k,http://www.youtube.com/channel/UCxd6qrJAPNmnLKqqqXvIa3A,UCxd6qrJAPNmnLKqqqXvIa3A,Meezan Malek,Ugz2YEVGVq_DWoSpiU94AaABAg,1,1585927179.0,thanks bro keep going,none,,2020-07-07 23:38:13.597391,0.0
109,ng2o98k983k,http://www.youtube.com/channel/UCOEG2r1ss5rBHlmEYiROCqQ,UCOEG2r1ss5rBHlmEYiROCqQ,Rabbid Lnk,Ugy-gnr3ORhwZcjHjUB4AaABAg,2,1585820123.0,One of the best tutorials Ive ever followed. Hopefully you help kickstart my career in data science!,none,,2020-07-07 23:38:13.597391,0.0
110,ng2o98k983k,http://www.youtube.com/channel/UCsWEUriTUM5NsdWrfKaXL-Q,UCsWEUriTUM5NsdWrfKaXL-Q,Ya Boi,Ugwa7jSmjWulHI3iGkJ4AaABAg,1,1585818712.0,"This is the only video I could find which explains this topic in a simple, understandable way. Thank you!",none,,2020-07-07 23:38:13.597391,0.0
111,ng2o98k983k,http://www.youtube.com/channel/UCocUss33kGtdJX2-VbRMU1Q,UCocUss33kGtdJX2-VbRMU1Q,Athira Krishnan,Ugw1seqSTb9JAx9Bs_B4AaABAg,1,1585740129.0,"Amazing content.


As a beginner in web scrapping . You content was informative and to the point.
Thank you.",none,,2020-07-07 23:38:13.597391,0.0
112,ng2o98k983k,http://www.youtube.com/channel/UC-IS0wu2r0GXy46zHbLkAMg,UC-IS0wu2r0GXy46zHbLkAMg,Ishan Pandey,UgzXkmeJ9jQWDnM_Ilt4AaABAg,0,1585651290.0,"Sir, when I scraped data in sublime I was getting question marks inside the box how can I resolve it?",none,,2020-07-07 23:38:13.597391,0.0
113,ng2o98k983k,http://www.youtube.com/channel/UCzvTKmy7mLrqzAKY7_iQEsg,UCzvTKmy7mLrqzAKY7_iQEsg,varun canamedi,UgxsWb195MrStNg58iR4AaABAg,0,1585581216.0,"i downloaded the html file on my laptop but when i pass it in the string, i get an error 'file not found'. what should i do?",none,,2020-07-07 23:38:13.598388,3.0
114,ng2o98k983k,http://www.youtube.com/channel/UCctHBBC6sceL4gD-A6Z1COQ,UCctHBBC6sceL4gD-A6Z1COQ,Bruno Barrios,UgzalsrjUuP2KHowWip4AaABAg,1,1585541670.0,This is so clean... omg! Best video!,none,,2020-07-07 23:38:13.598388,0.0
115,ng2o98k983k,http://www.youtube.com/channel/UC6GW938fiT8HBNxjIY2yoUA,UC6GW938fiT8HBNxjIY2yoUA,Toby Sullivan,Ugyb3T2nefOOH36Scnp4AaABAg,1,1585439324.0,lit!,none,,2020-07-07 23:38:13.598388,0.0
116,ng2o98k983k,http://www.youtube.com/channel/UC6f61SsLPdPmLs3mZIcvjWA,UC6f61SsLPdPmLs3mZIcvjWA,João Paulo,Ugw24v_ZVUQZBnrXO0V4AaABAg,0,1585375479.0,"Great video!
If you are a dotnet developer, there is a tool that helps in extracting information from HTML pages, it is called DotNetExpose. You can install using Package Manager. Follow the links:
Github: https://github.com/joao2391/DotNetExpose
Nuget: https://www.nuget.org/packages/DotNetExpose/

#webscraping #csharp #dotnet #webcrawler #HtmlAgilityPack #DotNetExpose #python #beautifulsoup #CoreySchafer",none,,2020-07-07 23:38:13.598388,0.0
117,ng2o98k983k,http://www.youtube.com/channel/UCociWN6of_sKR3xNwdZcWvg,UCociWN6of_sKR3xNwdZcWvg,Ldrago Gaming,UgzXH06UcrM-ynGuE3p4AaABAg,1,1585373575.0,"Nice video 
IBM staff suggested this link",none,,2020-07-07 23:38:13.598388,0.0
118,ng2o98k983k,http://www.youtube.com/channel/UCSwDvXwYQowwoMSgwSUgg9Q,UCSwDvXwYQowwoMSgwSUgg9Q,Jan Zaplatil,UgzyXE5woixnQPjzSPB4AaABAg,1,1585155857.0,"Hi great tutorial as always.  But i have i question: Is there a difference when scraping a XML SITE MAP. because when i use soup = BeautifulSoup(source, 'lxml') and use it to print out i dont get the source code but <loc> tags...how to scrape those ? thanks",none,,2020-07-07 23:38:13.598388,0.0
119,ng2o98k983k,http://www.youtube.com/channel/UCAON1AjVp2zBAfH6rB3Dp9A,UCAON1AjVp2zBAfH6rB3Dp9A,SabertoothElephant,UgzwgoX4s68mhTI74aB4AaABAg,1,1585005756.0,"GREAT video, thank you!",none,,2020-07-07 23:38:13.598388,0.0
120,ng2o98k983k,http://www.youtube.com/channel/UCe0P_UnTGcGmPhVZRgOfrtw,UCe0P_UnTGcGmPhVZRgOfrtw,Nilesh Suryavanshi,Ugyxo11iT-OmSPyhp-x4AaABAg,0,1584863270.0,"I am a fan of you, sir. Love the way you teach each topic.",none,,2020-07-07 23:38:13.598388,0.0
121,ng2o98k983k,http://www.youtube.com/channel/UCIlbSYeTpM_8ImWP-d4rTFw,UCIlbSYeTpM_8ImWP-d4rTFw,aVral,UgytzN2YzRGGn1t6Na14AaABAg,0,1584725124.0,Always the best videos are from you side...,none,,2020-07-07 23:38:13.598388,0.0
122,ng2o98k983k,http://www.youtube.com/channel/UCQ-aL9c66Edjom0h9KCxXag,UCQ-aL9c66Edjom0h9KCxXag,cheekygnome,UgyTHlm6-9pJMiZyKYx4AaABAg,0,1584665835.0,"To get these examples to work on Linux Mint I had to install the parsers with, for example, sudo apt install python3-lxml.  Pip appeared to be installing them, but I kept getting a bs4.FeatureNotFound error asking if I wanted to install a parser.  You can also ignore all of that and just use, for example, BeatifulSoup(html_file, ""html.parser"") to get the examples to work.",none,,2020-07-07 23:38:13.598388,0.0
123,ng2o98k983k,http://www.youtube.com/channel/UCHjSPh23sIOlCwueeJZa-yQ,UCHjSPh23sIOlCwueeJZa-yQ,Eric Moran,UgzLKhfxKX0ICpqr8FR4AaABAg,0,1584624937.0,"For anyone following along, toward the end, make sure you indent csv_writer.writerow. I forgot to do this and ended up with only 1 row returned in my csv.",none,,2020-07-07 23:38:13.598388,0.0
124,ng2o98k983k,http://www.youtube.com/channel/UCuKvIVPkZI4EmbAAy4haffw,UCuKvIVPkZI4EmbAAy4haffw,Eiza Stanford,UgwnynoGcFWjvL0Ime54AaABAg,0,1584567156.0,"Thank you so much for your video! I tried looking at tutorials online from numerous articles, but I couldn't truly wrap my head around BeautifulSoup; even after looking at my final pieces of code for long periods of time, I felt my way of scraping was very inefficient and ineffective.
Now, thanks to your tutorial, which has *so* much common sense by the way, I can now write readable code to scrape ebay listings and results!",none,,2020-07-07 23:38:13.598388,0.0
125,ng2o98k983k,http://www.youtube.com/channel/UCwGrWob00KgxP0Ted8oUpxg,UCwGrWob00KgxP0Ted8oUpxg,정Alex,UgzDoIRLTcHnEZp8WVR4AaABAg,0,1584500425.0,The best mentor in my life:),none,,2020-07-07 23:38:13.599395,0.0
126,ng2o98k983k,http://www.youtube.com/channel/UCPS0g6NWr3nLOGKoZmtgRHQ,UCPS0g6NWr3nLOGKoZmtgRHQ,devsaki,UgytHJsaPOXy9n7-2wN4AaABAg,0,1584444524.0,"Amazing video and very easy to follow. I landed here by recommendation by the instructor in the data science course https://www.coursera.org/learn/applied-data-science-capstone as a reference for scraping websites, which is a task in one of the assignments. Thank you very much.",none,,2020-07-07 23:38:13.599456,0.0
127,ng2o98k983k,http://www.youtube.com/channel/UCfucEfIuwQXXdpE-TCS_NuA,UCfucEfIuwQXXdpE-TCS_NuA,Amirhossein Shahabnia,UgzyYScQZc3L6eB5yHB4AaABAg,0,1584406097.0,"This is so great, thank you sir!",none,,2020-07-07 23:38:13.599456,0.0
128,ng2o98k983k,http://www.youtube.com/channel/UCg2jRJHY35HWsAN8PwGnVCA,UCg2jRJHY35HWsAN8PwGnVCA,Iain Munro,UgysNRYvB4dVhqVS30B4AaABAg,0,1584296171.0,"Hi Corey


Everything works with the exception of the scrape.py file creating the CSV file.


Any suggestions ?",none,,2020-07-07 23:38:13.599456,0.0
129,ng2o98k983k,http://www.youtube.com/channel/UClUJcyjCUsLo41CAxQo4rEw,UClUJcyjCUsLo41CAxQo4rEw,San Samman,UgzVnkMcu0Z922ltyLx4AaABAg,0,1584272530.0,You are a true man.,none,,2020-07-07 23:38:13.599456,0.0
130,ng2o98k983k,http://www.youtube.com/channel/UCktDHT7eYgV5MgRZ_JsDZtg,UCktDHT7eYgV5MgRZ_JsDZtg,Syed Humaid,Ugw0c87YNpvzngRc_bx4AaABAg,0,1584196481.0,"Hello,
Is it possible to extract a link as a clickable hyperlink as it appears on a web page?
Thank you",none,,2020-07-07 23:38:13.599456,0.0
131,ng2o98k983k,http://www.youtube.com/channel/UCagaHwei3ufhrP00j8VyOJg,UCagaHwei3ufhrP00j8VyOJg,Noor Ahmed Natali,UgyywKtNumoydTlLCMV4AaABAg,0,1583438543.0,How can we download the image,none,,2020-07-07 23:38:13.599456,0.0
132,ng2o98k983k,http://www.youtube.com/channel/UCeLzTaw-K6cspCtlFa31-2w,UCeLzTaw-K6cspCtlFa31-2w,‮ 7-4taC,Ugyq9PUodmuFKnfB9Nl4AaABAg,1,1583406557.0,19:36m,none,,2020-07-07 23:38:13.599456,0.0
133,ng2o98k983k,http://www.youtube.com/channel/UCYO2z52ohvSr7WSpVJ2AmGQ,UCYO2z52ohvSr7WSpVJ2AmGQ,Michal Jaffee,Ugw5xT5YwNfMK9-IKQ54AaABAg,0,1583397691.0,"great teacher, thank you!",none,,2020-07-07 23:38:13.599456,0.0
134,ng2o98k983k,http://www.youtube.com/channel/UCI5-kyJiwomMxF-vnRwKVxA,UCI5-kyJiwomMxF-vnRwKVxA,Rachel In,UgwpsbtMFIq-y15QeUl4AaABAg,0,1583289970.0,Thank you so much for making this video! I'm from China and I've searched many courses to learn Web Scraping. But it is you who make me successfully run my code.,none,,2020-07-07 23:38:13.599456,0.0
135,ng2o98k983k,http://www.youtube.com/channel/UCULtKxRHHorzn7D_zNWHolw,UCULtKxRHHorzn7D_zNWHolw,Lamerak,UgwvfaYuQ2Sy0h0TA2R4AaABAg,0,1583241583.0,"What is the practical use of web scraping? It seems very interesting to learn, but what is it used for?",none,,2020-07-07 23:38:13.599456,0.0
136,ng2o98k983k,http://www.youtube.com/channel/UCaxvDUwqFSV0w3wsNetyZrg,UCaxvDUwqFSV0w3wsNetyZrg,Sonam Rinzin,Ugz7RTH1_eEqVE8JfGR4AaABAg,0,1582772603.0,Is there any means to download sample html file for this tutorial?,none,,2020-07-07 23:38:13.599456,0.0
137,ng2o98k983k,http://www.youtube.com/channel/UC03GryMq4-kjhdKQW-C8Xxw,UC03GryMq4-kjhdKQW-C8Xxw,Shaniel Rivas,UgwO3_SleYFHu-cW1vN4AaABAg,0,1582686265.0,"i was getting to point at 23:33 where i want to print article... it doesn't show the whole thing just a small portion of it form the middle...  was there a reason for that? 


EXAMPLE:
 <article class=""post-1670 post type-post status-publish format-standard has-post-thumbnail category-development category-python tag-gzip tag-shutil tag-zip tag-zipfile entry"" itemscope="""" itemtype=""https://schema.org/CreativeWork""><header class=""entry-header""><h2 class=""entry-title"" itemprop=""headline""><a class=""entry-title-link"" href=""https://coreyms.com/development/python/python-tutorial-zip-files-creating-and-extracting-zip-archives"" rel=""bookmark"">Python Tutorial: Zip Files – Creating and Extracting Zip Archives</a></h2>
<p class=""entry-meta""><time class=""entry-time"" datetime=""2019-11-19T13:02:37-05:00"" itemprop=""datePublished"">November 19, 2019</time> by <span class=""entry-author"" itemprop=""author"" itemscope="""" itemtype=""https://schema.org/Person""><a class=""entry-author-link"" href=""https://coreyms.com/author/coreymschafer"" itemprop=""url"" rel=""author""><span class=""entry-author-name"" itemprop=""name"">Corey Schafer</span></a></span> <span class=""entry-comments-link""><a href=""https://c...",none,,2020-07-07 23:38:13.599456,0.0
138,ng2o98k983k,http://www.youtube.com/channel/UC9j7CzslPhPjXxBa_KkUxAg,UC9j7CzslPhPjXxBa_KkUxAg,Ru eL,UgxcO9cQxr0-8DV8KmB4AaABAg,0,1582603124.0,"I like your video, very informative
Can you show us how to make this with define and class?",none,,2020-07-07 23:38:13.599456,0.0
139,ng2o98k983k,http://www.youtube.com/channel/UC6H9DKUNGpgN7uHbhKGp44A,UC6H9DKUNGpgN7uHbhKGp44A,John Jacobs,UgyHoIi-iIYh6XUov_d4AaABAg,1,1582311647.0,"Just a note, BeautifulSoup doesn't work with dynamic content.
If you see something like:

::before
(html content)
::after

That content is dynamically loaded by your browser from some sort of redirect or Javascript.  The html inside it doesn't exist when BeautifulSoup parses everything.",none,,2020-07-07 23:38:13.600460,0.0
140,ng2o98k983k,http://www.youtube.com/channel/UCD8-NT4AOm72PuCeKkkc4Lw,UCD8-NT4AOm72PuCeKkkc4Lw,Fausto Pucheta,UgwenALzzlTpzGwEMoZ4AaABAg,2,1582154451.0,Best scraping web video ever! Thank you so much for this gem!!,none,,2020-07-07 23:38:13.600460,0.0
141,ng2o98k983k,http://www.youtube.com/channel/UCbi3zsdTZ-Uvu9zNXks1oqA,UCbi3zsdTZ-Uvu9zNXks1oqA,Prashant Singh Chauhan,Ugzv7qzkTMwNT-_AL614AaABAg,1,1582123842.0,"Awesome, thanks you very much.",none,,2020-07-07 23:38:13.600460,0.0
142,ng2o98k983k,http://www.youtube.com/channel/UCPwNgsZSqZuSJDB7U47wtXA,UCPwNgsZSqZuSJDB7U47wtXA,ikhsan w,UgxPpRxfQE4qWQT_TWB4AaABAg,1,1582022716.0,"wow this tutorial is completely clear to understand, you save my life man, thanks...",none,,2020-07-07 23:38:13.600460,0.0
143,ng2o98k983k,http://www.youtube.com/channel/UCh-gXn807caQaEUUG3hnN4A,UCh-gXn807caQaEUUG3hnN4A,S. M. Shafiq Ahmed,Ugz7xefmMDK5Ys7AzfB4AaABAg,1,1581910961.0,very good and helpful tutorial,none,,2020-07-07 23:38:13.600460,0.0
144,ng2o98k983k,http://www.youtube.com/channel/UCVRrViy3X9bP5dU5WUqMAug,UCVRrViy3X9bP5dU5WUqMAug,sameer k,UgzwtMY-rDTQ-gUEr7Z4AaABAg,1,1581453449.0,Thank you very much Sir for this excellent tutorial. 👌,none,,2020-07-07 23:38:13.600460,0.0
145,ng2o98k983k,http://www.youtube.com/channel/UCsU79KMFPtcI0AbTm_tjVfA,UCsU79KMFPtcI0AbTm_tjVfA,Mike Starr,Ugzv0wHFXC1slYcSTZR4AaABAg,0,1581433053.0,"I've been searching for a solution for the past 2 hours and having no luck.. I'm stuck at the first step. I've installed beautifulsoup4, but I keep getting the error: ImportError: No module named bs4. I look up my pip list and I see that I've got beautifulsoup4 4.8.2  installed. How can I get past this error?",none,,2020-07-07 23:38:13.600460,2.0
146,ng2o98k983k,http://www.youtube.com/channel/UCgu5F16zMJ7gBHlUPgtIcKw,UCgu5F16zMJ7gBHlUPgtIcKw,Acássio dos Anjos,Ugw4cORR12oeUtGHr3d4AaABAg,1,1581192849.0,I just wanna say a HUGE thank you. :),none,,2020-07-07 23:38:13.600460,0.0
147,ng2o98k983k,http://www.youtube.com/channel/UClQp10QlNQo5cbGWilicSnw,UClQp10QlNQo5cbGWilicSnw,Muhammad Zilal Ab Hamid Pahmi,UgxNLbf44MWHD2TLygt4AaABAg,1,1581128267.0,"Very good video and explanation, thank you!",none,,2020-07-07 23:38:13.600460,0.0
148,ng2o98k983k,http://www.youtube.com/channel/UCGgQ1O34MPafK0g8TGAiIfQ,UCGgQ1O34MPafK0g8TGAiIfQ,Joseph Steen,UgyNIYCKuDD6-xh5Dn54AaABAg,0,1580929438.0,"Outstanding, Corey. I really enjoy and appreciate the clear information in your videos. Keep it up, and I am joining your patreon.",none,,2020-07-07 23:38:13.600460,0.0
149,ng2o98k983k,http://www.youtube.com/channel/UC_CRJJrWC2495lzjjLpfjBg,UC_CRJJrWC2495lzjjLpfjBg,Nipun Chaudhary,UgwRyV0JaRKvGR8ZGBp4AaABAg,0,1580585184.0,"Hi Corey, 
When I was trying to follow your tutorial end up getting below error. Can you help to fix that.
Thanks in Advance!
Nipun

Code:
=========
from bs4 import BeautifulSoup
import requests
source = requests.get('http://coreyms.com').text
soup = BeautifulSoup(source, 'lxml')
print(soup.prettify())
===========
Traceback (most recent call last):
  File ""/Users/nipunchaudhary/python_learning/hello.py"", line 7, in <module>
    print(soup.prettify())
UnicodeEncodeError: 'ascii' codec can't encode character '\xbb' in position 2769: ordinal not in range(128)
[Finished in 1.2s with exit code 1]
[cmd: ['/usr/local/bin/python3', '-u', '/Users/nipunchaudhary/python_learning/hello.py']]
[dir: /Users/nipunchaudhary/python_learning]
[path: /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin]",none,,2020-07-07 23:38:13.600460,0.0
150,ng2o98k983k,http://www.youtube.com/channel/UCUyPLz8u7N5JyN6oMcLrFPw,UCUyPLz8u7N5JyN6oMcLrFPw,SNG,UgwIUVWJ5V-Ou7lw-PF4AaABAg,217,1580520157.0,*Great video! Even the IBM staff suggested watching this video to complete a part of a course offered by them. Did you know? Congratulations!*,none,,2020-07-07 23:38:13.601457,5.0
151,ng2o98k983k,http://www.youtube.com/channel/UCvIpPPh3fDFCCWAhEstk0Bg,UCvIpPPh3fDFCCWAhEstk0Bg,Fazal Mehmood,UgxV34sFWzXSPTwK-j94AaABAg,0,1580423314.0,"Hi, can you please make a video on dynamic web scraping ?  :)",none,,2020-07-07 23:38:13.601457,1.0
152,ng2o98k983k,http://www.youtube.com/channel/UC4_-03mNN6TXyar8XIZJXfw,UC4_-03mNN6TXyar8XIZJXfw,Deojeet Sarkar,UgwdE32tTP4_VUmWjOl4AaABAg,0,1580364521.0,Can someone tell me where my csv file is stored if I'm using idle??,none,,2020-07-07 23:38:13.601457,1.0
153,ng2o98k983k,http://www.youtube.com/channel/UCA8iT3W8BvYT-FgAgMmXY7A,UCA8iT3W8BvYT-FgAgMmXY7A,Liam/Brooke,Ugz1EQqTa3vuKQJhSIR4AaABAg,0,1580283280.0,"Followed the tutorial but used a diff web-page that contained special chars, if the page you are scraping has these then it won't write to the csv file, in this case you can update the code to say csv_file = open('cms_scrape.csv', 'w', encoding='UTF-8'), I'm still learning but this lets the data be written to a csv file.",none,,2020-07-07 23:38:13.601457,0.0
154,ng2o98k983k,http://www.youtube.com/channel/UCMI--fVJ3z3sZE-5GbN17vw,UCMI--fVJ3z3sZE-5GbN17vw,oselio lima,Ugza26CtC8Cp88iUPfV4AaABAg,1,1580259986.0,Your videos are great and they are helping me a lot these days. Keep the excellent job!!,none,,2020-07-07 23:38:13.601457,0.0
155,ng2o98k983k,http://www.youtube.com/channel/UCXeEwKr46CNqjNAY7ozEdrw,UCXeEwKr46CNqjNAY7ozEdrw,Ankush Singh,UgzQZQ9EGiI-5hJ9XMV4AaABAg,0,1580132558.0,"Great video for any class of learner, this was my introduction to web scraping and I feel little confident to scrap few datas from websites.
Thanks a lot .",none,,2020-07-07 23:38:13.601457,0.0
156,ng2o98k983k,http://www.youtube.com/channel/UCSby2L5ToohnA3mKo1jwu3A,UCSby2L5ToohnA3mKo1jwu3A,sachin chaurasiya,Ugx8BvaRMng9XpWKp4B4AaABAg,0,1579864815.0,"in this video you have only scraped the  one page. but if I want to scrape all the pages on your site what would I do??
pls reply:)",none,,2020-07-07 23:38:13.601457,0.0
157,ng2o98k983k,http://www.youtube.com/channel/UCq8zMqkAas0D9zmNhSdt3ZQ,UCq8zMqkAas0D9zmNhSdt3ZQ,The TekChef,Ugx3DYdGcUo89_UnFm54AaABAg,0,1579738019.0,"Thank you so much for the video, for whatever reason I forgot to not use "".text"" to see the HTML code and it was a little frustrating a first but then I saw my mistake lol",none,,2020-07-07 23:38:13.601457,0.0
158,ng2o98k983k,http://www.youtube.com/channel/UCDO1xxC2DHvwvXIpTcFLSEg,UCDO1xxC2DHvwvXIpTcFLSEg,Antonio Belančić,UgwpxCVuLFu471lItKt4AaABAg,1,1579712081.0,"You are a great teacher, thanks very much for this video !",none,,2020-07-07 23:38:13.601457,0.0
159,ng2o98k983k,http://www.youtube.com/channel/UCi-4nECQ9csyWYJtVPIBc7A,UCi-4nECQ9csyWYJtVPIBc7A,Jack Frost,UgwNcBicxRdSuW8cJCt4AaABAg,1,1579686050.0,still this is the best tutorial,none,,2020-07-07 23:38:13.601457,0.0
160,ng2o98k983k,http://www.youtube.com/channel/UCkeWU123cEkSEzRoKz6exSA,UCkeWU123cEkSEzRoKz6exSA,Pedro Chacin,Ugyq-Y6_Oynhz-tmU794AaABAg,1,1579639485.0,"Excelente tutorial (
excellent tutorial thanks)",none,,2020-07-07 23:38:13.601457,0.0
161,ng2o98k983k,http://www.youtube.com/channel/UCIkqv-3RDSBUuDWDL-5nTGg,UCIkqv-3RDSBUuDWDL-5nTGg,CodewithRafiq,UgyJq0Nt0UcZTsko83F4AaABAg,1,1579621736.0,"Thank you very much for this wonderful video..
From  :   Bangladesh",none,,2020-07-07 23:38:13.601457,0.0
162,ng2o98k983k,http://www.youtube.com/channel/UCh9HYGBtt5YShk_iB8ltXVw,UCh9HYGBtt5YShk_iB8ltXVw,Priyam Vashi,UgwD9S1fZB7AM3fYc4h4AaABAg,0,1579597960.0,Thank you so much  <3,none,,2020-07-07 23:38:13.601457,0.0
163,ng2o98k983k,http://www.youtube.com/channel/UC5qP9Tl-P2-hYDJ9hh8ODQA,UC5qP9Tl-P2-hYDJ9hh8ODQA,daniel marsh,UgzMdbgkzvlukNw2adV4AaABAg,0,1579343709.0,"probably not going to get a reply, but im using a rasbery pi, when i try to pip install any of these i always get an error... I dont know whythis is happening",none,,2020-07-07 23:38:13.601457,0.0
164,ng2o98k983k,http://www.youtube.com/channel/UCZT4SIkt3H7ELjNqnGKRscQ,UCZT4SIkt3H7ELjNqnGKRscQ,PRAVIN BAGUL,UgxcYyFTMOEkeJZ6wQ94AaABAg,1,1579317409.0,Thanks Corey for making my python journey very simpler with your tutorials!!!,none,,2020-07-07 23:38:13.602515,0.0
165,ng2o98k983k,http://www.youtube.com/channel/UC9bjXdT4UlRf2prZ8LjVZOg,UC9bjXdT4UlRf2prZ8LjVZOg,Sai Kiran,Ugy-c2_2KU8YpzO66yx4AaABAg,0,1579167268.0,❤ from India😊,none,,2020-07-07 23:38:13.602606,0.0
166,ng2o98k983k,http://www.youtube.com/channel/UCFrA86xVanGwxVr0qOAoyKA,UCFrA86xVanGwxVr0qOAoyKA,ashok ramuni,Ugw9Y-9i_3nKUGoxVc94AaABAg,0,1579156667.0,How can we scrape next pages in the website,none,,2020-07-07 23:38:13.602606,0.0
167,ng2o98k983k,http://www.youtube.com/channel/UCw8GQnxgDASka88NpwR1OvQ,UCw8GQnxgDASka88NpwR1OvQ,Biswashree Byomakesh Dash,UgwpSa4M4PGAzvJaBDJ4AaABAg,1,1578758210.0,It was really an excellent video. Thank you:),none,,2020-07-07 23:38:13.602606,0.0
168,ng2o98k983k,http://www.youtube.com/channel/UCCEAjVd2zvCTTaxzpMTcKkg,UCCEAjVd2zvCTTaxzpMTcKkg,anton sujarwo,Ugy5E5-RQj1Q2qagONJ4AaABAg,0,1578722377.0,"please help me, is anybody in here have a video tutorial link on how to extract data from webisite that required username and password for login. i am an admin of such website and i want to extract all the name, phone number, citizen id and email from that website to excel??? many thanks before",none,,2020-07-07 23:38:13.602606,0.0
169,ng2o98k983k,http://www.youtube.com/channel/UC_rBfb1L54zJphqfrFGZsZw,UC_rBfb1L54zJphqfrFGZsZw,Eric Farjun,UgylPdzTFfmwTr8TeP54AaABAg,0,1578502221.0,Can someone explain to me how I can prevent '\n' from appearing when I try to print the output? Thank you.,none,,2020-07-07 23:38:13.602606,1.0
170,ng2o98k983k,http://www.youtube.com/channel/UC5kR5kRZC79NjN8yay1Qnkg,UC5kR5kRZC79NjN8yay1Qnkg,Kiran,Ugw6diQq1hbe1uDvx0h4AaABAg,0,1578317585.0,"https://youtu.be/_P7X8tMplsw
https://youtu.be/fKl2JW_qrso
https://youtu.be/IEEhzQoKtQU
I scrapped some of the links from your website
https://youtu.be/mO_dS3rXDIs
https://youtu.be/2Fp1N6dof0Y
https://youtu.be/-nh9rCzPJ20
https://youtu.be/06I63_p-2A4
https://youtu.be/_JGmemuINww",none,,2020-07-07 23:38:13.602606,0.0
171,ng2o98k983k,http://www.youtube.com/channel/UCeC1J8T3r0De2iyniH2_RSw,UCeC1J8T3r0De2iyniH2_RSw,Robo,Ugy9HjegdD_brglc0JN4AaABAg,0,1578255486.0,"I've really enjoyed your tutorial but your editor markers has give me 30 mins of work/depression, just because I wrote (at 9:40) '_simple.html_' instead of 'simple.html' and (at 10:06)  '_lxml_' instead of 'lxml' just because there was underlines under the ' ....",none,,2020-07-07 23:38:13.602606,0.0
172,ng2o98k983k,http://www.youtube.com/channel/UCtS5NlwafrkfDdT6LHs01cw,UCtS5NlwafrkfDdT6LHs01cw,Md kaif Khan,UgzOebSu6j4g2p0Hayx4AaABAg,1,1578206954.0,You have taught to scrap data from first page of your website. How to scrap data from entire pages of your website,none,,2020-07-07 23:38:13.602606,0.0
173,ng2o98k983k,http://www.youtube.com/channel/UCXfXE95MP-xYEMG9HZAvQgQ,UCXfXE95MP-xYEMG9HZAvQgQ,maicom coelho lopes,Ugz-EPd2YIFrxT54Y6h4AaABAg,2,1578005723.0,"Corey muito obrigado eu sou brasileiro, seus tutoriais são excelentes e me ajudam muito, principalmente porque não tenho dinheiro para comprar  cursos, muito obrigado",none,,2020-07-07 23:38:13.602606,0.0
174,ng2o98k983k,http://www.youtube.com/channel/UCUxIRQZR7l0cXHt0706qg3g,UCUxIRQZR7l0cXHt0706qg3g,chaitanya sonagara,UgwSySYMTU9UZ0XTrFt4AaABAg,0,1577977009.0,"Hi, Thank you so much for this useful tutorial. I was looking for exactly what you mentioned here.",none,,2020-07-07 23:38:13.602606,0.0
175,ng2o98k983k,http://www.youtube.com/channel/UC3L2ctI1OnjMkALQa-2XO4g,UC3L2ctI1OnjMkALQa-2XO4g,Dimitar Dimitrov,UgwRW0wpVB2xFV0HE3N4AaABAg,0,1577810357.0,"I very rarely comment or like vidoes or even subsribe to channels. But Corey man you are doing great work. I learn alot of good practices and alot of stuff for python. I really enjoy your channel and i like every video. I can support only with likes, shares and subscrition for now. Please make a video about all your ideas that you have you wont get wrong. I enjoy everything that you make, its very usful when trying to learn python. I know its a good day when i am searching for a topic and see Corey Schafer video!",none,,2020-07-07 23:38:13.602606,1.0
176,ng2o98k983k,http://www.youtube.com/channel/UC5YZZ0DEh7r9IDsVfiOM4QQ,UC5YZZ0DEh7r9IDsVfiOM4QQ,zaheer habib,Ugz5741t6w3niTLIz_J4AaABAg,0,1577774064.0,if we are using online Jupiter note book on IBM lab; how we can access this said file through there? please advise.....,none,,2020-07-07 23:38:13.602606,0.0
177,ng2o98k983k,http://www.youtube.com/channel/UCHg_QyXO6TANcgLUZl6OC0Q,UCHg_QyXO6TANcgLUZl6OC0Q,Ferreira N,Ugw7wlt4xTMFEvmJDtB4AaABAg,0,1577712406.0,"First of all I would like to thank you Corey for the best coding videos I found on YT. I just have a simple question , and ill explain it. I am trying to get data from big websites, but its not possible to get the html info by just look up in the codes for some keyword. Does this method only works with smaller websites?",none,,2020-07-07 23:38:13.603497,1.0
178,ng2o98k983k,http://www.youtube.com/channel/UCh_0Y5J0hfS7o78NAYtiXcg,UCh_0Y5J0hfS7o78NAYtiXcg,Ankesh Singh,UgyucGLmUrASbOBb-YJ4AaABAg,0,1577636673.0,"Great work, Must say, Helped a lot to start my NLP from your channel.",none,,2020-07-07 23:38:13.603497,0.0
179,ng2o98k983k,http://www.youtube.com/channel/UCTQjzcoIzRxL6xkF5wVckIQ,UCTQjzcoIzRxL6xkF5wVckIQ,Jen TheReader,Ugz-TsKMuiGoN9XK0Ll4AaABAg,1,1577476991.0,"Clear and easy to follow, best tutorials!
If I learned from you at first, I would not quit due to mounted frustration and limited understanding of available tools.",none,,2020-07-07 23:38:13.603497,0.0
180,ng2o98k983k,http://www.youtube.com/channel/UCxT59EdsYqV0cWjgsaz9kJg,UCxT59EdsYqV0cWjgsaz9kJg,Rodrigo Vidal,UgxtAhMBSQyRmjCvIxp4AaABAg,0,1577291787.0,Excellent video. Thanks,none,,2020-07-07 23:38:13.603497,0.0
181,ng2o98k983k,http://www.youtube.com/channel/UCJI-l_yPytLuJ1ohpBUUADQ,UCJI-l_yPytLuJ1ohpBUUADQ,Rahul Das,Ugyqi-Coq_AUoHj5DmJ4AaABAg,1,1576952023.0,"Thanks, this was very helpful!",none,,2020-07-07 23:38:13.603497,0.0
182,ng2o98k983k,http://www.youtube.com/channel/UCueX2Q-jdCEL82AP4GVIeRg,UCueX2Q-jdCEL82AP4GVIeRg,Mike Wiseman,UgwbvOqBTdsz-w7658N4AaABAg,1,1576775885.0,"I haven't laid into a keyboard in 25 years since a little Basic, fortran, and some VB. I'm just wanting to make myself a little Kodi addon of my favorite live streams. I started teaching myself Python a few days ago. I learned a couple things from you. I followed you except when you went to the HTML. I don't want anyone to do this for me so obviously I have to learn HTML to complete my conquest. Thanks for filling a few holes.",none,,2020-07-07 23:38:13.603497,0.0
183,ng2o98k983k,http://www.youtube.com/channel/UCKKhJi0pKNjo7Clb46LHUbA,UCKKhJi0pKNjo7Clb46LHUbA,mdempse1,UgyEiJZOBejrItqIwIF4AaABAg,0,1576607853.0,"Corey, at this part of the code:
vid_src = article.find('iframe', class_='youtube-player')['src']
print(vid_src)


I get this:  
Traceback (most recent call last):

  File ""c:/Users/mdempse1/Documents/PythonTraining/mike.py"", line 16, in <module>
    vid_src = article.find('iframe', class_='youtube-player')['src']
TypeError: 'NoneType' object is not subscriptable


I assume it has something to do with the class_'youtube-player'....or???
Stuck!


Thanks,


Mike",none,,2020-07-07 23:38:13.603497,2.0
184,ng2o98k983k,http://www.youtube.com/channel/UCXyZW95GKb38XITvBUgB4PQ,UCXyZW95GKb38XITvBUgB4PQ,Devesh Datwani,Ugzrihi7TJpKs5P3JOF4AaABAg,1,1576585654.0,Good job at explaining. Much appreciated. I really feel the documentation online of BeautifulSoup is really weak and lacks clarity. I am facing a lot of issues while creating a project.,none,,2020-07-07 23:38:13.603497,0.0
185,ng2o98k983k,http://www.youtube.com/channel/UCrBPy7tBholI_Y6jy5arBBQ,UCrBPy7tBholI_Y6jy5arBBQ,Lakshya Gupta,Ugwg5TllkhXAmTkHXkF4AaABAg,0,1576554492.0,Why is the program giving me FileNotFoundError when I have clearly defined an html file in the same directory as the python file? Help! :(,none,,2020-07-07 23:38:13.603497,0.0
186,ng2o98k983k,http://www.youtube.com/channel/UCAeA2iPoSHxpVqxYkbnLvEw,UCAeA2iPoSHxpVqxYkbnLvEw,Тимурка Долгорукий,UgyCRzJ-_-r2-bgN6_R4AaABAg,2,1575955186.0,"I'm viewer from Russia, your English speaking is very clear. Thx.",none,,2020-07-07 23:38:13.603497,0.0
187,ng2o98k983k,http://www.youtube.com/channel/UCTDZiDpeCw9kL65COmo5Mig,UCTDZiDpeCw9kL65COmo5Mig,vaibhav wadate,Ugy7rLCIbcNEHo7dM-l4AaABAg,0,1575876770.0,"hey can i upload the video for, extract signature from body of emails. save the signature in csv file",none,,2020-07-07 23:38:13.603497,0.0
188,ng2o98k983k,http://www.youtube.com/channel/UC5g4T4Xjl57Is4hjPBJA6Ng,UC5g4T4Xjl57Is4hjPBJA6Ng,shubham sharma,UgyXrQmFEZ8WVuol3_94AaABAg,0,1575606849.0,"As Corey's website has changed little bit, i.e only the first article consists of video and rest , just the youtube link. 
Here's a link will fetch all data on Dec 2019: https://ideone.com/DDZb1v",none,,2020-07-07 23:38:13.603497,1.0
189,ng2o98k983k,http://www.youtube.com/channel/UCcE1IKG8fUNpUHGgLd6jkyg,UCcE1IKG8fUNpUHGgLd6jkyg,James Hahn II,UgzhOMvV__Eli4lSqt94AaABAg,2,1575555238.0,Been struggling mightily with BeautifulSoup for the past couple weeks. You cleared up a ton of confusion for me. Thank you!,none,,2020-07-07 23:38:13.603497,0.0
190,ng2o98k983k,http://www.youtube.com/channel/UC7sdicZwFCle9fMS61G0lEw,UC7sdicZwFCle9fMS61G0lEw,Tomer Harari,Ugx-_ECCLYbnekxqqM54AaABAg,0,1575476716.0,"Hey Corey,
Great video and thank you so much. I'm having an issue with simply printing header tags.
my code looks like this: 
secondClass = soup.find('div',class_=""second_Class"") #second_Class is the name of my div


header_tag = secondClass.h1.a.text   #I have header 1 with an anchor tag inside it
print(header_tag)


and I keep getting this error: AttributeError: 'NoneType' object has no attribute 'h1'
 What am I doing wrong??",none,,2020-07-07 23:38:13.604498,2.0
191,ng2o98k983k,http://www.youtube.com/channel/UCApxMIjXM2b6Gb6OLdQ_i3w,UCApxMIjXM2b6Gb6OLdQ_i3w,jagadeesh Sali,UgzJOlDb1rpMg6rnKK54AaABAg,0,1575086708.0,how to scrape of wepsite home page team page contact page,none,,2020-07-07 23:38:13.604498,0.0
192,ng2o98k983k,http://www.youtube.com/channel/UCziFIObf8veQDRu1gA5tmfQ,UCziFIObf8veQDRu1gA5tmfQ,Muddather Abuzaid,UgxckGVCiMysCDPxCxx4AaABAg,1,1574898400.0,"Alslamu Alikum Mr. Corey, thank you for such comprehensive tutorial. I wish you would make a tutorial on using scrapy as well because i couldn't find a good one like yours on YouTube so far. I wish you all the best.",none,,2020-07-07 23:38:13.604498,0.0
193,ng2o98k983k,http://www.youtube.com/channel/UCyDE5Ch50i_8LdNMHNmNjuw,UCyDE5Ch50i_8LdNMHNmNjuw,Korai ronams,UgzuzTdvHbFsMBV4tcR4AaABAg,0,1574709845.0,"Please help!
If I want to input a word that exists on a website so it finds it and gets the number which sits near it(which changes every 5 seconds). how do I do that?",none,,2020-07-07 23:38:13.604498,0.0
194,ng2o98k983k,http://www.youtube.com/channel/UCZaen3d-jgZogQEAF8NsiSA,UCZaen3d-jgZogQEAF8NsiSA,pj mclenon,UgwxAwUS9m_Nv1kkxtV4AaABAg,0,1574694821.0,hello it doesnt work i get no links for the vids in the csv file Lisa,none,,2020-07-07 23:38:13.604498,0.0
195,ng2o98k983k,http://www.youtube.com/channel/UCybqUqPutSD9vwB8EcbSdPQ,UCybqUqPutSD9vwB8EcbSdPQ,G L,Ugz5pQAtgOAH9Pn_Cs54AaABAg,2,1574488167.0,"Amazing. Now, this video is very beginner friendly. I'm learning Python and I didn't think I'd be able to use BS4 as a newbie. Thank you a lot Mr Schafer.",none,,2020-07-07 23:38:13.604498,0.0
196,ng2o98k983k,http://www.youtube.com/channel/UCjGFo3U5qqmtpMAQicp3d5Q,UCjGFo3U5qqmtpMAQicp3d5Q,alex takele,UgynHRIMpOvicb6Sgud4AaABAg,3,1574070541.0,"Thank you, I am data science student from university of gondar,Ethiopia,I seriously admire you way speech,thank you for all you did!",none,,2020-07-07 23:38:13.604498,0.0
197,ng2o98k983k,http://www.youtube.com/channel/UChIUB8Y28Fn3ItkJAFuZKJQ,UChIUB8Y28Fn3ItkJAFuZKJQ,tiga,Ugx6Mo7oBtP0FYhctLF4AaABAg,0,1573956906.0,yo what . editor you using?,none,,2020-07-07 23:38:13.604498,1.0
198,ng2o98k983k,http://www.youtube.com/channel/UCzUt4DWs5EMDJkJEKuGZ-uQ,UCzUt4DWs5EMDJkJEKuGZ-uQ,Nigel Warfield,Ugz4P4DjM577duXh4RB4AaABAg,0,1573649961.0,I wandered if you had finally blocked access to public scraping your website ?,none,,2020-07-07 23:38:13.604498,0.0
199,ng2o98k983k,http://www.youtube.com/channel/UCzUt4DWs5EMDJkJEKuGZ-uQ,UCzUt4DWs5EMDJkJEKuGZ-uQ,Nigel Warfield,UgyoaEBPlJ9NdZct1AN4AaABAg,0,1573649909.0,"Hi, Corey I am getting an error running this code on Windows10 in SLT as ""File ""C:\Users\nigel\AppData\Local\Programs\Python\Python38\lib\site-packages\bs4\__init__.py"", line 213, in __init__
    raise FeatureNotFound(

bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?""
I looked on Stack but cannot determine a fix? Any advice. it seems to be on line with - soup = BeautifulSoup(source, 'lxml')?",none,,2020-07-07 23:38:13.604498,0.0
200,ng2o98k983k,http://www.youtube.com/channel/UC_BCBGObNcLiyDVHsxWi2nQ,UC_BCBGObNcLiyDVHsxWi2nQ,Mohan Puri,UgxRpBbY3lIxp7BKqLd4AaABAg,4,1573288886.0,"I have watched couple of videos on web scrapping with Python. This video is very clear, and pace of delivery and the example teaching of the author is excellent.",none,,2020-07-07 23:38:14.144048,0.0
201,ng2o98k983k,http://www.youtube.com/channel/UCuiO6sxO1fYEXyUVZOIYNLQ,UCuiO6sxO1fYEXyUVZOIYNLQ,Sou Reachsak,Ugx_s5sXBdgA0WKJ0PJ4AaABAg,0,1572957237.0,"Hello! I have one problem. I have written the code the same way as you do, but i try to scrape Wikipedia. There is no error, however when i inspect the page in the browser and make a side by side comparison with my result, there are many differences. It seems like some text are missing. In other word, it is not exactly the same.
Why is that? Is it because of the parser?",none,,2020-07-07 23:38:14.144048,0.0
202,ng2o98k983k,http://www.youtube.com/channel/UCqCDlHoEhyPaWqR84gRLpng,UCqCDlHoEhyPaWqR84gRLpng,Dennis Asamoah,UgyfpgDMH9D5eLfhaYl4AaABAg,1,1572572763.0,Legend,none,,2020-07-07 23:38:14.144048,0.0
203,ng2o98k983k,http://www.youtube.com/channel/UCEAI6wwB3n51l8YQwdHtGuQ,UCEAI6wwB3n51l8YQwdHtGuQ,Nikhil Chauhan,UgwUXy9sgxFg9sQmbvN4AaABAg,3,1572531864.0,thank you so much. i got my first client just because of you.,none,,2020-07-07 23:38:14.144048,0.0
204,ng2o98k983k,http://www.youtube.com/channel/UCWLBuvEqd7iEUyju0xLSMXw,UCWLBuvEqd7iEUyju0xLSMXw,Patrick Duhirwe Nzivugira,UgxdSk433uivY9IY_-l4AaABAg,1,1572492582.0,Thank you very much!!!!!,none,,2020-07-07 23:38:14.144048,0.0
205,ng2o98k983k,http://www.youtube.com/channel/UCqulYOeY-TRQRHX5-slOLBg,UCqulYOeY-TRQRHX5-slOLBg,sairam subramaniam,UgyqKnU3doB4y8nR2np4AaABAg,3,1572491745.0,You are the best programming teacher I ever had.,none,,2020-07-07 23:38:14.145047,0.0
206,ng2o98k983k,http://www.youtube.com/channel/UCkCShq9oXlgb10U_hJQFDKQ,UCkCShq9oXlgb10U_hJQFDKQ,Helen K,UgxB5LQK51IrFPSqwwx4AaABAg,1,1572470757.0,Very appreciated!  So clear.,none,,2020-07-07 23:38:14.145047,0.0
207,ng2o98k983k,http://www.youtube.com/channel/UCC5iEfO1VMszdH3AD392Bnw,UCC5iEfO1VMszdH3AD392Bnw,Roger Wellard,UgzNjVtl7RfVnyNGcvx4AaABAg,1,1572266691.0,He does it again! Thank you,none,,2020-07-07 23:38:14.145047,0.0
208,ng2o98k983k,http://www.youtube.com/channel/UC3-P4jCWt5EX9eUzNgNW10Q,UC3-P4jCWt5EX9eUzNgNW10Q,Samuel Tukua,UgzJtTnACpXrIJRrrN14AaABAg,3,1572042791.0,I started watching your videos years ago and I was afraid you wouldn't be around when I returned from my programming hiatus. Thank you for still being here!,none,,2020-07-07 23:38:14.145047,0.0
209,ng2o98k983k,http://www.youtube.com/channel/UChwraAHJ9pa7RP4upLDxAxg,UChwraAHJ9pa7RP4upLDxAxg,Romain Geneste,Ugxql3rCOFZIJf3QsHN4AaABAg,0,1571823622.0,"I have a problem with this site: www.leboncoin.fr
I can't scrap whatever the media i use. Do you know why ?",none,,2020-07-07 23:38:14.145047,0.0
210,ng2o98k983k,http://www.youtube.com/channel/UCV6hEiTgDZXt-bpXAB-Kl5Q,UCV6hEiTgDZXt-bpXAB-Kl5Q,idrish chy,UgytFMXGfectI5B-d0Z4AaABAg,0,1571677168.0,"could you explain how to access the following all paragraph      <div class=main>    <div class=another-div>   <p>paragraph content </p>  </div>
<p>p content </p>   <p>p content </p>  <p>p-content </p>    <p>p-content </p>           </div>    I want scrape all the paragraph text inside main div",none,,2020-07-07 23:38:14.145047,0.0
211,ng2o98k983k,http://www.youtube.com/channel/UCumACunkBfDu7k_5R__LdDw,UCumACunkBfDu7k_5R__LdDw,Ben Zikri,Ugyv5kos6UmTuxTig_Z4AaABAg,0,1571670540.0,"Just an awesome vid as always, Thanks! 
while working on this project with SublimeText 3 on Windows and opening the CSV file with Excel I encountered a small problem when there was an extra line between the data, after googling it I found out that we should add ""newline=''"" after opening the CSV file.
it should look like this 
csv_file = open('cms_scrape.csv', 'w', newline='')
as noted in the stackoverflow question it only happens in Python 3.",none,,2020-07-07 23:38:14.145047,0.0
212,ng2o98k983k,http://www.youtube.com/channel/UC0Ev96ATQqjhEGXNgNtQ8pQ,UC0Ev96ATQqjhEGXNgNtQ8pQ,James Reichert,Ugxi4RvpI0xfLWKKLmN4AaABAg,1,1571372962.0,Great video. Thank you!,none,,2020-07-07 23:38:14.145047,0.0
213,ng2o98k983k,http://www.youtube.com/channel/UCLOLINeqrX5ULbDO_WSNVFg,UCLOLINeqrX5ULbDO_WSNVFg,Balraj Ashwath,Ugzm_jrpJViTk_sDUBR4AaABAg,1,1571128073.0,"Simple, crisp, clear and brilliant!",none,,2020-07-07 23:38:14.145047,0.0
214,ng2o98k983k,http://www.youtube.com/channel/UCtWzk0h0dgSwtgFCUoHNlsA,UCtWzk0h0dgSwtgFCUoHNlsA,Stanley John,UgzA_xCq0LomKb6WxDp4AaABAg,1,1571104596.0,"At 31:40:
When you want to receive the id from the string; instead of using a split method, is it okay to do it like this?:
for i in vid_id:
 if i == ""?"":
  index = vid_id.index(i)
  print(vid_id[:index])
  break


I know that your method is easiest and convenient but is there anything wrong with this method of writing?",none,,2020-07-07 23:38:14.145047,0.0
215,ng2o98k983k,http://www.youtube.com/channel/UCJtaRfTX6Ezp9l5ey3sEKBA,UCJtaRfTX6Ezp9l5ey3sEKBA,Mattia Papa,UgwVNA6zqOm1yVHwGsB4AaABAg,1,1570952104.0,"Man thank you so much, this video was enlightening!",none,,2020-07-07 23:38:14.145047,0.0
216,ng2o98k983k,http://www.youtube.com/channel/UCtWzk0h0dgSwtgFCUoHNlsA,UCtWzk0h0dgSwtgFCUoHNlsA,Stanley John,UgzpNwlykGir1wRL7iB4AaABAg,0,1570879429.0,When does a person usually use a web scraper? I mean in terms of practical sense. Why do I need to scrape someone else's or my own site for information?,none,,2020-07-07 23:38:14.145047,0.0
217,ng2o98k983k,http://www.youtube.com/channel/UCLmSgRRuS3TDzBjkaXDrtiQ,UCLmSgRRuS3TDzBjkaXDrtiQ,SAANNAATTE SHOW,Ugw06csdD6B6VicUs6x4AaABAg,1,1570870587.0,Hello hi,none,,2020-07-07 23:38:14.145047,0.0
218,ng2o98k983k,http://www.youtube.com/channel/UCo_ah_v5sR0dJlxathj6ngg,UCo_ah_v5sR0dJlxathj6ngg,Bishwas Bhandari,UgyDOQdGHJJQar82Dh14AaABAg,0,1570843163.0,"If you enduring through an error of *BeautifulSoup User Warning*, just have a check here: code.neptechofficial.com/python/how-to-get-rid-of-beautifulsoup-user-warning",none,,2020-07-07 23:38:14.146043,0.0
219,ng2o98k983k,http://www.youtube.com/channel/UCWeXxVfOc-wBxG7BU7oZfUA,UCWeXxVfOc-wBxG7BU7oZfUA,Maximilian Cynke,UgzhZMkRPbs061frpRR4AaABAg,1,1570602411.0,Very helpful vid :^) !,none,,2020-07-07 23:38:14.146043,0.0
220,ng2o98k983k,http://www.youtube.com/channel/UCCeFfkzmfs8DTY_HlaSaywg,UCCeFfkzmfs8DTY_HlaSaywg,Andre Nevares,UgxbyC869BW9B6gqcbF4AaABAg,0,1570407463.0,"Sorry .   I am a newbe... Anyone has this problem?      vid_id = vid_src.split('/')  TypeError: 'NoneType' object is not callable.   Visual Studio Code do not recognize this",none,,2020-07-07 23:38:14.146043,0.0
221,ng2o98k983k,http://www.youtube.com/channel/UCCeFfkzmfs8DTY_HlaSaywg,UCCeFfkzmfs8DTY_HlaSaywg,Andre Nevares,UgzNzOuAdZlG5g9x5JJ4AaABAg,2,1570407294.0,"Hello.... I a from Rio de Janeiro, Brazil...  You are the best of the best!  You rock!!!!  Thanks to sharing with us?  I want to contribute with you.   Can u get in touck please?  It´s gonna be a pleasure.",none,,2020-07-07 23:38:14.146043,0.0
222,ng2o98k983k,http://www.youtube.com/channel/UCqunw1txdL8k-Ft58UL5v9w,UCqunw1txdL8k-Ft58UL5v9w,Ziad Ammouri,UgxEKXqXFTFJ6ffygOF4AaABAg,0,1570283932.0,"You are awesome man with the way you teach!
You should write a book using python the impressive way!
I have a question can you show us how to data scrape a website need username and password .?
Thanks",none,,2020-07-07 23:38:14.146043,0.0
223,ng2o98k983k,http://www.youtube.com/channel/UCFwmgFfK1zicD3yXgJpv-ag,UCFwmgFfK1zicD3yXgJpv-ag,John Dabbs,UgwIlTZayTcXANL7e-J4AaABAg,1,1570186433.0,Thanks corey,none,,2020-07-07 23:38:14.146043,0.0
224,ng2o98k983k,http://www.youtube.com/channel/UCLCem5lPZAyIbO8gJbOHdYw,UCLCem5lPZAyIbO8gJbOHdYw,Elliott Carter,UgxG7bE3HBUIGEAuKIV4AaABAg,1,1570031110.0,I can always count on clear and concise explanations from Corey Schafer! Thanks!,none,,2020-07-07 23:38:14.146043,0.0
225,ng2o98k983k,http://www.youtube.com/channel/UClJmAEL1VhpAZ6USKys670A,UClJmAEL1VhpAZ6USKys670A,joehary ar,UgyQqX8PPBv9u9syPk54AaABAg,0,1569993217.0,"Hi Corey, first of all..thanks for sharing this tutorial. Your explanation clear and easy to understand. Full star for you. Good Job man... anyway...do you have video that show how to collect data from rest api, do some filtering/extract data and final send the clean data to flask html. If it is not available..I would like to request one...if you have the video please advise me...Thanks.",none,,2020-07-07 23:38:14.146043,0.0
226,ng2o98k983k,http://www.youtube.com/channel/UCEcuY9jZnMt9KLNfcEXAr9A,UCEcuY9jZnMt9KLNfcEXAr9A,Sarita Prabhu,UgxmAomF24xG6s-7JEN4AaABAg,2,1569986782.0,You're a legend! There's a lot to learn from you. 🙇,none,,2020-07-07 23:38:14.146043,0.0
227,ng2o98k983k,http://www.youtube.com/channel/UCDPt5hNQumc2C83N30BP_UA,UCDPt5hNQumc2C83N30BP_UA,Sreejith Munthikodu,UgyPQqDK73LbnCG7Rkh4AaABAg,4,1569598621.0,"If I want to learn anything in python, I always search for Corey Schafer  on youtube and never  had to look beyond that!!! Really appreciate your great work!",none,,2020-07-07 23:38:14.146043,0.0
228,ng2o98k983k,http://www.youtube.com/channel/UCutAhb6wq3hJTrqYKgShCRw,UCutAhb6wq3hJTrqYKgShCRw,Applied statistics,UgwhisL4S9R_D6G9FBp4AaABAg,2,1569485913.0,"simple and fast, best tutorial ever. Now I'm trying to scrape the IMDb review for text mining",none,,2020-07-07 23:38:14.146043,1.0
229,ng2o98k983k,http://www.youtube.com/channel/UCEa9mjd1Uzryo2FJiiiuLfQ,UCEa9mjd1Uzryo2FJiiiuLfQ,My YouTube Channel,UgyQZZ_GBPYv2wWqemV4AaABAg,1,1569463925.0,Thank you Corey. A *simply fantastic* tutorial.,none,,2020-07-07 23:38:14.146043,0.0
230,ng2o98k983k,http://www.youtube.com/channel/UCpuv-Qjgd4llAuYIJjQqSNA,UCpuv-Qjgd4llAuYIJjQqSNA,Mohib,UgwCEFMhga2QSTxCwzd4AaABAg,1,1569422522.0,how can i get requests in atom. I have it installed but it won't work. Works perfectly on IDLE,none,,2020-07-07 23:38:14.146043,11.0
231,ng2o98k983k,http://www.youtube.com/channel/UCEcAQdBZy4j24LV9G3hb4QQ,UCEcAQdBZy4j24LV9G3hb4QQ,azusapink,UgzHbjPxFL3wsXB60Yl4AaABAg,0,1569347841.0,Does he have one where he extracts a zip file?,none,,2020-07-07 23:38:14.147041,0.0
232,ng2o98k983k,http://www.youtube.com/channel/UCh_vbke7Az2HoK08EWjsCaQ,UCh_vbke7Az2HoK08EWjsCaQ,Eric Brass,Ugzl4el1Ay6dHLuDjr14AaABAg,1,1568317799.0,Excellent!,none,,2020-07-07 23:38:14.147041,0.0
233,ng2o98k983k,http://www.youtube.com/channel/UCb0OyTFu2P9BYVY5yQH6iUg,UCb0OyTFu2P9BYVY5yQH6iUg,acid aioli,UgxDOgg0vkwmhl-3-5N4AaABAg,0,1568297805.0,"When exporting to .csv, I get a .TUTORIAL file instead. Anyone knows how to fix this?


Great video!",none,,2020-07-07 23:38:14.147041,0.0
234,ng2o98k983k,http://www.youtube.com/channel/UCLWK9rC8gwi_HOaHNb5pOwA,UCLWK9rC8gwi_HOaHNb5pOwA,xoxUnD3R0aThxox,Ugz4KqEUyzOdIgXg9kB4AaABAg,0,1568156948.0,"How can I retrieve the fasta sequence and the length of sequence from this page: 

https://phytozome.jgi.doe.gov/phytomine/portal.do?class=Protein&externalids=Cre35.g759247.t1.1",none,,2020-07-07 23:38:14.147041,0.0
235,ng2o98k983k,http://www.youtube.com/channel/UC_WFOA6eIiNITKLcM1VqJ5Q,UC_WFOA6eIiNITKLcM1VqJ5Q,vishal verma,Ugyc4HqgFXJUzzbyHCR4AaABAg,0,1568019110.0,"I can't parse the full HTML from the website https://www.machinehack.com/course-cat/practise/. I am using lxml parser, but when I opened my beautiful soup, it contained incomplete HTML, i.e. not all articles were covered. Infact, only 1st article was covered and rest were missing from the HTML. Can you please tell how to solve this issue ?  Thanks",none,,2020-07-07 23:38:14.147041,0.0
236,ng2o98k983k,http://www.youtube.com/channel/UC2UfXujg9GMUh83SgDbanyA,UC2UfXujg9GMUh83SgDbanyA,Imad Oulhou,Ugx-vJ1kumAE7zF49A54AaABAg,1,1567980537.0,tnx,none,,2020-07-07 23:38:14.147041,0.0
237,ng2o98k983k,http://www.youtube.com/channel/UCBR5UzsNi13e62cnoqfj2NA,UCBR5UzsNi13e62cnoqfj2NA,Keshav Rawat,UgwKS6TLRYf43qpXUXx4AaABAg,1,1567935535.0,Very nice video,none,,2020-07-07 23:38:14.147041,0.0
238,ng2o98k983k,http://www.youtube.com/channel/UCx4nt15Fg5AL_fiJVCOYIeQ,UCx4nt15Fg5AL_fiJVCOYIeQ,Sthitaprajna Mishra,UgxTsbvXjSGRVGpcAe94AaABAg,0,1567834532.0,"Start at 8:35 if you want to skip the intro, a mini HTML tutorial and installation of the packages.",none,,2020-07-07 23:38:14.147041,0.0
239,ng2o98k983k,http://www.youtube.com/channel/UCTv_BFU5aWW3yC63At2iYHQ,UCTv_BFU5aWW3yC63At2iYHQ,robin vermillion,Ugycu_K1Rzo1Zba-oj14AaABAg,0,1567715689.0,the video is  outdated. had to add from (from bs4 import BeautifulSoup) to get the first ten minutes of the video to work with no errs in the code but it return nothing. no website headers or body.,none,,2020-07-07 23:38:14.147041,0.0
240,ng2o98k983k,http://www.youtube.com/channel/UCpfu1gUFdXQO31RzL5mK75Q,UCpfu1gUFdXQO31RzL5mK75Q,jagjeet randhawa,Ugzv1603yhDo0bEwK4l4AaABAg,1,1567497232.0,Thank you,none,,2020-07-07 23:38:14.148038,0.0
241,ng2o98k983k,http://www.youtube.com/channel/UCWZq39wf6vSJfk5GB9bNdEA,UCWZq39wf6vSJfk5GB9bNdEA,orpat007,UgyIFwSk_Q5-h3hocD94AaABAg,1,1567353367.0,Nice information packed in a lecture under a hour.,none,,2020-07-07 23:38:14.148038,0.0
242,ng2o98k983k,http://www.youtube.com/channel/UCyUrYEL4NMBgc5smrUsqHqg,UCyUrYEL4NMBgc5smrUsqHqg,Susantha Hettiarachchi,Ugyu6915K6axRyOc4pB4AaABAg,0,1567150366.0,pip install bs4 works too.,none,,2020-07-07 23:38:14.148038,0.0
243,ng2o98k983k,http://www.youtube.com/channel/UC5ONjD558MR35vAqXW8xGqw,UC5ONjD558MR35vAqXW8xGqw,zackery hatch,UgzglUjP7KcnEJjCJWt4AaABAg,0,1566692557.0,How can you use this terminal in windows?,none,,2020-07-07 23:38:14.148038,1.0
244,ng2o98k983k,http://www.youtube.com/channel/UC9XZbwQXzbh03fVe4fG0_fw,UC9XZbwQXzbh03fVe4fG0_fw,Vali Valizada,UgwRNIZ5NlFL296nc7p4AaABAg,0,1566653925.0,Can we get from all news websites? please reply,none,,2020-07-07 23:38:14.148038,0.0
245,ng2o98k983k,http://www.youtube.com/channel/UC6zR3cUJeYnbDqEbPS65xOg,UC6zR3cUJeYnbDqEbPS65xOg,Pulkit Sharma,UgxoB3FJc_uTrJMv7ih4AaABAg,1,1566531985.0,thank you so much corey,none,,2020-07-07 23:38:14.148038,0.0
246,ng2o98k983k,http://www.youtube.com/channel/UCRfhYQgkSytV8Ut83u31QJg,UCRfhYQgkSytV8Ut83u31QJg,Aytekin Özhan,UgyofFLM_uIMztgLgmh4AaABAg,1,1566367416.0,"Corey, You are awesome. :)",none,,2020-07-07 23:38:14.148038,0.0
247,ng2o98k983k,http://www.youtube.com/channel/UCB-gYmbqtQ2bh-Yd4yle-0w,UCB-gYmbqtQ2bh-Yd4yle-0w,sara saber,Ugx7eMa2NExpX4_HKdZ4AaABAg,2,1566287956.0,great great great,none,,2020-07-07 23:38:14.148038,0.0
248,ng2o98k983k,http://www.youtube.com/channel/UCqWmsiHM46SagrLyu6gGXVQ,UCqWmsiHM46SagrLyu6gGXVQ,Sudheer Veturu,UgwDs38NXNAihcR7A5t4AaABAg,1,1566274571.0,This is exactly what I was looking for. Thanks mate!,none,,2020-07-07 23:38:14.148038,0.0
249,ng2o98k983k,http://www.youtube.com/channel/UCPU-iob7rpqWz7jrtaDQz7g,UCPU-iob7rpqWz7jrtaDQz7g,Amador Cuenca,UgzUzheuvnuCmMrMWJx4AaABAg,0,1565963590.0,"This is pure gold!

Said that, I did not like your approach to get the YouTube video Id. I used regex instead as it is more efficient and clean. If anyone is interested find the code below:

import re
video_id = re.search(r”embed\/(.*?)\?”, video_src).group(1)

Cheers,",none,,2020-07-07 23:38:14.148038,0.0
250,ng2o98k983k,http://www.youtube.com/channel/UCEx1lPR8ax8pL8uaHp5CFeA,UCEx1lPR8ax8pL8uaHp5CFeA,V K,UgzwQN33wIEtmQGp5v54AaABAg,1,1565918203.0,Thank you Sir!,none,,2020-07-07 23:38:14.148038,0.0
251,ng2o98k983k,http://www.youtube.com/channel/UCKFUB8aj6ZXMfY5QQk5Xi1w,UCKFUB8aj6ZXMfY5QQk5Xi1w,Hao Li,UgzXBAo_MiOjFRZH8nd4AaABAg,0,1565745073.0,"Thank you so much Corey!! This is definitely the best! But I do have one question-- when I tried to save the scraped headlines, summaries and the video links into a csv file like you did, it gave me blank rows in between. So I get 21 rows (11 + 10 blank ones) in my csv file. I work in Windows so I don't know if that is the reason.",none,,2020-07-07 23:38:14.149036,0.0
252,ng2o98k983k,http://www.youtube.com/channel/UCDyx3wRfdtWotSu92IjM14w,UCDyx3wRfdtWotSu92IjM14w,EDUROM,UgyK4x9l64Rts4Qp_V94AaABAg,1,1565719900.0,"You are a legend sir, thank you for this content",none,,2020-07-07 23:38:14.149036,0.0
253,ng2o98k983k,http://www.youtube.com/channel/UCTXr-K3WYUGHDZCC78s8EmA,UCTXr-K3WYUGHDZCC78s8EmA,jae seung lee,UgzPFCF2slzGA-u_Ct14AaABAg,2,1565711545.0,"now, how do we go about scraping multiple pages ? do we loop the request method? say page/1 page/2...",none,,2020-07-07 23:38:14.149036,1.0
254,ng2o98k983k,http://www.youtube.com/channel/UCCGTcUgJFRoD3nS1SupEopQ,UCCGTcUgJFRoD3nS1SupEopQ,DEEPAK PRATAP,UgyXPEPft9XRtlgBun54AaABAg,1,1565592069.0,Hey @Corey Schafer...It's a great video...I learned lots of here...I am wondering the scraper give us result for page 1 only...is that possible it can scrape all pages in a site...Please provide some advice or any link so that can be helpful..Cheers :) Thanks a lot again..,none,,2020-07-07 23:38:14.149036,1.0
255,ng2o98k983k,http://www.youtube.com/channel/UCklgYn33HBw8aEkCzZjiaEg,UCklgYn33HBw8aEkCzZjiaEg,Esteban Licea,UgxPVjZbfAsShVrt-lN4AaABAg,1,1565587702.0,Amazing video! Gave me some insight on a project of my own! Thank you !!!,none,,2020-07-07 23:38:14.149036,0.0
256,ng2o98k983k,http://www.youtube.com/channel/UC5LXKKqTwm8tdq9UxrVUTgw,UC5LXKKqTwm8tdq9UxrVUTgw,William Reed,Ugxchehl1Lcfqti_Ysx4AaABAg,0,1565491725.0,Is it possible to have a program find out how many people visit a certain web page that you don’t own?,none,,2020-07-07 23:38:14.149036,0.0
257,ng2o98k983k,http://www.youtube.com/channel/UCHuMwIL1T9gfDK9fO7FoPxQ,UCHuMwIL1T9gfDK9fO7FoPxQ,Alex Supremo,Ugw9QUmS2Ejci9CDVkJ4AaABAg,0,1565278626.0,Wouldn't it be MUCH more exciting to start the video with scarping sale prices or discounts from another website?..,none,,2020-07-07 23:38:14.149036,0.0
258,ng2o98k983k,http://www.youtube.com/channel/UCt1-ArmzwOowzIubjDnsgSQ,UCt1-ArmzwOowzIubjDnsgSQ,Sensei Bunny,UgxzXLF64_mLYd4avf94AaABAg,1,1565266386.0,thx very good video with nice explanation!,none,,2020-07-07 23:38:14.149036,0.0
259,ng2o98k983k,http://www.youtube.com/channel/UCj1XGPyw8WyCyecL9cwAAYA,UCj1XGPyw8WyCyecL9cwAAYA,Jitendra Raghuwanshi,UgweM4VyFT33anNPNu54AaABAg,1,1565005551.0,I OWE YOU MAN BIG TIME,none,,2020-07-07 23:38:14.149036,0.0
260,ng2o98k983k,http://www.youtube.com/channel/UCGNVlCKOVDSrDQiNGtahYRQ,UCGNVlCKOVDSrDQiNGtahYRQ,Adebayo Oladipo,UgykncGPSA7vzl2cr9l4AaABAg,1,1564936594.0,"Good job, bro!",none,,2020-07-07 23:38:14.149036,0.0
261,ng2o98k983k,http://www.youtube.com/channel/UC11O5TJqUKI787Cl9rmtVrQ,UC11O5TJqUKI787Cl9rmtVrQ,Pawan Blaze,Ugyw6seAgkJJWvBdJMF4AaABAg,1,1564812150.0,How can I scrape from a website with multiple webpages? The webpages have an identical template.,none,,2020-07-07 23:38:14.149036,0.0
262,ng2o98k983k,http://www.youtube.com/channel/UCeizCnO9mNo6PqMoPDbUnUQ,UCeizCnO9mNo6PqMoPDbUnUQ,MasterJi & Friends,Ugxj9G0UeVN6KODSjfB4AaABAg,0,1564745038.0,"Thank you very much corey. Due to this video, I am able to build my first web-scrapper. How can I scrap the whole website? Do you have any idea how to do that?",none,,2020-07-07 23:38:14.149036,0.0
263,ng2o98k983k,http://www.youtube.com/channel/UCXW69gF_6TVdZLdFGLnzpcQ,UCXW69gF_6TVdZLdFGLnzpcQ,Ultrakicks,UgxafPduHT1riFqyW-R4AaABAg,1,1564703477.0,This was a wonderful video that helped me understand the basic of web scrapping with Python. Thank for you so much for the in-depth tutorial!!,none,,2020-07-07 23:38:14.150033,0.0
264,ng2o98k983k,http://www.youtube.com/channel/UCFr1TTG8cKf9rGomtK7yOIQ,UCFr1TTG8cKf9rGomtK7yOIQ,Amit Anand,UgyPcFjJmiskUbcxVl14AaABAg,2,1564582179.0,"Corey Schafer : I don't know if Python is easy or you made the Python Learning very easy.I did watch almost all your videos till date and refer my friend to do so.
Thank you for making our live easier with such a ease of learning🙏",none,,2020-07-07 23:38:14.150033,0.0
265,ng2o98k983k,http://www.youtube.com/channel/UCKrVYSvCRe5L30gZRG_PqkQ,UCKrVYSvCRe5L30gZRG_PqkQ,Arafat Khan,UgyEfc9JlQ9LKO7JT6R4AaABAg,0,1564462212.0,Your videos are just so high quality and easy to grasp. Thanks for these. But I had a question. Can you explain what are CSS selectors? And how they might be used?,none,,2020-07-07 23:38:14.150033,0.0
266,ng2o98k983k,http://www.youtube.com/channel/UCEcFuTxJdcyE4g_-Ya_VaWw,UCEcFuTxJdcyE4g_-Ya_VaWw,Elliyahu Rosha,Ugy8E-WTyE7eXr91M6h4AaABAg,1,1564399143.0,Perfect!,none,,2020-07-07 23:38:14.150033,0.0
267,ng2o98k983k,http://www.youtube.com/channel/UC4xTdlbu7bs2VIwIROAMxGw,UC4xTdlbu7bs2VIwIROAMxGw,Shilpa Patil Solapure,UgxctCWsgDo3LXwerOl4AaABAg,0,1563937493.0,"Below is also a great article on Web Scraping using Python & Beautiful Soup and what we can do with scrapped data. Complete Source Code is also provided.
https://www.opencodez.com/web-development/web-scraping-using-beautiful-soup-part-1.htm",none,,2020-07-07 23:38:14.150033,0.0
268,ng2o98k983k,http://www.youtube.com/channel/UC5HgNxFWcINg81ynYZ39_mA,UC5HgNxFWcINg81ynYZ39_mA,Ali Mansour,Ugy_C9--zi0O_jlJuKR4AaABAg,1,1563908208.0,"First let thank you for precise and great explanation for all the subjects you have covered.
I have a question too! regarding beautifulsoup4.
I am looking at a website that has three divs with the same name, how can I skip over one and choose the second or skip over the first two and choose the third one.
Thank you again.",none,,2020-07-07 23:38:14.150033,2.0
269,ng2o98k983k,http://www.youtube.com/channel/UC0lkIJqYpB6xsd7enMGCj6g,UC0lkIJqYpB6xsd7enMGCj6g,Patrick Vanegas,Ugyq8_itbmIMBm-EqA54AaABAg,0,1563486249.0,"hey corey, can you make a video for scraping pdfs?",none,,2020-07-07 23:38:14.150033,0.0
270,ng2o98k983k,http://www.youtube.com/channel/UCndfaEdlESXrEN8nGRyAAGg,UCndfaEdlESXrEN8nGRyAAGg,Amy Sandstrum,Ugw9Vhc3o8NOfDsDZi14AaABAg,3,1563461430.0,"Thanks! I have very limited knowledge of html but was able to scrape price information off of websites. Just for funsies :D 
Really, all i did was tweak it and instead of div they use span class, so i switched 'div' with 'span' and got the same result.",none,,2020-07-07 23:38:14.150033,0.0
271,ng2o98k983k,http://www.youtube.com/channel/UC4U7hkyW0dahbn9HgluttPw,UC4U7hkyW0dahbn9HgluttPw,Deniz Utku Aktürk,UgyBd1JocIBOAG1tXi54AaABAg,1,1563258026.0,"clean, simple, direct, effective. a bitchass tutorial. thank you.",none,,2020-07-07 23:38:14.150033,2.0
272,ng2o98k983k,http://www.youtube.com/channel/UCkJ34IauJDD7TZXqExZqr8g,UCkJ34IauJDD7TZXqExZqr8g,Fábio Machado,UgwkFqjxLOcnXedk1bJ4AaABAg,0,1563229715.0,"Este foi o melhor vídeo de web scraping que assisti em todo o YouTube, explicações simples, diretas e eficientes. Muito obrigado por compartilhar!",none,,2020-07-07 23:38:14.150033,0.0
273,ng2o98k983k,http://www.youtube.com/channel/UCFrofStkSAT-VNh7VIAsUJA,UCFrofStkSAT-VNh7VIAsUJA,Qianli Ma,Ugy_3dG2px7FcFEPJsF4AaABAg,3,1563227079.0,"I spent 1 hour to figure out why my outcome is different from yours, then I finally realized it's because your website content has changed!",none,,2020-07-07 23:38:14.151031,0.0
274,ng2o98k983k,http://www.youtube.com/channel/UCTv_BFU5aWW3yC63At2iYHQ,UCTv_BFU5aWW3yC63At2iYHQ,robin vermillion,UgwC05O4PBElOHqpPuh4AaABAg,0,1563040153.0,"<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN"" ""http://www.w3.org/TR/html4/strict.dtd"">
<html>

 <head>

  <title>

   This domain may be for sale

  </title>

  <meta content=""This domain may be for sale"" name=""description""/>

  <meta content=""domain for sale"" name=""keywords""/>

 </head>

 <frameset border=""0"" rows=""100%,*"">

  <frame frameborder=""0"" src=""https://form.jotform.com/81154656345257""/>

 </frameset>

</html>



Traceback (most recent call last):

  File ""untitled1"", line 15, in <module>

    summary = article.find('div', class_='entry-content').p.text

AttributeError: 'NoneType' object has no attribute 'find'
not sure why it does this",none,,2020-07-07 23:38:14.151031,0.0
275,ng2o98k983k,http://www.youtube.com/channel/UC02Ji42nvLPBrA8OxaamCtQ,UC02Ji42nvLPBrA8OxaamCtQ,Jeffrey Zhao,UgyGZAEvHMZ7AI_KDox4AaABAg,0,1562964778.0,why not use a with as statement for writing the csv file?,none,,2020-07-07 23:38:14.151189,0.0
276,ng2o98k983k,http://www.youtube.com/channel/UCwkHh_On9zUDddwRXs9_5aA,UCwkHh_On9zUDddwRXs9_5aA,Дмитрий Ходыкин,UgzK9mFk_NyeWM1VMMN4AaABAg,1,1562948376.0,"Thank you, for this great video-tutorial",none,,2020-07-07 23:38:14.151189,0.0
277,ng2o98k983k,http://www.youtube.com/channel/UC-Sy_50z_GC10la8gSrpubg,UC-Sy_50z_GC10la8gSrpubg,Mohammad Tsalis,Ugxzy17Z-B5hWLxP14Z4AaABAg,0,1562935369.0,what about detecting if an alert is shown to the web and I want to get the text from the alert (javascript alert)?,none,,2020-07-07 23:38:14.151189,0.0
278,ng2o98k983k,http://www.youtube.com/channel/UCzkWT6lynpqlizTrMj-fNuQ,UCzkWT6lynpqlizTrMj-fNuQ,rocky bhai,Ugy3on08uK_ihr6fr_54AaABAg,0,1562890401.0,Sir I am using your website for scrapping. I am your student sir. please don't take any action.,none,,2020-07-07 23:38:14.151189,0.0
279,ng2o98k983k,http://www.youtube.com/channel/UCkff0qk-Ld6HheCD_NhYJvw,UCkff0qk-Ld6HheCD_NhYJvw,Imtiaz Shahed,UgwNHnQnkH4UwAW-bjZ4AaABAg,0,1562856335.0,nice video. I like this. Would like to know some field where we could apply this scrapping tech. and secure the job. Thank you for the video,none,,2020-07-07 23:38:14.151189,0.0
280,ng2o98k983k,http://www.youtube.com/channel/UCBJ1H_j9LV0dMZWoBD_G4_A,UCBJ1H_j9LV0dMZWoBD_G4_A,Robin KH,UgzwSukBdwvYmbcUowh4AaABAg,0,1562750601.0,Not very easy to follow this example when it involves simple.html which we do not have access to,none,,2020-07-07 23:38:14.151189,1.0
281,ng2o98k983k,http://www.youtube.com/channel/UCBZr3X_umgeSnUftDmwC0fQ,UCBZr3X_umgeSnUftDmwC0fQ,Moha Ham,UgxR5RTCNJGQjZXDZBB4AaABAg,1,1562707234.0,very very informative thanx alot,none,,2020-07-07 23:38:14.151189,0.0
282,ng2o98k983k,http://www.youtube.com/channel/UC8qbSysa8k2c-pif5Y3gAxg,UC8qbSysa8k2c-pif5Y3gAxg,Young Star,UgwJFKEuacLD-l8aaiJ4AaABAg,0,1562616354.0,How would you get an image from a website?,none,,2020-07-07 23:38:14.152029,1.0
283,ng2o98k983k,http://www.youtube.com/channel/UCrGVJScusxke7PZTQb1SiJg,UCrGVJScusxke7PZTQb1SiJg,prince Ps,Ugw4AlEr9TOEOUENb7p4AaABAg,0,1562420448.0,"sir i am trying to get <div class = ""col-xs-12""> 
from this website       ""https://www.gartner.com/reviews/review/view/634478""  and i tried 


response = requests.get('https://www.gartner.com/reviews/review/view/634478').text


soup = BeautifulSoup(response,""lxml"")


article = soup.find('div' , class_ = 'col-xs-12')


but i am getting NoneType object please help!",none,,2020-07-07 23:38:14.152029,0.0
284,ng2o98k983k,http://www.youtube.com/channel/UCqRfEstBdD8ArCuGj04Allg,UCqRfEstBdD8ArCuGj04Allg,Adam Attia,UgwAom6L78TI_6sYwad4AaABAg,0,1562386144.0,"Thank you so much! Hope I didn't ping your website too much while following around with this tutorial, was a massive help :)",none,,2020-07-07 23:38:14.152307,0.0
285,ng2o98k983k,http://www.youtube.com/channel/UC99PbRcpV9WOHeSc-HZyg-g,UC99PbRcpV9WOHeSc-HZyg-g,Aymen Lagha,Ugzl_HbUz0UDK91h5Y94AaABAg,2,1562361226.0,when I was here 25:10 I was like that's not the headline that I got then I realised that he updated his site and then I just felt stupid lol,none,,2020-07-07 23:38:14.152307,0.0
286,ng2o98k983k,http://www.youtube.com/channel/UCXm445iKqJi9v6SMNERQKpQ,UCXm445iKqJi9v6SMNERQKpQ,varun Kulshrestha,Ugx69Z8pgqrnYRSlBbp4AaABAg,0,1562171600.0,"sir i tried to convert your website data scrapped into  csv by i am getting an error in the corey.csv file as
web scraping/corey.csv is not UTF encoded
saving disabled
see console for more details",none,,2020-07-07 23:38:14.152307,0.0
287,ng2o98k983k,http://www.youtube.com/channel/UCs2V1tb3RRrfvjadErhakQQ,UCs2V1tb3RRrfvjadErhakQQ,jen katan,UgxZVwMyFixsMR9UKXJ4AaABAg,0,1561835162.0,"Thanks for the thorough explanation. I'm a noob that just discovered your videos, instant subscribe. 
I'm using this to scrape data from my tumblr. The url changes anytime I navigate to another page. is there any way to scrape the entire thing without changing the url to website.com/page/2.../page/3....etc",none,,2020-07-07 23:38:14.152307,0.0
288,ng2o98k983k,http://www.youtube.com/channel/UCphxSG3OusJCRuzqnOhBJDA,UCphxSG3OusJCRuzqnOhBJDA,Amit Jajoo,UgwDB5pW09n95Kwjt394AaABAg,1,1561819758.0,thx sir for th8is beautiful content,none,,2020-07-07 23:38:14.152307,0.0
289,ng2o98k983k,http://www.youtube.com/channel/UCFIN7ADNciE2wcW-XuJ1gmw,UCFIN7ADNciE2wcW-XuJ1gmw,Sam Ha,UgxHGygCPlA1Kv2S-AV4AaABAg,0,1561477828.0,Great work as always. But how can you interact with the webpage to go-to the next page? I have tried selenium but it's pretty slow. Is it possible to use the request lib to click on the links? Thanks,none,,2020-07-07 23:38:14.152307,0.0
290,ng2o98k983k,http://www.youtube.com/channel/UCFn0Xdc42vbeeJxowzMdtiw,UCFn0Xdc42vbeeJxowzMdtiw,Yeyzon,UgxsVkenAkKptdSVH9x4AaABAg,1,1561401646.0,"This is great, thank you!",none,,2020-07-07 23:38:14.152307,0.0
291,ng2o98k983k,http://www.youtube.com/channel/UC2_nbmuzRyXYvgEzaoqigkQ,UC2_nbmuzRyXYvgEzaoqigkQ,llsniperll men,UgxddHoSAFYtK1INadh4AaABAg,0,1561334453.0,Maybe this would help to understand everything about scraping https://websitescrapingtutorials.wordpress.com/2019/06/23/how-to-scrape-yelp/,none,,2020-07-07 23:38:14.152307,0.0
292,ng2o98k983k,http://www.youtube.com/channel/UCii97y8RKNJTsGxrVdeFN6Q,UCii97y8RKNJTsGxrVdeFN6Q,Lawrence Krukrubo,UgzodOGJwwtAuCONVn94AaABAg,2,1561317343.0,"This is just amazing, your knowledge, no your practical knowledge is amazing",none,,2020-07-07 23:38:14.153025,0.0
293,ng2o98k983k,http://www.youtube.com/channel/UCRd7ibvWZDHAILwcJ0VupPw,UCRd7ibvWZDHAILwcJ0VupPw,env0y W,Ugwz8kxKFCaY64YgN0B4AaABAg,0,1561201841.0,how  to get the site need Login first，thanks alot,none,,2020-07-07 23:38:14.153025,0.0
294,ng2o98k983k,http://www.youtube.com/channel/UCVlL9WbbgN1uysrz42AH56A,UCVlL9WbbgN1uysrz42AH56A,story factory,UgzsafvfdZsIkG4Jn794AaABAg,0,1561181594.0,I want to scrap youtube video subtitles can u help me,none,,2020-07-07 23:38:14.153025,0.0
295,ng2o98k983k,http://www.youtube.com/channel/UCbcTndruJyS_mUqv603C7mw,UCbcTndruJyS_mUqv603C7mw,Shubham Bansal,UgyiKmzEkZGh5hYYmEN4AaABAg,3,1560930973.0,Corey a Big Bow from India..... really you are too amazing,none,,2020-07-07 23:38:14.153025,0.0
296,ng2o98k983k,http://www.youtube.com/channel/UC-ATXXSithiy9TdhKcBv9EQ,UC-ATXXSithiy9TdhKcBv9EQ,Patriotic Ghanaian,Ugy4On3rwAvTL3ylOCJ4AaABAg,0,1560907176.0,"Great tutorial as always. Just one issue though: I encountered an encoding error looking like this:
 UnicodeEncodeError: 'ascii' codec can't encode character u'\u2013' in position 29: ordinal not in range(128)
I solved that by using the encode(utf8) to encode the html text to utf-8 format like this:

#For the headline
headline = article.h2.a.text.encode('utf8')
#For the summary
summary = article.find('div', class_='entry-content').p.text.encode('utf8')

The rest is the same! Keep up the good work man!",none,,2020-07-07 23:38:14.153025,0.0
297,ng2o98k983k,http://www.youtube.com/channel/UCdzv-qX5Uuob_9NC5kFA0EA,UCdzv-qX5Uuob_9NC5kFA0EA,Rohith P,UgzNaY4SAHgOQBYHaQJ4AaABAg,0,1560743862.0,why can't we scrape complete amazon page without headers?,none,,2020-07-07 23:38:14.153025,0.0
298,ng2o98k983k,http://www.youtube.com/channel/UCj9hcRsV4siCBd6CLR86hRA,UCj9hcRsV4siCBd6CLR86hRA,Mazikeen,Ugwk-LK2kHcvn8y47dp4AaABAg,1,1560575347.0,"I used requests and BeautifulSoup to write a python script, that allows you to scrape all historical data of a cryptocurrency by simply putting in the name of currency.
https://github.com/gitFaisal/crypto_currency_scraper",none,,2020-07-07 23:38:14.153025,0.0
299,ng2o98k983k,http://www.youtube.com/channel/UCdZknrMbN3o396rEO7sW7Fw,UCdZknrMbN3o396rEO7sW7Fw,Tal Barak,UgwPFmenKiW2g8YfkSh4AaABAg,0,1560487524.0,"When trying this in Visual Studio Code, I got ""no bs4 module could be found"". So, I used ""pip install bs4"".",none,,2020-07-07 23:38:14.154023,0.0
300,ng2o98k983k,http://www.youtube.com/channel/UCEOIPeIzegX02bFDmCY_6kA,UCEOIPeIzegX02bFDmCY_6kA,Aryan Chauhan,UgwIM44b9ELxe_Hw2gh4AaABAg,1,1560415216.0,"thanks, man through this I am able to scrap live cricket scores  from cricbuzz.com",none,,2020-07-07 23:38:14.691097,0.0
301,ng2o98k983k,http://www.youtube.com/channel/UClUCdGN-RdhYfYMapiFjrMA,UClUCdGN-RdhYfYMapiFjrMA,Mani Srujan,Ugz30XgxT7PfDgJOM5d4AaABAg,0,1560409924.0,"I have a doubt
why does sometimes,the downloaded HTML file using requests look different from code in inspect elements ?",none,,2020-07-07 23:38:14.691097,0.0
302,ng2o98k983k,http://www.youtube.com/channel/UClGJ1EaOsCfKMZ_ETBjiFPg,UClGJ1EaOsCfKMZ_ETBjiFPg,lardosian,UgzmLaAyVq0Pzye81HJ4AaABAg,0,1560362284.0,Is this an alternative way of getting data as opposed to an API for example?,none,,2020-07-07 23:38:14.691097,0.0
303,ng2o98k983k,http://www.youtube.com/channel/UCKZgkCqgvvCtM2MXawWATuQ,UCKZgkCqgvvCtM2MXawWATuQ,Akshay Bhadange,Ugx1xs0UTwrOHqMOKQ54AaABAg,1,1560190276.0,"thanks sir
very helpful",none,,2020-07-07 23:38:14.691097,0.0
304,ng2o98k983k,http://www.youtube.com/channel/UCE3Gdhl2voXdOKTsasvMDBA,UCE3Gdhl2voXdOKTsasvMDBA,Nick Gibra,UgzT2N_pEnqd0McVbFl4AaABAg,2,1560021108.0,Thank you very much for this tutorial. Your explanations were better than some courses and books that I tried. Thank you again.,none,,2020-07-07 23:38:14.691097,0.0
305,ng2o98k983k,http://www.youtube.com/channel/UCUInuCsLup9-pimOjA5WG0A,UCUInuCsLup9-pimOjA5WG0A,Jester j,Ugz0P1kQ9B9mIZs6PGx4AaABAg,1,1559903018.0,Thank you so much.   You answered several of my questions and literally saved my ‘avoiding boredom’ project,none,,2020-07-07 23:38:14.691097,0.0
306,ng2o98k983k,http://www.youtube.com/channel/UCq68Vyex2f4hpKVFh16bQsw,UCq68Vyex2f4hpKVFh16bQsw,Harshit Khandelwal,UgwxuhmeEFFnUb0sOVN4AaABAg,2,1559651234.0,"East and West, Corey Schafer is the Best!",none,,2020-07-07 23:38:14.691097,0.0
307,ng2o98k983k,http://www.youtube.com/channel/UCi8EBLA7_I4MPVViuMJAPbA,UCi8EBLA7_I4MPVViuMJAPbA,Virtual Reality,UgzkmLra-kfk_pk-T_94AaABAg,2,1559640181.0,Thanks man.,none,,2020-07-07 23:38:14.691097,0.0
308,ng2o98k983k,http://www.youtube.com/channel/UC1ycuQHWIG9DdAIe9S7SLNw,UC1ycuQHWIG9DdAIe9S7SLNw,Carlos Matos Fanpage,UgzxSxD2Wfsr_OZNvgl4AaABAg,1,1559592403.0,Is this legal,none,,2020-07-07 23:38:14.691097,0.0
309,ng2o98k983k,http://www.youtube.com/channel/UCcSQcNjVDpmUWiECcWtqGWg,UCcSQcNjVDpmUWiECcWtqGWg,Mg,Ugzu3tAXZzkaWkYiT4d4AaABAg,1,1559247181.0,nice,none,,2020-07-07 23:38:14.691097,0.0
310,ng2o98k983k,http://www.youtube.com/channel/UCF4esw93wYKy2Lttxta6cJg,UCF4esw93wYKy2Lttxta6cJg,Avinash Kumar,UgwVNbltp1OB4UyvWqZ4AaABAg,1,1559149081.0,thanks a lot,none,,2020-07-07 23:38:14.691097,0.0
311,ng2o98k983k,http://www.youtube.com/channel/UCAQ1n1WJZCqY8uqnHstdbuA,UCAQ1n1WJZCqY8uqnHstdbuA,skhanal123,UgwPaLAI6-bgzvPMALB4AaABAg,1,1559139552.0,Thank you for making this video. It is really easy to understand web scraping for beginners with this video.,none,,2020-07-07 23:38:14.691097,0.0
312,ng2o98k983k,http://www.youtube.com/channel/UCm6OWW7nq5q_gkISxBJgtXA,UCm6OWW7nq5q_gkISxBJgtXA,Ali,UgxhAVNdQZw_13z3J2l4AaABAg,2,1558976829.0,"Corey.. after watching this scraping tutorial.. im more liking these scraping techniques..
You are an Excellent Tutor :)",none,,2020-07-07 23:38:14.692045,0.0
313,ng2o98k983k,http://www.youtube.com/channel/UC-NM5AOKBmCdXJJ7ZKTYHuA,UC-NM5AOKBmCdXJJ7ZKTYHuA,Shaz Kingdom,Ugy-snxYMiwvLqajR3x4AaABAg,1,1558949446.0,""" 😮 😉 👌 😮 😉 Beautiful 😮 😉 👌 😮 😉 """,none,,2020-07-07 23:38:14.692045,0.0
314,ng2o98k983k,http://www.youtube.com/channel/UCXs5hizZ8rExXeZgioFCu0g,UCXs5hizZ8rExXeZgioFCu0g,Ram Mahour,UgxEwPmtvJCAePMWWKR4AaABAg,0,1558851834.0,Thanks A Lot,none,,2020-07-07 23:38:14.692045,0.0
315,ng2o98k983k,http://www.youtube.com/channel/UCz8L2weBwF4voy-d6Xk0CvQ,UCz8L2weBwF4voy-d6Xk0CvQ,Nima Mohammadi,UgwhzON2iwZG0aRGE5p4AaABAg,1,1558362625.0,WOW,none,,2020-07-07 23:38:14.692045,0.0
316,ng2o98k983k,http://www.youtube.com/channel/UCUm-SGfLVIRFMzd_4QYjPyQ,UCUm-SGfLVIRFMzd_4QYjPyQ,Gabriel Félix,Ugy9o9qdfta4N9nxyV14AaABAg,1,1558298580.0,"Excellent video, very helpful and easy to follow. Many thanks!",none,,2020-07-07 23:38:14.692045,0.0
317,ng2o98k983k,http://www.youtube.com/channel/UC4UgSZu5YCMEOgPtBFhQCbA,UC4UgSZu5YCMEOgPtBFhQCbA,Shahriar Kabir Tashik,Ugz1S6XUI8_ceP8O3lp4AaABAg,1,1558195667.0,Awesome! Thanks a lot!,none,,2020-07-07 23:38:14.692045,0.0
318,ng2o98k983k,http://www.youtube.com/channel/UC1ms0k9jUBaa3tC44klDCHg,UC1ms0k9jUBaa3tC44klDCHg,Samantha S,UgwaU2yYXu_J3BkgDMR4AaABAg,5,1558075722.0,"Whenever I take long breaks from Python, I tend to forget a lot of stuff I learned... When I come to you're videos, it really helps refresh my memory!!

Thank you soo much! :)",none,,2020-07-07 23:38:14.692045,1.0
319,ng2o98k983k,http://www.youtube.com/channel/UCkff0qk-Ld6HheCD_NhYJvw,UCkff0qk-Ld6HheCD_NhYJvw,Imtiaz Shahed,UgwVRdhNI4TTdJi_aKp4AaABAg,33,1557952860.0,"Corey Schafer Thanks a million. best tutorial I have ever seen on Web Scrapping in Python, No one explain like you in a very single point of code. That's how we learn. great job and keep it up.",none,,2020-07-07 23:38:14.692045,0.0
320,ng2o98k983k,http://www.youtube.com/channel/UCuEHm_bi45mCHcy50zt6HqQ,UCuEHm_bi45mCHcy50zt6HqQ,Max Kiley,UgwwqNRl3C9-g5WppyN4AaABAg,1,1557707376.0,Your a great teacher,none,,2020-07-07 23:38:14.692045,0.0
321,ng2o98k983k,http://www.youtube.com/channel/UCW3wQXPFloC4-kCN2X8i0gw,UCW3wQXPFloC4-kCN2X8i0gw,Ozkee,UgwZ4ndGL2Y7Cn-EU_N4AaABAg,0,1557566499.0,"HELP! couldn't seperate columns. it just shows commas between headline,summary,video_links. Anybody ? i guess it's because of old version excel :/",none,,2020-07-07 23:38:14.692045,0.0
322,ng2o98k983k,http://www.youtube.com/channel/UCW3wQXPFloC4-kCN2X8i0gw,UCW3wQXPFloC4-kCN2X8i0gw,Ozkee,Ugzkj7cgwiqhdXR4UJd4AaABAg,0,1557565277.0,thank you so much!,none,,2020-07-07 23:38:14.692045,0.0
323,ng2o98k983k,http://www.youtube.com/channel/UCQMEHsslFDiKlOcvr_6no1w,UCQMEHsslFDiKlOcvr_6no1w,FABRÍCIO CASTRO,Ugzi0nH9w2twAviljLh4AaABAg,0,1557327558.0,I've tried in some urls and i've got some errors...,none,,2020-07-07 23:38:14.692045,1.0
324,ng2o98k983k,http://www.youtube.com/channel/UCetcLQtjmZNURFf46kYOoKQ,UCetcLQtjmZNURFf46kYOoKQ,Deboparna Banerjee,UgyYbJxwDSY9g7xhv294AaABAg,0,1557163637.0,"how to scrape if even the class is same, no other detail is given and it is under the same tag?",none,,2020-07-07 23:38:14.692045,0.0
325,ng2o98k983k,http://www.youtube.com/channel/UCKv_yIqqkisaz0tHu3K6ADQ,UCKv_yIqqkisaz0tHu3K6ADQ,Harsh Shah,UgzshsdAl_oi5LstRa94AaABAg,0,1556904787.0,"Hi, can you also post a tutorial about how to use the public APIs you mentioned in the end?",none,,2020-07-07 23:38:14.692045,0.0
326,ng2o98k983k,http://www.youtube.com/channel/UCTJQnKbgyImMDDNGKOzdOyw,UCTJQnKbgyImMDDNGKOzdOyw,HumanStartups.com,Ugz3kwdNPO-HG7xU04t4AaABAg,0,1556675906.0,"I have an excel code below that scrape morningstar for apple financial information, so I can I convert it into python? Thanks


=arrayformula(SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(IMPORTHTML(""http://financials.morningstar.com/finan/financials/getFinancePart.html?t=AAPL&region=usa&culture=en-US&ops=clear"",""table"", 1), ""<\/td>"" , """" ),""<\/tr>"",""""),""<\/th>"",""""),""<\/thead>"",""""),""<\/span>"",""""))",none,,2020-07-07 23:38:14.693074,0.0
327,ng2o98k983k,http://www.youtube.com/channel/UCW8M3e3XxdxcuqOq2yHCOqg,UCW8M3e3XxdxcuqOq2yHCOqg,Kiran Rao,UgyZKY2_C2IHYiA8NUl4AaABAg,0,1556604231.0,Any sample websites to work with just to learn and explore more.... Almost every website deny scraping their website..,none,,2020-07-07 23:38:14.693074,0.0
328,ng2o98k983k,http://www.youtube.com/channel/UC1HCA1CCyBFtfWr8u9Gg5Zw,UC1HCA1CCyBFtfWr8u9Gg5Zw,maraffio72,UgzGU8bKji0nknY0-jV4AaABAg,0,1556577658.0,"Great video. Thank you.
I am wondering if there are any specific techniques to scrape websites which include dynamic content generated by interacting with user input (e.g. search text, drop down cells)",none,,2020-07-07 23:38:14.693074,0.0
329,ng2o98k983k,http://www.youtube.com/channel/UCEUUl0PfYSSJ57DYutiVFiw,UCEUUl0PfYSSJ57DYutiVFiw,Sai Kiran,UgzYbxvfuofFf5hNCBF4AaABAg,0,1556516494.0,"Hi Corey, can you make an extended version to this video by caching the requested data and storing in various formats such as python dict, databases and other",none,,2020-07-07 23:38:14.693074,0.0
330,ng2o98k983k,http://www.youtube.com/channel/UCnLPEEb5sXZgDlwHbtRedzg,UCnLPEEb5sXZgDlwHbtRedzg,Nebuer,UgyNzF9_fl3WT5B9W_x4AaABAg,2,1556146486.0,"I just loved the way you explained every bit about this topic. Crisp, clear and to the point. :) I have subscribed your channel for more content.",none,,2020-07-07 23:38:14.693074,0.0
331,ng2o98k983k,http://www.youtube.com/channel/UCQyUQ6IYYtWlavZHeWN2rGg,UCQyUQ6IYYtWlavZHeWN2rGg,Efqan Mustafayev,Ugxkn_Cka9xa-XSxoKV4AaABAg,3,1556097876.0,"Thanks for video, sir. 
   How we can scrape login required websites?",none,,2020-07-07 23:38:14.693074,1.0
332,ng2o98k983k,http://www.youtube.com/channel/UCtTealz1OnRdW0xz4BEC_Zw,UCtTealz1OnRdW0xz4BEC_Zw,Ryan Conley,Ugw3wExXSJ7f5KiT0hN4AaABAg,1,1555984212.0,"Can anyone help me?  I tried running the code but I get this error:
bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lmxl. Do you need to install a parser library?

I installed all the pip stuff in the terminal before running... I don't understand.
(I'm on MacOS and Python 3, and all the pips installed just fine.  I tried this on Python 2.7 and this doesn't work either.",none,,2020-07-07 23:38:14.693074,2.0
333,ng2o98k983k,http://www.youtube.com/channel/UCuHh-6HKGgwIpAqyvVlhUyg,UCuHh-6HKGgwIpAqyvVlhUyg,Tashdeep Singh Saluja,UgwCO-mXNJF8eGLVb2V4AaABAg,0,1555925302.0,"This tutorial was based for scraping a single web page,do you have any tutorial regarding scraping the whole website?
It would be a huge help if you reply.Btw thanks this video was helpful too..",none,,2020-07-07 23:38:14.693074,1.0
334,ng2o98k983k,http://www.youtube.com/channel/UCluxCRUWnOmPc2ezWSmGNJQ,UCluxCRUWnOmPc2ezWSmGNJQ,Jeff Hunter,Ugx9aYrdrS_qA8SdtdV4AaABAg,0,1555887724.0,my data sorts into columns all screwed up butchering the strings and sometimes even mixing columns together.,none,,2020-07-07 23:38:14.693074,0.0
335,ng2o98k983k,http://www.youtube.com/channel/UCZ2qVAGR8nCDZ1zoUyoe2BA,UCZ2qVAGR8nCDZ1zoUyoe2BA,Patrick Ribeiro,UgxN18AlGHXyUq2tgAZ4AaABAg,1,1555803957.0,your video saved my life! thanks a lot!,none,,2020-07-07 23:38:14.693074,0.0
336,ng2o98k983k,http://www.youtube.com/channel/UComsvMQe7235CearQXZHIJg,UComsvMQe7235CearQXZHIJg,Farhat Ahmed,UgwtWule7zVH4DeEYa14AaABAg,1,1555436401.0,Great video very helpful !,none,,2020-07-07 23:38:14.693074,0.0
337,ng2o98k983k,http://www.youtube.com/channel/UCBt08bDTn5zW2s_fCtwtRtw,UCBt08bDTn5zW2s_fCtwtRtw,Unifono2012,UgwORbzfNThZ0GcPrbN4AaABAg,1,1555430559.0,amazing. thanks so much!,none,,2020-07-07 23:38:14.693074,0.0
338,ng2o98k983k,http://www.youtube.com/channel/UC6DtcMKODtYAr8YAEV1YC1w,UC6DtcMKODtYAr8YAEV1YC1w,Xiaohua Liu,UgxVJnOU7nF436-wvGt4AaABAg,1,1555275302.0,Great thanks for your share. It is very detail. I can watch it again and again.,none,,2020-07-07 23:38:14.693074,0.0
339,ng2o98k983k,http://www.youtube.com/channel/UCtQvhLj5n6gRoUXEUSfxFoA,UCtQvhLj5n6gRoUXEUSfxFoA,c w,Ugzii0LB2mKPUInRS1N4AaABAg,1,1555080191.0,you are incredible at teaching in a way that helps people understand,none,,2020-07-07 23:38:14.694072,0.0
340,ng2o98k983k,http://www.youtube.com/channel/UCFgQYJKMzedt2n_ABdSwpHw,UCFgQYJKMzedt2n_ABdSwpHw,Arnel Seco,UgxHUv9PqkrxLsIyaAd4AaABAg,2,1555040673.0,"I watch the video till the end, the explanation is very clear and easy to follow.
Thanks Sir for this educational video. GodBless",none,,2020-07-07 23:38:14.694072,0.0
341,ng2o98k983k,http://www.youtube.com/channel/UCvp5jq-inf8zTo5QQ_x48iA,UCvp5jq-inf8zTo5QQ_x48iA,Huzail Jamil,UgzEI2nXT4JlgBPlsPJ4AaABAg,0,1554888685.0,How can i change this code to scrap data from whole website link email address for leads ? For example i've a website with email and these emails categories by the city ? is there  a way to search all the page of a website to get the email instead of manual approach?,none,,2020-07-07 23:38:14.694072,0.0
342,ng2o98k983k,http://www.youtube.com/channel/UCJ3WNaWGfKn7GYZIxSN90Vg,UCJ3WNaWGfKn7GYZIxSN90Vg,John Corcoran,Ugyq7Akz7EkWeKUAgFN4AaABAg,0,1554719977.0,thank you for the great video. is beautiful soup compatible with Spyder & if so how do I install it?,none,,2020-07-07 23:38:14.694072,0.0
343,ng2o98k983k,http://www.youtube.com/channel/UCsiwWskyY6DZ5sOGMCK0skw,UCsiwWskyY6DZ5sOGMCK0skw,hary store,UgyEXPNgV4alzvGXp6R4AaABAg,1,1554500833.0,Excellent video very good way to explain,none,,2020-07-07 23:38:14.694072,0.0
344,ng2o98k983k,http://www.youtube.com/channel/UCLNkJV5Apj5cTv5cj11Haew,UCLNkJV5Apj5cTv5cj11Haew,prabav86,Ugz0biTryRToKI5COPJ4AaABAg,1,1554454980.0,"Hello from Canada! I've been impressed with the way you teach Python and related subjects. So I was referring/recommending your youtube videos to my friends. Recently, I'm finishing up online course on Data Science from IBM, I was pleasantly surprised that course recommended your video on webscrapping! All your hard works helping a lot of folks out there. Simply outstanding! take care",none,,2020-07-07 23:38:14.694072,0.0
345,ng2o98k983k,http://www.youtube.com/channel/UC6Acjch-lDsFj9FOGmMZNtw,UC6Acjch-lDsFj9FOGmMZNtw,Alex Paes,UgwfRILj5O6e21guQLB4AaABAg,15,1554039193.0,"It could be possible to hit ""like"" twice for ur videos! Mate, ur a legend! Congrats!",none,,2020-07-07 23:38:14.694072,0.0
346,ng2o98k983k,http://www.youtube.com/channel/UCo8P9miyqy2Loo1eh5BMIGw,UCo8P9miyqy2Loo1eh5BMIGw,FaLL_Nemesis00,Ugzglxvm7gF477Ad1z54AaABAg,0,1553958721.0,what if for a given class there are multiple <p> 's how would I approach this?,none,,2020-07-07 23:38:14.694072,2.0
347,ng2o98k983k,http://www.youtube.com/channel/UCnoEZmv6Zum2LHCp0uXYriw,UCnoEZmv6Zum2LHCp0uXYriw,shyambhu Mukherjee,UgyPu2Tf8mvPuvHfSiB4AaABAg,0,1553952596.0,visit https://shyambhu20.blogspot.com/2019/01/web-scrapping-things-you-will-not-know.html for learning web scraping in details with python codes snippets available.,none,,2020-07-07 23:38:14.694072,0.0
348,ng2o98k983k,http://www.youtube.com/channel/UCnP8hVI4ev71V_l7XnwgZOQ,UCnP8hVI4ev71V_l7XnwgZOQ,Bala Subramanian,UgzHxsegf8otdlOUGoF4AaABAg,2,1553803144.0,ONE OF THE BEST DEVELOPER I HAVE EVER SEEN in this planet! Hats Off to your skills!,none,,2020-07-07 23:38:14.694072,0.0
349,ng2o98k983k,http://www.youtube.com/channel/UCkcSMFM6tfE2ptcxeLqpKCQ,UCkcSMFM6tfE2ptcxeLqpKCQ,Tripti Jain,UgyJ5vvWt9HRQFNiQvh4AaABAg,1,1553787298.0,you are an awesome teacher,none,,2020-07-07 23:38:14.694072,0.0
350,ng2o98k983k,http://www.youtube.com/channel/UCoI0uD4AQxFuiSnwsYjwQ8Q,UCoI0uD4AQxFuiSnwsYjwQ8Q,M Schuer,UgwP0e4ZMu1UfbURABh4AaABAg,1,1553637857.0,"Corey, this video is fantastic. Thank you so much for taking the time to make this tutorial. I am starting to do some web scraping and have been using different books and methods, but your video is very easy to understand and follow.. Your excellent explanation has me starting off very comfortably. Thanks for doing this video and please keep them coming.",none,,2020-07-07 23:38:14.694072,0.0
351,ng2o98k983k,http://www.youtube.com/channel/UCSQ5B3Bib_U3v05uY7cLHuw,UCSQ5B3Bib_U3v05uY7cLHuw,Saiprasad Hakki,Ugz6wy6Dw6PyviT-RwN4AaABAg,0,1553486428.0,Python 2 or 3????,none,,2020-07-07 23:38:14.694072,1.0
352,ng2o98k983k,http://www.youtube.com/channel/UCd_13nDgEosf4JXkNN7Wmzw,UCd_13nDgEosf4JXkNN7Wmzw,Nearco1000,Ugx43imEBtnQmfcNJ854AaABAg,8,1553192003.0,"Just curious about those 34 slackers that did Thumbs down on this vid, seriously?.

It's very clear and straight forward, plain English, highly understandable .......................Thumb up....
Very appreciated #Corey, I was looking for this, great vid, again and again tku for your contribution.",none,,2020-07-07 23:38:14.694072,1.0
353,ng2o98k983k,http://www.youtube.com/channel/UC2S065GGZkCsZEAUBlVD1sQ,UC2S065GGZkCsZEAUBlVD1sQ,Jethro Cao,UgxpIYnBtP-tROlcxoZ4AaABAg,0,1552680067.0,"Corey, why aren't you using conda instead of pip to manage your packages? I see you have anaconda installed. Plus conda takes care of managing python environments for you too, so you don't need to use things like venv anymore either.",none,,2020-07-07 23:38:14.695068,2.0
354,ng2o98k983k,http://www.youtube.com/channel/UCskcrYCbJXZ_S-4KoJeqC1A,UCskcrYCbJXZ_S-4KoJeqC1A,Feanor Chris,UgwlwMJmFzMri0w0L8d4AaABAg,0,1552581608.0,"Hi, thank you for this v helpful video! One question I hope you might be able to answer - despite import csv the module 'csv' has no attribute 'writer'. I am using Pycharms and have searched around but unable to come up with any solutions apart from search and rename any 'csv.py' file in directory (which i have done). Anyone please able to help?",none,,2020-07-07 23:38:14.695068,0.0
355,ng2o98k983k,http://www.youtube.com/channel/UCCQpbTmBIZpmdQ4ITdz8Siw,UCCQpbTmBIZpmdQ4ITdz8Siw,shubham saxena,UgwAkBcHgRmrqHmENg14AaABAg,0,1552319852.0,How to scrap LinkedIn data using python,none,,2020-07-07 23:38:14.695068,0.0
356,ng2o98k983k,http://www.youtube.com/channel/UCr-T0Bk4BMQLPaCKJxBH5Mg,UCr-T0Bk4BMQLPaCKJxBH5Mg,Shubham Tiwari,UgzvqE5HfxdPL82PvSZ4AaABAg,0,1552181627.0,How can we do the YouTube video scraping?,none,,2020-07-07 23:38:14.695068,0.0
357,ng2o98k983k,http://www.youtube.com/channel/UCpbio6m377ewfyCXn9g_3jw,UCpbio6m377ewfyCXn9g_3jw,Okan Ozdemir,Ugx-LS4WvqPmoHw-pnN4AaABAg,0,1552062075.0,Can you make a quiz app through Flask web application or how to use best python on web?,none,,2020-07-07 23:38:14.695068,0.0
358,ng2o98k983k,http://www.youtube.com/channel/UCpffhtVGH0bb9cv2HNBaTEg,UCpffhtVGH0bb9cv2HNBaTEg,LUCAS LAU,Ugy8iK6EZoUSjT_Qa614AaABAg,0,1551875862.0,"<td class=""dojoxGridCell"" idx=""3"" role=""gridcell"" style=""width:40%;"" tabindex=""-1"">
               LOWER GROUND FL. PLAN </td>


if it's like this how to I scrape only the text?    


I tried:
 drawing_no = article.find_all('td', class_='dojoxGridCell', idx='3')
 print(drawing_no)",none,,2020-07-07 23:38:14.695068,0.0
359,ng2o98k983k,http://www.youtube.com/channel/UCDAOD3X4wJn8yGfj27tyidQ,UCDAOD3X4wJn8yGfj27tyidQ,Frank Conte,UgxxgSJPw8GKUT4oJaB4AaABAg,0,1551363610.0,"As always, I appreciate your great videos. I'm happy to be Patreon patron! I was wondering if you could help me with an issue. Using this tutorial as a model I tried to scrape code on my own website. However, even after closely following you, I could not replicate your python script. Here is the code I am trying to scrape (particularly, the elements ""h2"", ""pbanner"" and the link which is  served from the class ""button big""
 
<article data-position=""left"">
      <div class=""inner"">
       <img alt=""BPDNewsHEROTobin"" src=""Archives/History/HERO2-13-19c.jpg""/>
       <h2>
        BPD remembers the service and sacrifice of Officer Charles E. Deininger
       </h2>
       <p class=""pbanner"">
        Killed in the line of duty on this day, Feb. 13, 100 years ago
       </p>
       <ul class=""actions"">
        <li>
         <a class=""button big"" href=""http://bpdnews.com/news/2019/2/13/bpd-remembers-the-service-and-sacrifice-of-officer-charles-e-deininger-killed-in-the-line-of-duty-on-this-day-100-years-ago"">
          Read more
         </a>
        </li>
        .
       </ul>
      </div>
     </article>

''I get good results with the folllowing code (based on yours)
#headline = article.h2.text
print(headline)

banner = article.find('div', class_='inner').p.text
print (banner)

buttonLink = article.find('ul',class_='actions')
print (buttonLink)

What's giving me a problem is the split method that follows. I can't seem to determine how best to split the webpage data. I get a nonetype object is not callable error. Any advice is appreciated. The most recent Requests tutorial was most helpful in learning how to pull data.",none,,2020-07-07 23:38:14.695068,5.0
360,ng2o98k983k,http://www.youtube.com/channel/UCADxxXFcIokAkVPAAspVfdQ,UCADxxXFcIokAkVPAAspVfdQ,Suraj Singh,UgwXzAcFoQkUEf4WsYJ4AaABAg,2,1551257699.0,"Thank You Very Much!!
Love from India!!",none,,2020-07-07 23:38:14.695068,0.0
361,ng2o98k983k,http://www.youtube.com/channel/UCwCse-_VuoIN21OYhMIMgmA,UCwCse-_VuoIN21OYhMIMgmA,Tarun Garg,UgxkrzwYdmBJiI4RmAl4AaABAg,0,1551193017.0,Thanks For Such an awesome Video !! very Well explained. One question though. The output CSV file came for me is having one empty row between two row contains information. How can I solve this?,none,,2020-07-07 23:38:14.695068,2.0
362,ng2o98k983k,http://www.youtube.com/channel/UCcCvTblYGG6770c5Tbl0h5Q,UCcCvTblYGG6770c5Tbl0h5Q,Parveen Mann,Ugyww8sBsXwZI0lYlwV4AaABAg,1,1551010227.0,"nice Mr. Schafer
its really informative",none,,2020-07-07 23:38:14.695068,0.0
363,ng2o98k983k,http://www.youtube.com/channel/UCr3dcNyai5QqDxxToYLxtEw,UCr3dcNyai5QqDxxToYLxtEw,Koushik Ahmed,UgyH1qJlvCWsbbHZIxJ4AaABAg,1,1550937684.0,very very very nice video.,none,,2020-07-07 23:38:14.695068,0.0
364,ng2o98k983k,http://www.youtube.com/channel/UC3J1J2UU9FCpTMupIp2-pGA,UC3J1J2UU9FCpTMupIp2-pGA,Psyche,UgyQ5kLDj5Cba7OmNjx4AaABAg,1,1550813446.0,Damn! I have learned a lot and the English is so clear. Your method the way you teach is awesome!. Thank you so much! :),none,,2020-07-07 23:38:14.695068,0.0
365,ng2o98k983k,http://www.youtube.com/channel/UC5BfKvTZ639iwegqM-53nNg,UC5BfKvTZ639iwegqM-53nNg,Manish Khanna,UgyMqah42v02inNcX594AaABAg,0,1550801233.0,"Your articulation and presentation is impressive. It really inspire on how one should teach. 

About the topic, I feel that getting the video id from the URL could also be done as follows - 
#Step 1 - Split source URL on '?' 
### This will give us only 2 elements. And we know that the video id will always be in first

#Step 2 - Split the first element from Step 1 based on '/'
### Here we know that the video id will always be the last element

# IMO this could relieve us from counting and hard coding the indexes. 
v_id = v_src.split('?')[0]                                          #Step 1
v_id = v_id.split('/')[len(v_id.split('/')) - 1]            #Step 2",none,,2020-07-07 23:38:14.695068,1.0
366,ng2o98k983k,http://www.youtube.com/channel/UCPnCDj3z-LF8RXu0ZwUzGhg,UCPnCDj3z-LF8RXu0ZwUzGhg,R B,Ugz4LcIXRlg6k8Q9lsd4AaABAg,1,1550498772.0,Great introductory information! Would love a similar one for how to scrape javascript rendered websites!,none,,2020-07-07 23:38:14.695068,0.0
367,ng2o98k983k,http://www.youtube.com/channel/UCKVqni4-Nmhr7MnIj1jjsyA,UCKVqni4-Nmhr7MnIj1jjsyA,Bob Jime,UgykBbYLcuwii7x5NrJ4AaABAg,2,1550138682.0,"thank you brother 
ilove u",none,,2020-07-07 23:38:14.696171,0.0
368,ng2o98k983k,http://www.youtube.com/channel/UCsCyQMhPWsG3_ZG7fE75QYA,UCsCyQMhPWsG3_ZG7fE75QYA,Leo Simon,Ugz1INz9a9zQE12cdsd4AaABAg,2,1550074938.0,I have to say you did a great job! You covered pretty much everything that a novice programmer needs to know about scraping. I immediately developed my own solution after watching the video. Thank you so much!!!,none,,2020-07-07 23:38:14.696171,0.0
369,ng2o98k983k,http://www.youtube.com/channel/UCKbcbJEgz8KPNUv1oCqZ59g,UCKbcbJEgz8KPNUv1oCqZ59g,Musarath Jahan,UgwfFrleRQPU12TRXIx4AaABAg,1,1549672339.0,"Thank you for this wonderful video. I have ben struggling to get data through web scrapping, and was successful after watching this video.",none,,2020-07-07 23:38:14.696171,0.0
370,ng2o98k983k,http://www.youtube.com/channel/UCLjPta7FiYy2oC_c_WYaDKg,UCLjPta7FiYy2oC_c_WYaDKg,Alexey,UgxbclCF4zt8tShCA_h4AaABAg,0,1549614883.0,"beautiful soup.. wtf who the hell figures out such dump namings in IT..
great tutorial though",none,,2020-07-07 23:38:14.696171,0.0
371,ng2o98k983k,http://www.youtube.com/channel/UCw66-uVm2_Pek8Ebo5lWiQA,UCw66-uVm2_Pek8Ebo5lWiQA,pankaj kumar,UgzuTKnS_XxDkqVxT4h4AaABAg,0,1549079890.0,I have  to find the 5 app I'd and app name from Google play store how can I get the HTML source code???,none,,2020-07-07 23:38:14.696171,0.0
372,ng2o98k983k,http://www.youtube.com/channel/UCwAK7Omu8FaT2zJPqufqFiA,UCwAK7Omu8FaT2zJPqufqFiA,Harish Gupta,UgyMUMLrTzgSMlzX8014AaABAg,0,1549076700.0,"Thanks for this video Corey.. I love your teaching.... 
Can you also make a video on how to scrape from websites that load data dynamically (Ajax + JS), covering 'infinite scrolling', 'load more' pagination... I'm not able to find the right tutorial/blog post... Would be very helpful in future... Thanks again for all your videos and keep uploading good content...!!",none,,2020-07-07 23:38:14.696171,0.0
373,ng2o98k983k,http://www.youtube.com/channel/UCWvgTnpmFwbLnDM1hGsfazg,UCWvgTnpmFwbLnDM1hGsfazg,Robert Lee,UgznRfx6J1PuLkSf6SN4AaABAg,2,1549072232.0,"Superb tutorial, by far one of the best explained BeautifulSoup tutorials to date. Thank you Corey.",none,,2020-07-07 23:38:14.697063,0.0
374,ng2o98k983k,http://www.youtube.com/channel/UCA7uz6DrC4ViYIIZrndyHbg,UCA7uz6DrC4ViYIIZrndyHbg,Boris Grigoryan,Ugz-Ud0NfNzfPEa7sit4AaABAg,1,1548859705.0,Thank you brother you are super. You  are good teacher and explain very good. Thanks a lot ! ! !,none,,2020-07-07 23:38:14.697063,0.0
375,ng2o98k983k,http://www.youtube.com/channel/UCyomKW58r4P3fgz30lp_oDQ,UCyomKW58r4P3fgz30lp_oDQ,Game Explorer,UgzzNOCSSS_QySaA_EB4AaABAg,1,1548636456.0,Great quick tutorials,none,,2020-07-07 23:38:14.697063,0.0
376,ng2o98k983k,http://www.youtube.com/channel/UCofTWxpGUUl_ArJa-UvGZIg,UCofTWxpGUUl_ArJa-UvGZIg,Jae Kim,Ugym5T-R7qDxKUkAuKF4AaABAg,0,1548199775.0,"Corey, thanks for the vidoe. I am a complete newbie but wanted to just copy what you've done and learn from there. I, however, get an error message after copying the codes on the 20 min 50 sec mark on the video. First of all when I run the file it's displaying a different web site address and then saying 'AttributeError: module 'requests' has no attribute 'get'. What would be causing this problem, you would know?",none,,2020-07-07 23:38:14.697063,2.0
377,ng2o98k983k,http://www.youtube.com/channel/UCf1_mbu4B3jrjraPMfBb0kQ,UCf1_mbu4B3jrjraPMfBb0kQ,Gokul Sambath,Ugy-pacU0Wm8ZE4jeAR4AaABAg,0,1548122669.0,Anybody knows how to scrap the  physical file to get downloaded  during crawling.?,none,,2020-07-07 23:38:14.697063,0.0
378,ng2o98k983k,http://www.youtube.com/channel/UCchDcCCVRcTWdNgfu1z7KGg,UCchDcCCVRcTWdNgfu1z7KGg,Emeka Orji,UgyqgNCKKMy_WvHbgDl4AaABAg,0,1547996976.0,"Please how did you come about the title ""simple.html"" am asking because the wikipedia page am to scrap data from is given to me as an url link. what should I do?",none,,2020-07-07 23:38:14.697063,0.0
379,ng2o98k983k,http://www.youtube.com/channel/UCx6YTnKUTFNi7jPuRyfK7AA,UCx6YTnKUTFNi7jPuRyfK7AA,Allah Akbar,UgzYgHEL9E9ncMLOWQt4AaABAg,5,1547758803.0,"Such a beautiful and comprehensive explanation!  I liked the idea that you started from the simplest things, like scraping a small HTML file, not an entire website! Because that what I could not find in the many videos I watched about BeautifulSoup, they jump into the deep straight ahead!",none,,2020-07-07 23:38:14.697063,0.0
380,ng2o98k983k,http://www.youtube.com/channel/UCrdZBKBDQiQEaLMMDnrr9SQ,UCrdZBKBDQiQEaLMMDnrr9SQ,trouser python,Ugx1aVThBHZ1mJUPhyx4AaABAg,2,1547702089.0,Fucking excellent video!,none,,2020-07-07 23:38:14.697063,0.0
381,ng2o98k983k,http://www.youtube.com/channel/UCBwQaKwYcwekk81GPnzCyog,UCBwQaKwYcwekk81GPnzCyog,Jarod Morris,UgwhGPd6GK0Nt0nG_st4AaABAg,0,1547604450.0,"How fast can you roll through a list of URLs?  100 / min?  500 / min?  How do you balance getting the data as fast as you can, but not trigger a server thinking you're attacking it?",none,,2020-07-07 23:38:14.697063,0.0
382,ng2o98k983k,http://www.youtube.com/channel/UCvv5Dx7zUHB7VMx4bn4Ic5g,UCvv5Dx7zUHB7VMx4bn4Ic5g,Jiaming Qu,UgwI_2I6GYr1E4pOxF94AaABAg,1,1547402871.0,"great video, thank you",none,,2020-07-07 23:38:14.697063,0.0
383,ng2o98k983k,http://www.youtube.com/channel/UCYT7PVDjCtMFjXFfwfbtnpw,UCYT7PVDjCtMFjXFfwfbtnpw,Владислав Демченко,UgyYjLIFOCxR4kUR9hR4AaABAg,3,1547027932.0,"Thanks, man. You're great! Everything is clear for the beginner like me.",none,,2020-07-07 23:38:14.697063,0.0
384,ng2o98k983k,http://www.youtube.com/channel/UCoBwCqe52X3BkE79T6WKeMQ,UCoBwCqe52X3BkE79T6WKeMQ,백영래,UgwoCKgR6BdX0msw9al4AaABAg,0,1547021185.0,Python 2?,none,,2020-07-07 23:38:14.697063,0.0
385,ng2o98k983k,http://www.youtube.com/channel/UCm5xfxPgvT7834i1jpo8Y-g,UCm5xfxPgvT7834i1jpo8Y-g,Ed Tix,Ugxj5MEadTFsWsFSesl4AaABAg,0,1546743682.0,What if the site requires login? I need to scrape my earnings from auction portal but it obviously requires login... Unfortunately the site has no API :(,none,,2020-07-07 23:38:14.697063,2.0
386,ng2o98k983k,http://www.youtube.com/channel/UC_AIfab6mcB9TLjidVkVLyQ,UC_AIfab6mcB9TLjidVkVLyQ,Colin Burgess,UgzChzXahXyZgNoTSUd4AaABAg,0,1546731832.0,"I use coderunner on my mac and cannot import beautifulsoup, or really any libraries, even they are on my computer. I am also having difficulty using the pip command. Any advice?",none,,2020-07-07 23:38:14.698059,1.0
387,ng2o98k983k,http://www.youtube.com/channel/UChw2H0e6_10NoHkhlBl6wKw,UChw2H0e6_10NoHkhlBl6wKw,rahul parmar,Ugy6H-9J3MeKdFk-dcl4AaABAg,0,1546437657.0,How to use those public api to scrape,none,,2020-07-07 23:38:14.698059,1.0
388,ng2o98k983k,http://www.youtube.com/channel/UCLDD919bUAj9ahi7tkMatng,UCLDD919bUAj9ahi7tkMatng,Ashwani Garg,Ugyx_iU9UpRUdXmwHw54AaABAg,0,1546151758.0,"Hi, Big Thanks for this tutorial, really saved my days and tons of searching. Just one thing to ask -> At 45:08, you said they even block your program, I think they block our IP address or something else? - Just for educational purpose.",none,,2020-07-07 23:38:14.698059,1.0
389,ng2o98k983k,http://www.youtube.com/channel/UC7qgoMJVYoXsfxrqBeK1YpA,UC7qgoMJVYoXsfxrqBeK1YpA,Harish S,UgwucF_Nwx-A0fLCwWB4AaABAg,0,1545805936.0,how do we do the same for multiple pages? rather than going one by one?,none,,2020-07-07 23:38:14.698059,2.0
390,ng2o98k983k,http://www.youtube.com/channel/UCNQy4v1VtDD8ggwE-JKPhiw,UCNQy4v1VtDD8ggwE-JKPhiw,Harrison Hobs,Ugxsl1d45IVwsKBwvX94AaABAg,0,1545784027.0,"Thanks for this wonderful tutorial, I really got everything. I have a question though. This was done with your site's html design. I just would like to know how to go about it on a site that doesn't have an article header, but instead just has mutiple div tags. What then should be done?",none,,2020-07-07 23:38:14.698059,1.0
391,ng2o98k983k,http://www.youtube.com/channel/UCM9mgmH0Sa-yt-La1zCXmMQ,UCM9mgmH0Sa-yt-La1zCXmMQ,João Pedro Souza Rocha,Ugz4FEH8S1ZU3fxYyDZ4AaABAg,1,1545578297.0,Thx!!!!,none,,2020-07-07 23:38:14.698059,0.0
392,ng2o98k983k,http://www.youtube.com/channel/UCpBrWymKJb1mZJ7EfAvGYaw,UCpBrWymKJb1mZJ7EfAvGYaw,TmanD54,Ugx9Ygp_QV9MulskLa94AaABAg,0,1545358912.0,"any idea why when i write to my csv file, it skips are row? so A1 would write, A2 is skipped, A3 writes. very strange",none,,2020-07-07 23:38:14.698059,1.0
393,ng2o98k983k,http://www.youtube.com/channel/UCsAJ49CWmJ77NaVeFFEnZeA,UCsAJ49CWmJ77NaVeFFEnZeA,sv Math Tutor,UgyPRf6hgQ93xBwA3uB4AaABAg,0,1545327653.0,"At 16:22 how did you get the menu item Tools to flash? I'm assuming you used the shortcut Ctrl + B to automatically do the Tools > Build step without moving the mouse.  I was wondering how we could run python without going to the terminal or clicking on any Run button.  How did you get your Sublime Text to flash whenever you use a shortcut so we can see which menu item flashes. Thanks Corey.  When I use the same shortcut Ctrl + B, nothing flashes.",none,,2020-07-07 23:38:14.698059,0.0
394,ng2o98k983k,http://www.youtube.com/channel/UCevI2gHxIV-iMoF4HsN7hPw,UCevI2gHxIV-iMoF4HsN7hPw,WillyVodka,Ugx7t5hQy_3GmaWOYLR4AaABAg,0,1545277277.0,"Excellent video, I've learned a bunch!
However I do have an issue with the CSV-file that was generated - it all ends up in one row comma-seperated.
I tried to basically copy-paste your code into mine, but it all ends up rather plain text in one row and several columns.
This is most updated excel in O365.

Do you have any ideas why this happen?
Greets!",none,,2020-07-07 23:38:14.698059,0.0
395,ng2o98k983k,http://www.youtube.com/channel/UC5gOM2oeYQXdH6lb-sQAmjw,UC5gOM2oeYQXdH6lb-sQAmjw,akram malek,Ugx6Uy--eAOn8UL1aZh4AaABAg,1,1545173451.0,"amazing video, information, and amazing man",none,,2020-07-07 23:38:14.698059,0.0
396,ng2o98k983k,http://www.youtube.com/channel/UCZys1jsVei3OVCTWxmMChzw,UCZys1jsVei3OVCTWxmMChzw,Ken Pryor,UgzCtsuCPgEMDZt1Qal4AaABAg,13,1545167048.0,This was the first of your tutorials I've seen. I definitely subscribed after this great video. I've struggled with learning Python and this was very helpful.,none,,2020-07-07 23:38:14.698059,0.0
397,ng2o98k983k,http://www.youtube.com/channel/UCYVS_RjnfJhou8SLZNvRaGA,UCYVS_RjnfJhou8SLZNvRaGA,Mankaran Singh,UgxovZl4CUlKTfJxyeZ4AaABAg,1,1545145159.0,"Most professional guy on youtube, who dosent boasts to be know stuff.",none,,2020-07-07 23:38:14.698059,0.0
398,ng2o98k983k,http://www.youtube.com/channel/UCk9NELrSP2Wfhm8tYm6huvg,UCk9NELrSP2Wfhm8tYm6huvg,Eyoba,UgyE0cmu0MOzjhoVZYB4AaABAg,7,1545127892.0,"My first video from your channel and it didn't took me long to press the subscribe button. You are just perfect, no second wasted. Every word is important and worth mentioning. Cheers!",none,,2020-07-07 23:38:14.698059,1.0
399,ng2o98k983k,http://www.youtube.com/channel/UCyNM_sooWzhREXc4OzSNAsw,UCyNM_sooWzhREXc4OzSNAsw,Radiant32 32,UgwZBDvsTEw-QZTjIeh4AaABAg,1,1545127635.0,Crystal clear explanation,none,,2020-07-07 23:38:14.698059,0.0
400,ng2o98k983k,http://www.youtube.com/channel/UC85e9gwzx6AiWekBSl47b3w,UC85e9gwzx6AiWekBSl47b3w,Dento,Ugy8jaOz7H4tMMYI6ux4AaABAg,0,1544954162.0,What should I do if there is multiple collums with the same name?,none,,2020-07-07 23:38:15.180333,0.0
401,ng2o98k983k,http://www.youtube.com/channel/UCNN2_TAIsdcaVXwWvMZ9Fmw,UCNN2_TAIsdcaVXwWvMZ9Fmw,Edy,Ugyz3QlpyyzkAIqnsZ94AaABAg,0,1544652049.0,Great video as usual.Excellent demonstration of parsing.One question:how would you do that to retrieve all that data from all the pages?I'm sure that you would put all that code in a for loop that would loop through all the pages.But how could you determine that number of pages without hardcoding it?I mean how exactly would you write that for loop?,none,,2020-07-07 23:38:15.180333,2.0
402,ng2o98k983k,http://www.youtube.com/channel/UCu17bVYd68Z_XtLqJJ6ggMA,UCu17bVYd68Z_XtLqJJ6ggMA,Sergey Solod,UgzZ1yS7RfQ92BJKlaN4AaABAg,1,1544627111.0,Thank you for all tutorials! Great job.,none,,2020-07-07 23:38:15.180333,0.0
403,ng2o98k983k,http://www.youtube.com/channel/UCM--7r9ecPlcf6zAB15ZjCA,UCM--7r9ecPlcf6zAB15ZjCA,shirmito,UgwN_ezztiyhvBEsnld4AaABAg,0,1544499772.0,u are fucking robot :D,none,,2020-07-07 23:38:15.180333,0.0
404,ng2o98k983k,http://www.youtube.com/channel/UCrbfgIYC_HsCV76WIMt8k6A,UCrbfgIYC_HsCV76WIMt8k6A,Vatsalay Khobragade,UgyEh_d1vFmxNItqCU14AaABAg,0,1544147342.0,please make a video on flask whooshalchemy. btw nice video.,none,,2020-07-07 23:38:15.180333,0.0
405,ng2o98k983k,http://www.youtube.com/channel/UCjy59FVRakuitTZkZXKDLwg,UCjy59FVRakuitTZkZXKDLwg,narmadha anu,Ugyt7M97nGqfcFEjVhJ4AaABAg,0,1544082220.0,"Any one can tell,  how to find number of occurrence of a particular word in a given website using python beautiful soup...",none,,2020-07-07 23:38:15.180333,0.0
406,ng2o98k983k,http://www.youtube.com/channel/UC5T5pOOl-vx3jckgHTZlFiw,UC5T5pOOl-vx3jckgHTZlFiw,Prashant Kolhar,UgwEAUUP-k3wCIP4pEd4AaABAg,1,1543978034.0,"Corey you made my career from an absolute IT nerd to hardcore Python developer. Thanks to you. You are gr8. God bless you. 
looking forward to learn Machine learning. It would be great if you make some videos on it. Thanks.",none,,2020-07-07 23:38:15.180333,1.0
407,ng2o98k983k,http://www.youtube.com/channel/UCGasWslod8i5nMG1LJPMUPA,UCGasWslod8i5nMG1LJPMUPA,coder gun,UgwjMs9b4t7s4iVZRy94AaABAg,0,1543918464.0,"help anyone,
i want to use scrape data in django. for  eg,
i want to scrape quotes of the day(that change every day)from website and use it in my own websites.",none,,2020-07-07 23:38:15.181330,0.0
408,ng2o98k983k,http://www.youtube.com/channel/UCDNL9W72QKAjAHjdZvoXs7g,UCDNL9W72QKAjAHjdZvoXs7g,samaresh yadav,UgzZaey9z8kmtEcw_sN4AaABAg,0,1543563665.0,I want to download files using scraping techniques,none,,2020-07-07 23:38:15.181330,0.0
409,ng2o98k983k,http://www.youtube.com/channel/UCBuHA8CwftbO9iSLcqz2krw,UCBuHA8CwftbO9iSLcqz2krw,Robert Caldwell,UgyGqw3xNS0fLUzwi8l4AaABAg,1,1543368877.0,Outstanding video.,none,,2020-07-07 23:38:15.181330,0.0
410,ng2o98k983k,http://www.youtube.com/channel/UClxrnYWVJgu83xFCx-9n8IA,UClxrnYWVJgu83xFCx-9n8IA,Răzvan Afloarei,UgwWtWMajxFk4Z2m5D94AaABAg,1,1543157430.0,"How do I web scrape on Windows? I tried installing beautifulsoup, but I believe it's only for Linux.",none,,2020-07-07 23:38:15.181330,1.0
411,ng2o98k983k,http://www.youtube.com/channel/UCPFo27gqemhEGe1ZnDengKw,UCPFo27gqemhEGe1ZnDengKw,ahmed abdelmalek,UgywlcOQg3XoEIGYVwp4AaABAg,0,1542946512.0,anything for Scrapy ?? :D,none,,2020-07-07 23:38:15.181330,0.0
412,ng2o98k983k,http://www.youtube.com/channel/UCzAYpP4ClAZg-TD0BLCK0NA,UCzAYpP4ClAZg-TD0BLCK0NA,Asad Noorzaie,UgzFFnrQIzwwtPUgtZB4AaABAg,1,1542742863.0,First video of watching your channel. Awesome explanation!,none,,2020-07-07 23:38:15.181330,0.0
413,ng2o98k983k,http://www.youtube.com/channel/UCmuwlHCVsD3dRVF_V36h7MA,UCmuwlHCVsD3dRVF_V36h7MA,Yang Feng,UgzmDj3dHJ0xNEWzMs14AaABAg,0,1542731005.0,Your videos on Python are always super clear! I am wondering whether you can also generate some on scraping data from social media using Python?,none,,2020-07-07 23:38:15.181330,0.0
414,ng2o98k983k,http://www.youtube.com/channel/UCEsariDVajxjuYB7ZnRIIeg,UCEsariDVajxjuYB7ZnRIIeg,Siddhartha Sehgal,UgyqyJbs60gqDimArd94AaABAg,0,1542521196.0,"How did writing into the csv file just once worked? Shouldn't we need to write to the file everytime we  get a new Headline, summary and link? Using a loop?",none,,2020-07-07 23:38:15.181330,0.0
415,ng2o98k983k,http://www.youtube.com/channel/UC8KglJuhtu6Nxm8PTk5mwYw,UC8KglJuhtu6Nxm8PTk5mwYw,nicholas hammonds,UgwzYDrZMuR6E-yQyEp4AaABAg,1,1542155143.0,excellent video!!! thankyou so much,none,,2020-07-07 23:38:15.181330,0.0
416,ng2o98k983k,http://www.youtube.com/channel/UCTCS4pk6koNT4omcJOjH6MQ,UCTCS4pk6koNT4omcJOjH6MQ,Tim Lawrence,Ugwkjm_yYJWixDQ41bF4AaABAg,0,1541963803.0,"Awesome video :) however I am stuck! The site i am scraping has some information set out in a list.... It has 
14 <li class=""row"">...</li> tags of which I only want one (the one I want is the second one). I can scrape the data but can only return the first result...... I tried to find_all but it threw an error. Any suggestions?",none,,2020-07-07 23:38:15.181330,0.0
417,ng2o98k983k,http://www.youtube.com/channel/UCCD2znrrnxF-M08eR_-CHgQ,UCCD2znrrnxF-M08eR_-CHgQ,Sagi Polaczek,Ugxv7Lz-6cgqeQ5Tg4Z4AaABAg,2,1541951502.0,liked before even watching... again!,none,,2020-07-07 23:38:15.181330,0.0
418,ng2o98k983k,http://www.youtube.com/channel/UCfrEJjlrM9WijOs5ll09U8w,UCfrEJjlrM9WijOs5ll09U8w,daniel merchad,Ugx7bzXReE4qL9v_wL94AaABAg,1,1541607002.0,How are you so good at making me  watch all your videos??!!,none,,2020-07-07 23:38:15.181330,0.0
419,ng2o98k983k,http://www.youtube.com/channel/UCcYhGcBmttm1mJTRmXUhJ_Q,UCcYhGcBmttm1mJTRmXUhJ_Q,d p,UgwbzfUcY-25wtvN9zd4AaABAg,0,1540644285.0,"Holy bejebus, dude.   This one video answered sooo many questions i had after trying to build a scraper & referencing 2 other tutorials.  SMH!  Can't wait to get home & fix my script based on this tutorial!  Thanks!",none,,2020-07-07 23:38:15.181330,0.0
420,ng2o98k983k,http://www.youtube.com/channel/UCyg4aBpLwNcbSJHZgV1OGxw,UCyg4aBpLwNcbSJHZgV1OGxw,Tal Sofer,UgzF6WQOTMF_ChojxP94AaABAg,1,1540295173.0,you are the best!,none,,2020-07-07 23:38:15.182334,0.0
421,ng2o98k983k,http://www.youtube.com/channel/UC58zdVysuhmRdNOz4WKOqvg,UC58zdVysuhmRdNOz4WKOqvg,Ali Radmehr,UgxNW5LnZV8k4JYFIph4AaABAg,0,1540201986.0,"if print(soup) cause an error, you can use print(soup.encode(""utf-8"")) instead",none,,2020-07-07 23:38:15.182334,0.0
422,ng2o98k983k,http://www.youtube.com/channel/UCvnHR5ltpicjhHPClxJ6J4w,UCvnHR5ltpicjhHPClxJ6J4w,farmakoxeris,UgwEBik6IryqrtBrWch4AaABAg,0,1540147645.0,"Usually, http requests are sent through port 80 or 8080. What about other ports? I have a connection through a router and the port is different. In the web browser I have to type the IP and the port eg. 192.193.194.195:520. Can it work with Beautifulsoup?",none,,2020-07-07 23:38:15.182334,0.0
423,ng2o98k983k,http://www.youtube.com/channel/UC9OZ3smunNTIEiYkMpU9KFg,UC9OZ3smunNTIEiYkMpU9KFg,Evo Wolf,UgzctxzUQeUUe4nzis14AaABAg,1,1539919549.0,"what if there are multiple ,p. tags like, lets say 3 of them. and I only want to grab the text of he second ,p. tag in a paragraph",none,,2020-07-07 23:38:15.182334,2.0
424,ng2o98k983k,http://www.youtube.com/channel/UCjaarXp7IiHwhpbIwPBcPHQ,UCjaarXp7IiHwhpbIwPBcPHQ,Stephen M,UgwX3FLB2hcto8-9vz14AaABAg,1,1539469569.0,This is great man thank you,none,,2020-07-07 23:38:15.182334,0.0
425,ng2o98k983k,http://www.youtube.com/channel/UCGS3M1SGvnmvivoMOSYRz6g,UCGS3M1SGvnmvivoMOSYRz6g,Artaza Sameen,Ugwa677LfHfW1KxqMIB4AaABAg,0,1539446150.0,"Sir, I am a great fan of yours.
Because of you I learnt regular expression and more.
Could you please add some videos on the tkinter module for gui programs and  how do we create freezed binaries out of .py files",none,,2020-07-07 23:38:15.182334,0.0
426,ng2o98k983k,http://www.youtube.com/channel/UCJTfAiKgEeH1WP-gmkVEllg,UCJTfAiKgEeH1WP-gmkVEllg,Saurav S,Ugxg15X9pLHauaMPWGh4AaABAg,0,1539240865.0,"(Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1045)'))) i am getting this error can you please advise?",none,,2020-07-07 23:38:15.182334,0.0
427,ng2o98k983k,http://www.youtube.com/channel/UCdHWmocPTZsRmg5I3ro6lyQ,UCdHWmocPTZsRmg5I3ro6lyQ,Jivnesh Sandhan,UgwPZQXwxMsncGq3dtJ4AaABAg,1,1539087025.0,Nicely explained. Thank you very much.,none,,2020-07-07 23:38:15.182334,0.0
428,ng2o98k983k,http://www.youtube.com/channel/UCj_yWYv_K-WJ4PARgEzP07Q,UCj_yWYv_K-WJ4PARgEzP07Q,Degardin Arnaud,Ugwg1sFbBQ6Xh2MPlZV4AaABAg,0,1539073443.0,Automate Everything with web bots! This actually made my life easier... https://simplestipsandtricks.blogspot.com/2018/10/the-power-of-headless-chrome-and.html,none,,2020-07-07 23:38:15.182334,0.0
429,ng2o98k983k,http://www.youtube.com/channel/UCk_nifXmCmkiwZlssWoC7Nw,UCk_nifXmCmkiwZlssWoC7Nw,Gautam R,UgytCM-FZTHHedzHrE54AaABAg,0,1538910417.0,"im unable to scrape a website "" https://www.hackathon.com/city/india/bengaluru"" could u help me out with that it doesn't have a public Api",none,,2020-07-07 23:38:15.182334,0.0
430,ng2o98k983k,http://www.youtube.com/channel/UCQ793InXJEJwZwmJVE_INrg,UCQ793InXJEJwZwmJVE_INrg,Nicholas Maloof,UgzB6geQrwQOvrIJTex4AaABAg,0,1538484644.0,"Do you have any advice on resources we could check out for web scraping responsibly? I want to use web crawling/web scraping for work, as my job sometimes involves a considerable amount of data entry. Also, any advice on how to properly read a robot.txt file? Thank you for the tutorial!",none,,2020-07-07 23:38:15.182334,0.0
431,ng2o98k983k,http://www.youtube.com/channel/UCTNNewJSoNnubwONHhgINNQ,UCTNNewJSoNnubwONHhgINNQ,Nishanth Chandrashekar,UgwIcZoDHyitIZkG7OJ4AaABAg,1,1538226436.0,"Beautifully explained man, thank you very much",none,,2020-07-07 23:38:15.182334,0.0
432,ng2o98k983k,http://www.youtube.com/channel/UCTXPhl1c81OeTDwtI0U1n7Q,UCTXPhl1c81OeTDwtI0U1n7Q,jawad mansoor,UgzTwI5mX0DW5fAgf3p4AaABAg,2,1538019892.0,"Thank you sir, for uploading the best and easiest web scraping turorial",none,,2020-07-07 23:38:15.182334,0.0
433,ng2o98k983k,http://www.youtube.com/channel/UChFoxJNr8WfMPqlhohH70eQ,UChFoxJNr8WfMPqlhohH70eQ,Set P,Ugzwnmgvh17IODHn3VZ4AaABAg,1,1537929388.0,Excellent tutorial - thanks a lot!,none,,2020-07-07 23:38:15.182334,0.0
434,ng2o98k983k,http://www.youtube.com/channel/UCKZ4LrFqJkFml9pKG1nAR-Q,UCKZ4LrFqJkFml9pKG1nAR-Q,David,UgxNu7wEU5thPHoVb7Z4AaABAg,0,1537251489.0,Nice video as always Corey. Just a question: I've dabbled with Scrapy too. When you should use Scrapy and when bs4?,none,,2020-07-07 23:38:15.183384,0.0
435,ng2o98k983k,http://www.youtube.com/channel/UCnFhHhC-hKBYWNByp9QSscw,UCnFhHhC-hKBYWNByp9QSscw,Darryl Lobo,Ugxo36d4M-_2Ua6zYkl4AaABAg,0,1537223909.0,Can you use this to go into your gmail account and grab emails and scrape content from it?,none,,2020-07-07 23:38:15.183384,2.0
436,ng2o98k983k,http://www.youtube.com/channel/UCYzOzEcUnavZxsSlzboDbHw,UCYzOzEcUnavZxsSlzboDbHw,Robin Green,Ugx7daXD19QyNdk9yKh4AaABAg,0,1536509763.0,"I have, from what i can tell, the exact same code, yet when i run the final program i get a traceback error telling me that: AttributeError: 'builtin_function_or_method' object has no attribute 'writerow' please can someone help me?",none,,2020-07-07 23:38:15.183384,1.0
437,ng2o98k983k,http://www.youtube.com/channel/UCHY3phCUK-mjW6xHa4OD1MQ,UCHY3phCUK-mjW6xHa4OD1MQ,Bob Stout,UgywF_GRXLcaTI859fJ4AaABAg,21,1536097829.0,"As always, Corey’s videos are top-notch. Thank you, Corey!",none,,2020-07-07 23:38:15.183384,0.0
438,ng2o98k983k,http://www.youtube.com/channel/UCHEU4cItD3UhnZ0oWktpspA,UCHEU4cItD3UhnZ0oWktpspA,Leonardo Grinstein,Ugx0Yntuf6Cu7YeKDm54AaABAg,0,1535380518.0,Hi Corey. I loved your video and I am following your channel. But I couldn't use any of the tactics you explained in this video to grab the SongName of this URL - https://www.azlyrics.com/lyrics/gunsnroses/sweetchildomine.html. Mayble you should add a trick to explain what to do in situations like this.,none,,2020-07-07 23:38:15.183384,0.0
439,ng2o98k983k,http://www.youtube.com/channel/UCdiXX_ni60id0NwYSl7PVeQ,UCdiXX_ni60id0NwYSl7PVeQ,Daniel Fernandes,UgyOh4F-QotugSka4mN4AaABAg,1,1535362373.0,"Hi Corey! My intention is not to scrape webpages, but to extract data from purchase orders in html format and use this data to create an invoice!  So thank you very much for this small but complete tutorial! This is really handy!",none,,2020-07-07 23:38:15.183384,0.0
440,ng2o98k983k,http://www.youtube.com/channel/UCgUxnMu5EoSeoq1xfyHa3-Q,UCgUxnMu5EoSeoq1xfyHa3-Q,sandeep mishra,UgzevaYguGG-O-Hh9Et4AaABAg,0,1535318203.0,"Awesome Tutorial!!Corey

I have one question , How we will collect data from multiple pages in website?",none,,2020-07-07 23:38:15.183384,1.0
441,ng2o98k983k,http://www.youtube.com/channel/UCXHo2bjBRvpeIX-AZya4ZsQ,UCXHo2bjBRvpeIX-AZya4ZsQ,Murugan Ramaswamy,UgzCMKTbs65MZDcBI_94AaABAg,1,1534735541.0,"Hi Corey Schafer,

Your videos are really fantastic it helped me so much in learning python.

Unlike other channels you video is very simple and professional.
Easy to understand, and my request for you is can you do a separate tutorial video for python urllib library.",none,,2020-07-07 23:38:15.183384,0.0
442,ng2o98k983k,http://www.youtube.com/channel/UCeyS7ftFB3SfNyFvD-bUeVg,UCeyS7ftFB3SfNyFvD-bUeVg,TheScrpk,UgzkCfVXkrdXOVBnBYl4AaABAg,0,1534049376.0,"Hey Corey, firstly thank you for the amazing tutorial. Really helped me to understand webscraping in a crisp manner. I just had one question, I ran the script and the csv that got generated had blank rows in between each entry and it wasn't structured as a table for me. Would be really helpful if you could suggest how to incorporate the same in the code. TIA :)",none,,2020-07-07 23:38:15.183384,0.0
443,ng2o98k983k,http://www.youtube.com/channel/UCBzYI8oWCjeevic7G9NhmxA,UCBzYI8oWCjeevic7G9NhmxA,ablg 13,UgwLe0fsCg4UZMRTBfF4AaABAg,0,1533712917.0,Is this legal?,none,,2020-07-07 23:38:15.183384,2.0
444,ng2o98k983k,http://www.youtube.com/channel/UCDGJa8z9hPq8DmRJgAMh3uA,UCDGJa8z9hPq8DmRJgAMh3uA,Gacoka Mbui,Ugz9NyZ4XocgWSDQKOR4AaABAg,1,1533664792.0,Make more videos... You're the best!,none,,2020-07-07 23:38:15.183384,0.0
445,ng2o98k983k,http://www.youtube.com/channel/UCvnHR5ltpicjhHPClxJ6J4w,UCvnHR5ltpicjhHPClxJ6J4w,farmakoxeris,Ugx5bzeCa64-pZru1gR4AaABAg,0,1533636392.0,I got a UnicodeEncodeError. How can I gewt rid of it?,none,,2020-07-07 23:38:15.183384,0.0
446,ng2o98k983k,http://www.youtube.com/channel/UCvnHR5ltpicjhHPClxJ6J4w,UCvnHR5ltpicjhHPClxJ6J4w,farmakoxeris,UgwapdS8cHyEW3XEl4Z4AaABAg,0,1533632370.0,I have a webpage that has a bug. Therefore I need to save the data in a file and then process it (I know what must be done). Is there any way I handle all these data as a huge string? Just put it in a file and then process the file.,none,,2020-07-07 23:38:15.183384,0.0
447,ng2o98k983k,http://www.youtube.com/channel/UCvnHR5ltpicjhHPClxJ6J4w,UCvnHR5ltpicjhHPClxJ6J4w,farmakoxeris,UgwJBY6L7d3Z173GWDZ4AaABAg,0,1533632256.0,"soup.find('article') gives you the first article (Number one? Number zero? What's the first?).
soup.find('article') gives you all articles.

How can I get a particular article? Let's say that there are 20 articles and I want to get the article number 11. How can it be done?",none,,2020-07-07 23:38:15.184385,0.0
448,ng2o98k983k,http://www.youtube.com/channel/UCyHnET3-g2wvOAQBCR5EuWw,UCyHnET3-g2wvOAQBCR5EuWw,Ryan Kao,UgxAzhc1pKeUCPsYtPx4AaABAg,0,1533513038.0,Is it possible to do a series of web scraping via scrapy tutorials for text contents and images? Thank you.,none,,2020-07-07 23:38:15.184385,0.0
449,ng2o98k983k,http://www.youtube.com/channel/UCrTEH1uh4fPP-qOYT0kX1GA,UCrTEH1uh4fPP-qOYT0kX1GA,Charan Teja,UgxTYAOdjtf2EdvIYc94AaABAg,2,1533468408.0,"This is the video , which i saw completely.
print('Excellent')",none,,2020-07-07 23:38:15.184385,0.0
450,ng2o98k983k,http://www.youtube.com/channel/UCQ59HAQp7v-O89yuV5QR_RQ,UCQ59HAQp7v-O89yuV5QR_RQ,stephen barter,UgyxntY1x5_K5ITeySt4AaABAg,0,1533413445.0,Beautiful soup can be used to save images as well right?,none,,2020-07-07 23:38:15.184385,0.0
451,ng2o98k983k,http://www.youtube.com/channel/UC8eGPhqKQrzpfzIWJWbeiHg,UC8eGPhqKQrzpfzIWJWbeiHg,TheSagitube,UgzFuQPKckOYAoN0WXl4AaABAg,0,1533217311.0,how to select a date in a datepicker on a webpage using python?,none,,2020-07-07 23:38:15.184385,0.0
452,ng2o98k983k,http://www.youtube.com/channel/UC3k5Eri6FY7wiCpZQUNQ3jw,UC3k5Eri6FY7wiCpZQUNQ3jw,offline music,UgyToUIQULVHuKc_W6l4AaABAg,0,1533061960.0,"why 'lxml', but not html.parser ?",none,,2020-07-07 23:38:15.184385,0.0
453,ng2o98k983k,http://www.youtube.com/channel/UCiqPiythGXXrOLNxP4QTHVw,UCiqPiythGXXrOLNxP4QTHVw,R M,Ugxi18xNz93tedCYl9l4AaABAg,0,1532940493.0,"Hi Corey, first of all I want to thank you for your teachings, your videos are spectacular and I am learning a lot with them much more than with books. Are you planning to make a series about scrapy too? I love web scraping but I am struggling with some advanced uses of scrapy and your videos could help a lot, me and others interested in this topic. My best wishes for your channel. Thanks",none,,2020-07-07 23:38:15.184385,0.0
454,ng2o98k983k,http://www.youtube.com/channel/UCaMG5tj_XDDRZC3v3U5Ggcw,UCaMG5tj_XDDRZC3v3U5Ggcw,Kai Sun,UgzoRGQXexZDPhwuRFR4AaABAg,1,1532695465.0,Thank you! it works very well,none,,2020-07-07 23:38:15.184385,0.0
455,ng2o98k983k,http://www.youtube.com/channel/UC-ajPi6hsb-Z5mEKlXCydZA,UC-ajPi6hsb-Z5mEKlXCydZA,Vishesh Mangla,UgxkeJXpaL4qbAFDqG14AaABAg,1,1532446214.0,great explanation.I really liked it:),none,,2020-07-07 23:38:15.184385,0.0
456,ng2o98k983k,http://www.youtube.com/channel/UCInuvhBi9-XanhMt4-ujFHw,UCInuvhBi9-XanhMt4-ujFHw,Mista T,UgzXHOP0TMsl9t55Xpx4AaABAg,1,1532368209.0,You are the best teacher!!!!!!!,none,,2020-07-07 23:38:15.184385,0.0
457,ng2o98k983k,http://www.youtube.com/channel/UCLfz_kQJ_CAiBItAKX7NV0Q,UCLfz_kQJ_CAiBItAKX7NV0Q,Muhammad Usman,UgyfP_bjANmKZAO5BUp4AaABAg,1,1531844707.0,you are something really extra ordinary 😘😘😘😘😘😘😘😘,none,,2020-07-07 23:38:15.184385,0.0
458,ng2o98k983k,http://www.youtube.com/channel/UCmzEgKjpmqXdGZfXkXAgXPA,UCmzEgKjpmqXdGZfXkXAgXPA,Increadible Angel,UgzwaMqSMUdH9XMWpch4AaABAg,1,1531576186.0,it was awesome,none,,2020-07-07 23:38:15.184385,0.0
459,ng2o98k983k,http://www.youtube.com/channel/UCj55lCb8HyYtL0HCf2DlC5g,UCj55lCb8HyYtL0HCf2DlC5g,Rafael Benetton,Ugx9JqXCX-f9_yxeSlB4AaABAg,0,1531415898.0,"i have only one phase without span, div, id, class, how i can get it?
<body>www.google.com</body>",none,,2020-07-07 23:38:15.184385,0.0
460,ng2o98k983k,http://www.youtube.com/channel/UCQRqyRyS3hx7bSZzOsVa9cw,UCQRqyRyS3hx7bSZzOsVa9cw,justin ceiley,UgyM1HJHbmiaUwHjLYJ4AaABAg,1,1531346931.0,"You're python tutorials are honestly the best thing ever. I'm only a sophomore in high school but learning to code has been a hobby of mine for about two years. I just recently started to learn python more in depth (classes, 3rd party modules, reading and writing files etc.) and your videos explain the concepts with such detail and understanding that by the end of the video I feel like I really have a grasp on the subject of the video. So really just thank you for making these videos and empowering others with the skills of programming.",none,,2020-07-07 23:38:15.185376,2.0
461,ng2o98k983k,http://www.youtube.com/channel/UCx5H9tIYNh5u_zWSrl8SOQw,UCx5H9tIYNh5u_zWSrl8SOQw,krishna narwani,UgyhBGIJbr1qIIMnaz94AaABAg,0,1530884874.0,"lxml is unable to install in my lappy
showing an error: -  FileNotFoundError: [WinError 2] The system cannot find the file specified


please help me out I'm tired of all this now

BTW your tutorials are really great.I love them Watching",none,,2020-07-07 23:38:15.185376,1.0
462,ng2o98k983k,http://www.youtube.com/channel/UCU-nT__FfO4dZ6iiswK31Tg,UCU-nT__FfO4dZ6iiswK31Tg,Fabio de Abreu,UgwHEqUn9LYGebpoW2N4AaABAg,0,1530813910.0,"Great video thanks a lot.Just one doubt can someone explain to me this line
vid_src = article.find('iframe' , class_='youtube-player')['src'] 

He said it has to do with dictionaires but still I dont get.",none,,2020-07-07 23:38:15.185376,0.0
463,ng2o98k983k,http://www.youtube.com/channel/UCg-E7kz4XV10UgQZts57RgA,UCg-E7kz4XV10UgQZts57RgA,Video Collector,UgyAnp57rX-DXxHVBnp4AaABAg,0,1530792395.0,I am using Windows10 and I have write some codes to scrape information from a website but I can't copy anything from my command prompt or anaconda prompt. I figure it out that my mouse dosen't working on both command prompt. Can anybody help me?,none,,2020-07-07 23:38:15.185376,0.0
464,ng2o98k983k,http://www.youtube.com/channel/UCJLNqrYtkflV6vmyjBG0L6g,UCJLNqrYtkflV6vmyjBG0L6g,Anoubhav Agarwaal,UgxNrehy47DhvEQFqgl4AaABAg,0,1530712527.0,Do you have videos on web crawling? Thanks,none,,2020-07-07 23:38:15.185376,0.0
465,ng2o98k983k,http://www.youtube.com/channel/UCdpsRuesmQr9bIDzU7mpzbQ,UCdpsRuesmQr9bIDzU7mpzbQ,Aditya Tripathi,UgwfI4vJ168D2_AdplJ4AaABAg,3,1530704931.0,your understanding of python is very deep and your teaching method is excellent,none,,2020-07-07 23:38:15.185376,0.0
466,ng2o98k983k,http://www.youtube.com/channel/UCmShCxm__LasvUfqAWeB_wA,UCmShCxm__LasvUfqAWeB_wA,souless,Ugwb9MZsuHSehOO_TqJ4AaABAg,0,1530381740.0,hey im getting none when i run the python script in my terminal. the source link im using to scrape from is after ive logged into my companies schedule page. how would i be able to tell if they are blocking me from scraping the page?,none,,2020-07-07 23:38:15.185376,0.0
467,ng2o98k983k,http://www.youtube.com/channel/UC_bKFjhm1BB03TPUEX8C6Ew,UC_bKFjhm1BB03TPUEX8C6Ew,Luka Yu,UgzQ4glyJKv_J3Ezjph4AaABAg,0,1530256501.0,There is a difference between the result csv file you showed in this video and the actual csv file one scraped by using the code. I find there is an extra blank row between each iteration. Can you show me how to remove the blank row in the code?  Thank you.,none,,2020-07-07 23:38:15.186321,0.0
468,ng2o98k983k,http://www.youtube.com/channel/UCuwfbeWLta3HALTFUyLftRA,UCuwfbeWLta3HALTFUyLftRA,Kumar Jitendra,UgzsjajZjZ_Cotk4u6N4AaABAg,1,1529942982.0,Thank you so much Corey for this beautiful video. You taught such a vast topic in a very very simple manner. It's really appreciable.,none,,2020-07-07 23:38:15.186321,0.0
469,ng2o98k983k,http://www.youtube.com/channel/UCZA1THh5vp8HiGkfpa-V49Q,UCZA1THh5vp8HiGkfpa-V49Q,Mohammed Zia,UgxhlG-8ot1ulKnU1Qd4AaABAg,0,1529686790.0,"Thanks Corey for this excellent tutorial. I tried this in Sublime Text and got an Import Error: No module named bs4. But when I did the same using a normal .py file and executed the python script from terminal, I am getting the result. Why the SUBLIME text is not recognizing the BeautifulSoup package ? how to fix this ?",none,,2020-07-07 23:38:15.186321,1.0
470,ng2o98k983k,http://www.youtube.com/channel/UCXcpv66MIVvOkE1j37S_ieQ,UCXcpv66MIVvOkE1j37S_ieQ,Isaac Hen,Ugx3BF12acoarn2ut_R4AaABAg,0,1529604933.0,"Is there a way just to print the content of the page? this is my code



import requests
from bs4 import BeautifulSoup

r = requests.get(""https://zillow.com"")
c = r.content
soup = BeautifulSoup(c, ""html.parser"")

print(soup.prettify())

and then i got the code for the website",none,,2020-07-07 23:38:15.186321,0.0
471,ng2o98k983k,http://www.youtube.com/channel/UCcKVPjhQjKqHd_TRWJdXAHA,UCcKVPjhQjKqHd_TRWJdXAHA,Jeff Myers,UgxaZ3PxBr3PjNnYAht4AaABAg,0,1529506194.0,"Help! I'm getting this error after print( soup ):

File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py"", line 57, in <module>
    for k, v in dis.COMPILER_FLAG_NAMES.items():
AttributeError: module 'dis' has no attribute 'COMPILER_FLAG_NAMES'

What does this mean?? I can't continue working with the video until this works!",none,,2020-07-07 23:38:15.186321,0.0
472,ng2o98k983k,http://www.youtube.com/channel/UCJKsmzmlbVEWxGtihyVCgxQ,UCJKsmzmlbVEWxGtihyVCgxQ,Ram,UgyhiDoQKnyDm0CRMrV4AaABAg,0,1528820046.0,hi corey can we extract  youtube videos url with python ?,none,,2020-07-07 23:38:15.186321,0.0
473,ng2o98k983k,http://www.youtube.com/channel/UCWfBYkkOJ7KMblWaQErD6Bw,UCWfBYkkOJ7KMblWaQErD6Bw,Maor Hajaj,UgzIzLevwfeKECGY8bp4AaABAg,1,1528737710.0,The best video I found on YouTube. Search for a long long time but this is simple one the most understood one and the best one thank you man,none,,2020-07-07 23:38:15.186321,0.0
474,ng2o98k983k,http://www.youtube.com/channel/UCLveEwYtxp0SYrHgEM43tjA,UCLveEwYtxp0SYrHgEM43tjA,Lucy H,UgycWkRhgy1hSSjZwmN4AaABAg,0,1528700721.0,"i have been trying this on different websites and each time i run the code it comes back with 'none', what am i doing wrong?",none,,2020-07-07 23:38:15.186321,0.0
475,ng2o98k983k,http://www.youtube.com/channel/UCG9bUN94IcgbKFg61Gjugjg,UCG9bUN94IcgbKFg61Gjugjg,Chakib BENHABIB,UgywcNw5d1pRi83OH8B4AaABAg,0,1528307221.0,"Hi Corey, thanks for your video. Just a small late feedbacks, you could avoid the try/except block by accessing your dictionnary with a method get articledict.get('src', None). Also, regular expressions should allow you to parse the video id in a strait forward manner. That's said, thx again for your work",none,,2020-07-07 23:38:15.186321,0.0
476,ng2o98k983k,http://www.youtube.com/channel/UCnmOLrx8Be_IeuSq0qWN-HA,UCnmOLrx8Be_IeuSq0qWN-HA,Nick Spiess,UgxfJVrJByEEte2CThp4AaABAg,0,1528290325.0,"I'm getting a UnicodeEncodeError when importing the original text from your site saying 'acsii' codec can't encode character'\xbb' in position 2423.  I've found multiple issues like this online, but can't figure out how to convert that character into something that is readable.",none,,2020-07-07 23:38:15.186321,0.0
477,ng2o98k983k,http://www.youtube.com/channel/UCLzErsLaDogEh2lBAl3CNoA,UCLzErsLaDogEh2lBAl3CNoA,Sam Dave Pollard,Ugyr0FPhKDNuWbXKaH94AaABAg,103,1528220796.0,"45 minutes of pure, unadulterated, hardcore geek-porn.
Absolutely loved it.
Thanks, Corey. Even by your standards this one was pretty special.",none,,2020-07-07 23:38:15.186321,3.0
478,ng2o98k983k,http://www.youtube.com/channel/UC4ja-vnGxv490h92JASPCcQ,UC4ja-vnGxv490h92JASPCcQ,Ranco Xu,UgxaPgRaymSqkNcRfex4AaABAg,1,1528025169.0,Thx a lot!,none,,2020-07-07 23:38:15.186321,0.0
479,ng2o98k983k,http://www.youtube.com/channel/UCICP8TZP01eh7NQqgEBdhMA,UCICP8TZP01eh7NQqgEBdhMA,Samsri Champ,Ugxd4tvzVRjhE6tdUO14AaABAg,0,1527836941.0,please make one video on restful also,none,,2020-07-07 23:38:15.186321,0.0
480,ng2o98k983k,http://www.youtube.com/channel/UCBnHfgnUPcgZvUsMvLYiEgw,UCBnHfgnUPcgZvUsMvLYiEgw,KeyNCar,UgxGgi-2nxN2U4x2Dz94AaABAg,0,1527581843.0,"Corey... I have struggled to find a way to scrape table data from a website that generates tables using javascript.  I finally found a solution to my dilemma.  I am using Selenium, Beautiful Soup, and Pandas.   It would be great if you could build a teaching tutorial to explain why it is so difficult to access tables generated by javascript and explain why this code below solves this problem.   cheers...   
        html = self.driver.execute_script(""return document.getElementsByTagName('html')[0].innerHTML"")
        soup = BeautifulSoup(html,'lxml')
        table = soup.find_all('table')[0]
        df = pd.read_html(str(table))[0]",none,,2020-07-07 23:38:15.187372,0.0
481,ng2o98k983k,http://www.youtube.com/channel/UCXDdfxaX74-1dvRDVEYFnWA,UCXDdfxaX74-1dvRDVEYFnWA,Father Hotdog,Ugy_xo6N4oZVE5eQwbR4AaABAg,0,1527555959.0,"Hi Cory. Another excellent video. I am just curious. With writing a scraping script to bring in some basic data such as: articles, videos, images etc. It is pretty easy to simply go in and copy and paste or do a save as.

How is web scraping need and used in the real world?  is this how real-estate site and job sites pop-up all over the place?  Do a lot of companies simply scrape data from a competitor's site and re-package the data instead of purchasing access to a database?

Where is the line between simply collecting data and being considered a hacker?",none,,2020-07-07 23:38:15.187372,2.0
482,ng2o98k983k,http://www.youtube.com/channel/UCN5JXvkgxDcgRi15Qk1wONg,UCN5JXvkgxDcgRi15Qk1wONg,Mauladous Basantes,UgxtiZrkSNNSiGQ8nTJ4AaABAg,0,1527319150.0,Woow what a great video.,none,,2020-07-07 23:38:15.187372,0.0
483,ng2o98k983k,http://www.youtube.com/channel/UCd7ZnGnyw02QyssQLxMYgrg,UCd7ZnGnyw02QyssQLxMYgrg,Ramisa Anjum Aditi,UgwmuKJVofmhxnza7Rx4AaABAg,1,1527267935.0,"Yours tutorials are much easier than programming books. Thanks a lot.
I was thinking of building  a program that would show the title of the latest episode of my favorite cartoon. But hearing your last advice , I'm a bit worried :(",none,,2020-07-07 23:38:15.187372,0.0
484,ng2o98k983k,http://www.youtube.com/channel/UCkHYqyR5L3d7CuM37koP3vg,UCkHYqyR5L3d7CuM37koP3vg,J Shepin,UgxHGnxNrr2kRF0aRYF4AaABAg,0,1527267401.0,"If you cant install lxml than run this command instead ""pip3 install lxml""",none,,2020-07-07 23:38:15.187372,0.0
485,ng2o98k983k,http://www.youtube.com/channel/UCrI_tQzRcvtJybKmNlMdrRg,UCrI_tQzRcvtJybKmNlMdrRg,Einsteincy,UgwtbFbxcMblKej5ln94AaABAg,359,1527229675.0,"Hello,I'm a viewer from China, comparing with other youtube python tutors, your English speaking is very clear for me to understand. I just registered as a one dollar patron for a little support.If I can find a job as programer later,  I will contribute more to help you continue making these high quality videos. Thank you Mr Corey!",none,,2020-07-07 23:38:15.187372,11.0
486,ng2o98k983k,http://www.youtube.com/channel/UCI11bCkx9tkDYywRAx69IXA,UCI11bCkx9tkDYywRAx69IXA,Jithu Nair,UgyePHQFTzqOoyqkHxN4AaABAg,0,1526990163.0,"I really desperately need this! Hope this helps me. :( I am somewhat stuck in a situation, thank you for this.",none,,2020-07-07 23:38:15.187372,0.0
487,ng2o98k983k,http://www.youtube.com/channel/UCXasLuUB9PjCdb0lTX_e0Rw,UCXasLuUB9PjCdb0lTX_e0Rw,Vertigo 101,Ugw3wSr1nVM1ic_nIkV4AaABAg,0,1526890312.0,"great video, what I like to do is scrape the website and deploy on my server and use it as a rest api via node js, really easy too",none,,2020-07-07 23:38:15.187372,0.0
488,ng2o98k983k,http://www.youtube.com/channel/UCwdbPn45YRlE2dKF6vvTa4A,UCwdbPn45YRlE2dKF6vvTa4A,Kid Amador,UgyLMugP-7Sd7FepJ_x4AaABAg,0,1526793830.0,What’s the difference between Requests and BeautifulSoup,none,,2020-07-07 23:38:15.187372,1.0
489,ng2o98k983k,http://www.youtube.com/channel/UC-VsljS0tJd_xDA25uvHfpw,UC-VsljS0tJd_xDA25uvHfpw,Petrock508,UgzFNOJDN5rx5-g07K54AaABAg,0,1526660722.0,"Great vid, thanks!

How do you deal with a tag that has more than one class?

e.g. <h2 class =a b c>",none,,2020-07-07 23:38:15.187372,0.0
490,ng2o98k983k,http://www.youtube.com/channel/UC-qdRNkTyLcBz-IqaaXtQBQ,UC-qdRNkTyLcBz-IqaaXtQBQ,Prince Kumar,UgzAdMwzLiM50EBGHqh4AaABAg,0,1526477062.0,"I liked your video very much. I have one question, could you please tell me how to navigate through the web pages to extract the data. For example: navigating through the web page by clicking on Next button. Looking forward for your reply.",none,,2020-07-07 23:38:15.187372,0.0
491,ng2o98k983k,http://www.youtube.com/channel/UCtPJrqWeV8egl7GVc5MSWXw,UCtPJrqWeV8egl7GVc5MSWXw,TruthSikher,Ugz8U5PM1H_euzXfAJB4AaABAg,0,1526388074.0,"What would people suggest as a good way of learning this? I understand how things work in the video, but end up forgetting them later.",none,,2020-07-07 23:38:15.187372,0.0
492,ng2o98k983k,http://www.youtube.com/channel/UCWWdBoVpqq9lgKFzncwrfjA,UCWWdBoVpqq9lgKFzncwrfjA,profetik777,Ugz4x46O5AMWsNmrTfl4AaABAg,0,1526167056.0,thanks so much man!,none,,2020-07-07 23:38:15.187372,0.0
493,ng2o98k983k,http://www.youtube.com/channel/UCItod6gn-jEDThkdktYeizQ,UCItod6gn-jEDThkdktYeizQ,Elvis Bicharri,UgzrT6mqaPRVcrWrGnl4AaABAg,0,1526122063.0,"Hey Corey, great tutorial and intro to scraping I learnt a great deal. Do you have a guide on how to scrape multiple pages? In the video it scrapes only the articles on your home page, but how do we go about scraping the other pages 2, 3, 4, ... etc? Thanks!",none,,2020-07-07 23:38:15.187372,2.0
494,ng2o98k983k,http://www.youtube.com/channel/UCt1MgjyuMnLWPbKZvT78cKw,UCt1MgjyuMnLWPbKZvT78cKw,Edson Araújo,UgxtYq0RKhaPWRx9MFF4AaABAg,0,1525464704.0,Very good Corey! Thank you,none,,2020-07-07 23:38:15.188369,0.0
495,ng2o98k983k,http://www.youtube.com/channel/UC9YgrQMMxsimeF3hQJakr0Q,UC9YgrQMMxsimeF3hQJakr0Q,Nishant Kashyap,UgzHUJI2fyb4bgWoPo54AaABAg,3,1525438484.0,"Man..The way you explains !!! It works like a charm. Be it Regex or web scarping, it's just awesome ! Love your posts.",none,,2020-07-07 23:38:15.188369,0.0
496,ng2o98k983k,http://www.youtube.com/channel/UCunmimB2KYJ4MnNAUhWTpLA,UCunmimB2KYJ4MnNAUhWTpLA,Han Jiang,UgwI3UMtr178hn-Qhad4AaABAg,0,1524762789.0,Corey your fucking awesome!,none,,2020-07-07 23:38:15.188369,0.0
497,ng2o98k983k,http://www.youtube.com/channel/UCLCem5lPZAyIbO8gJbOHdYw,UCLCem5lPZAyIbO8gJbOHdYw,Elliott Carter,Ugx2plY7Nw0uw-aQAvR4AaABAg,1,1524681837.0,This was a great video Corey. It was a helpful refresher for me which in turn will help me with a python web app project i'm working on. Thanks again!,none,,2020-07-07 23:38:15.188369,0.0
498,ng2o98k983k,http://www.youtube.com/channel/UCdW3d3_kXVnUYUIgQePtzBA,UCdW3d3_kXVnUYUIgQePtzBA,MD Kawsar Ahmed Jami,UgxmawmfdNUCsBc49oN4AaABAg,0,1524531679.0,"web scraping is great! i can do any scaping with pythin and scrapy for you,
see my profile here https://goo.gl/iTHgqT",none,,2020-07-07 23:38:15.188369,0.0
499,ng2o98k983k,http://www.youtube.com/channel/UCbkFwxof9bVRnU2UmC1ukkA,UCbkFwxof9bVRnU2UmC1ukkA,Donovan Keating,Ugy3qHy8PKylcZ81PFV4AaABAg,0,1524159905.0,"On 24:42 , why isn't the header tag specified too?",none,,2020-07-07 23:38:15.188369,0.0
500,ng2o98k983k,http://www.youtube.com/channel/UCbkFwxof9bVRnU2UmC1ukkA,UCbkFwxof9bVRnU2UmC1ukkA,Donovan Keating,UgyRVemruzWwhNDPOg54AaABAg,0,1524154694.0,Love your tutorials man! You thinking of makinh Django tutorials any time soon?,none,,2020-07-07 23:38:16.102098,0.0
501,ng2o98k983k,http://www.youtube.com/channel/UCfEyg5O_JAllSNL3XIJjrew,UCfEyg5O_JAllSNL3XIJjrew,Youssef Esseddiq,Ugw3dtO2dQikue9IFT94AaABAg,0,1524043353.0,Thanks bro for this but how we parse a ol it always gives me none :/ ! In fact when I print the whole html code we parsed those <ol>'s don't even show up.,none,,2020-07-07 23:38:16.102098,0.0
502,ng2o98k983k,http://www.youtube.com/channel/UC4RzcWvvBXx9JWC2G_dpiAA,UC4RzcWvvBXx9JWC2G_dpiAA,R P,UgxM9KdxSsmMh2t3apx4AaABAg,0,1523620649.0,Thank you very much for this video- best tutorial on BeautifulSoup I've come across!,none,,2020-07-07 23:38:16.102098,0.0
503,ng2o98k983k,http://www.youtube.com/channel/UCqhouPospdSoMmPyEA5zyLg,UCqhouPospdSoMmPyEA5zyLg,Andrei Bica,UgwvKagHJGclyFW9rKF4AaABAg,0,1523475177.0,"great detailed explanation, i like your music too, it makes everything so much more clear",none,,2020-07-07 23:38:16.102098,0.0
504,ng2o98k983k,http://www.youtube.com/channel/UCFyqrUg5CI4jN6ji5e8vE3A,UCFyqrUg5CI4jN6ji5e8vE3A,Nehat Khan,UgzJwuxGoKnw7TTXJKx4AaABAg,0,1523458373.0,Man you are awesome. Your videos are very well detailed and easy to understand.  Feeling lucky to found you brother.  Thanks a lot! .  Keep up the good work,none,,2020-07-07 23:38:16.102098,0.0
505,ng2o98k983k,http://www.youtube.com/channel/UCxziQ5069TCwNWOIncfQifQ,UCxziQ5069TCwNWOIncfQifQ,Chandan Pradhan,UgxF_UfHb27hA55ch-F4AaABAg,1,1523344014.0,"Great, you are great teacher in this world",none,,2020-07-07 23:38:16.103214,0.0
506,ng2o98k983k,http://www.youtube.com/channel/UCJSz5tEDs9_vO06STp_ZKNg,UCJSz5tEDs9_vO06STp_ZKNg,TheEye?,Ugy2Bj_ObvgtGDkM3lN4AaABAg,0,1523296090.0,"Oh my god, I love you. Subbed. Thank you SO much. That was just amazing.",none,,2020-07-07 23:38:16.103214,0.0
507,ng2o98k983k,http://www.youtube.com/channel/UC0S7i0W_tqeNQOQTseOLN5g,UC0S7i0W_tqeNQOQTseOLN5g,李永胜,UgzGB1KsqsNXTBk2TvV4AaABAg,0,1523275677.0,"Hi Corey, your python tutorial videos are really helpful, and I've learned a lot from these. I've got a problem with grabbing info from web written by javascript. it's seems BeautifulSoup can't parse javescrip code(<a title=""go to page 2"" href=""javascript:__doPostBack('ctl00$ContentPlaceHolder1$pager','2')"">2</a>). So, would you pls post a video to show how to solve this kind of problem? using headless browser or manually analyze javascript code before sending further requests, any kind of solution is OK. Just want to know how you solve this kind of problem. THANKS!",none,,2020-07-07 23:38:16.103214,0.0
508,ng2o98k983k,http://www.youtube.com/channel/UChHq04qPaKvsSEOA8axa_nQ,UChHq04qPaKvsSEOA8axa_nQ,525gigidy,UgzBdDiqBsErcUkyV754AaABAg,0,1522942018.0,how did you pull up the thing on the right side of your screen? I got chrome so I can see the parts of websites but I dont understand how you went from the command line installing pips to black thing on the right of your screen,none,,2020-07-07 23:38:16.103214,2.0
509,ng2o98k983k,http://www.youtube.com/channel/UCJ8feJZNtpPpEZiS4C-Dx_A,UCJ8feJZNtpPpEZiS4C-Dx_A,LIAM ENEUK,UgwRh2gQdEAXx8f0bMR4AaABAg,0,1522495065.0,Hello. Can this technique be used to monitor website changes? Thanks!,none,,2020-07-07 23:38:16.103214,2.0
510,ng2o98k983k,http://www.youtube.com/channel/UCXR8BUoKmC6fuHtyNQ1NWEg,UCXR8BUoKmC6fuHtyNQ1NWEg,Jitendra Trivedi,UgxO_0mR-vh9E8CyKI54AaABAg,0,1521919357.0,"Great Video, Any video series on EDA with pandas ..",none,,2020-07-07 23:38:16.103214,0.0
511,ng2o98k983k,http://www.youtube.com/channel/UC0l-FXkSCwMlp26bxnSLSjA,UC0l-FXkSCwMlp26bxnSLSjA,James Darlack,UgyRcS1sk1Xdy6Ogied4AaABAg,0,1521620371.0,EXCELLENT!    Nice concise intro.,none,,2020-07-07 23:38:16.103214,0.0
512,ng2o98k983k,http://www.youtube.com/channel/UCDUdphnU6mbdY4lGukEWeoA,UCDUdphnU6mbdY4lGukEWeoA,Ankit Chouhan,UgzdC6imcq85D3xffNN4AaABAg,0,1521339374.0,Great Video !! Just edited your code to scrap all 12 pages information of your website.,none,,2020-07-07 23:38:16.104092,0.0
513,ng2o98k983k,http://www.youtube.com/channel/UCq0ekD_O6TDFcSjZF2HimUQ,UCq0ekD_O6TDFcSjZF2HimUQ,Japan Loves,UgyZn1qIKjYBBik7f_Z4AaABAg,0,1521161270.0,Can you suggest a proxy to protect my IP address when scraping sites data?,none,,2020-07-07 23:38:16.104092,0.0
514,ng2o98k983k,http://www.youtube.com/channel/UClw-puyseuEBmIS_4S8mDYg,UClw-puyseuEBmIS_4S8mDYg,Vincent Tern,UgylBGQYEYarf6jJ77d4AaABAg,0,1521160984.0,"After storing the data in csv file, how would I export it to be displayed on my web application for users to see?",none,,2020-07-07 23:38:16.104092,0.0
515,ng2o98k983k,http://www.youtube.com/channel/UCZxZDsT0J1qNBcdElGlWKsQ,UCZxZDsT0J1qNBcdElGlWKsQ,Rajnil Guha,UgxO5DM18NAnm_bX9u94AaABAg,0,1520872496.0,Corey could you please see if it would be possible for you to make a series of django videos.,none,,2020-07-07 23:38:16.104092,0.0
516,ng2o98k983k,http://www.youtube.com/channel/UC-xznyMt8Bc-iiF8FfHNYSg,UC-xznyMt8Bc-iiF8FfHNYSg,sudipta samanta,UgyID8TI4sx6ZLBY-hp4AaABAg,0,1520687152.0,"Hi Corey I really love your tutorials. I have a question.. when you said that for using a new library we should go through the documentation. Now, I tried that and when I open the documentation it's just overwhelming. So how should I look at the documentation and what are the things I should look at ?",none,,2020-07-07 23:38:16.104092,0.0
517,ng2o98k983k,http://www.youtube.com/channel/UCMjpQDETEOfiCfHtDa78hRw,UCMjpQDETEOfiCfHtDa78hRw,Cody Davis,Ugw7TJ0iAFeIbDC_bDZ4AaABAg,0,1520215630.0,One question I have is when splitting the links for a youtube video yours was split from '?'. is that because it is embedded in a page? Just looking at a video from youtube you would have to split from 'v=',none,,2020-07-07 23:38:16.104092,1.0
518,ng2o98k983k,http://www.youtube.com/channel/UClDKlYiW1iXmek65PQ9yECg,UClDKlYiW1iXmek65PQ9yECg,Not Gate,UgyAmK636k7EhMP6F0d4AaABAg,0,1520196989.0,"Easy one-liner for the link once you get the source. 

link = 'youtu.be/'+re.search(r'embed/(.*)\?',src).group(1)

and for printing

print(headline,summary,link,'\n',sep='\n')

great video as always :)",none,,2020-07-07 23:38:16.104092,0.0
519,ng2o98k983k,http://www.youtube.com/channel/UCZSx1JbREWz1m0hGAppj3Yw,UCZSx1JbREWz1m0hGAppj3Yw,Jeff the Pharmacist,UgyeuoIO_sXdSBHtMMl4AaABAg,0,1519954900.0,"I keep getting an error ""no module named bs4"" anybody know why?",none,,2020-07-07 23:38:16.104092,1.0
520,ng2o98k983k,http://www.youtube.com/channel/UCQeMrFaKr6iUrRp-4W_UXFg,UCQeMrFaKr6iUrRp-4W_UXFg,Weiting Qi,UgySqej7t6dEpWVPio14AaABAg,0,1519798641.0,Can you tell us how to scrape multiple pages that have some patterns,none,,2020-07-07 23:38:16.104092,0.0
521,ng2o98k983k,http://www.youtube.com/channel/UCUGdWaG1Gp5P92IKB26t1dA,UCUGdWaG1Gp5P92IKB26t1dA,A 66,UgyZvRgm7jvFL5tdVil4AaABAg,0,1519698975.0,Please do tutorials in django..Anyone could excel in programming if they watch your videos.,none,,2020-07-07 23:38:16.104092,0.0
522,ng2o98k983k,http://www.youtube.com/channel/UC0ojJjrRy0L3s34JvuPzaCg,UC0ojJjrRy0L3s34JvuPzaCg,Mahender Thakur,Ugwc4JiLUhQ6R5C9Z4d4AaABAg,0,1519545029.0,for summary  we could have also used : summary = article.div.p.text,none,,2020-07-07 23:38:16.104092,0.0
523,ng2o98k983k,http://www.youtube.com/channel/UC1Z8jn3UzqzL3rLUOvXmH-A,UC1Z8jn3UzqzL3rLUOvXmH-A,Arjun Gaihre,UgzpR3wsCP3UdEomafp4AaABAg,0,1519321598.0,"Hey Corey, I found this tutorial very helpful. I run your code at my environment with all the libraries installed, and I got this output:
Python Tutorial: Context Managers – Efficiently Managing Resources
In this Python Programming Tutorial, we will be learning how to use context managers to properly manage resources. Context Managers are great for when we need to setup or teardown some resources during use. So these can be used for: open and closing files, opening and closing database connections, acquiring and releasing locks, and much much more. Let’s get started…
https://youtube.com/watch?v={vid_id}

What's missing here is {vid_id}...instead of getting the ID, I got this . Can you please help me out with this? Thank you",none,,2020-07-07 23:38:16.104092,3.0
524,ng2o98k983k,http://www.youtube.com/channel/UCKsSjTXYcMY0m8dfGoeW2BQ,UCKsSjTXYcMY0m8dfGoeW2BQ,Ashish M,UgwG6b4YkM1LRpcRZ5h4AaABAg,2,1519227302.0,It's nice how you break down the subject into small increments that others can understand and build upon easily. True quality of a great teacher. Thank you!,none,,2020-07-07 23:38:16.104092,0.0
525,ng2o98k983k,http://www.youtube.com/channel/UCDNzCl0z5d_i3A6QlCIKU4g,UCDNzCl0z5d_i3A6QlCIKU4g,crypto king,UgwVp0LeurBDVptDP2F4AaABAg,6,1519167812.0,this guy for president...thats all i can say.....WOW..JUST WOW.,none,,2020-07-07 23:38:16.105101,0.0
526,ng2o98k983k,http://www.youtube.com/channel/UCVIoHXkfoUHYFMGL5R1Fwrw,UCVIoHXkfoUHYFMGL5R1Fwrw,daryl sato,Ugwc-UOeblMT1D0a0r94AaABAg,0,1519163381.0,"Corey, echo great video!  Just started using Python.  Can follow your logic but I keep getting No Module named BeautifulSoup errors.  I believe this to be a set up issue (file, folder, location..).   Searched internet in vain but can not seem to find a tutorial that is clear, crisp, concise..  on set up.  I'm running Windows and have installed Python 2.7 and Python 3.6.   I installed all pkg's with success using your Atom tutorial.  help?",none,,2020-07-07 23:38:16.105101,1.0
527,ng2o98k983k,http://www.youtube.com/channel/UCiOBOYZltnK7w6OJzfTuajw,UCiOBOYZltnK7w6OJzfTuajw,hashcoder,UgwgLqDfIIs1f6bIKU54AaABAg,5,1518817370.0,"I've tried today! Successfully. Thank you, Corey!",none,,2020-07-07 23:38:16.105101,0.0
528,ng2o98k983k,http://www.youtube.com/channel/UC63CkVeWlO0JEU0IxXeWPdQ,UC63CkVeWlO0JEU0IxXeWPdQ,PocketFPS,UgySF12Y-UOXpDyIJ6d4AaABAg,0,1518709867.0,How do you do this man!,none,,2020-07-07 23:38:16.105101,0.0
529,ng2o98k983k,http://www.youtube.com/channel/UCXxvp_mme5drdvEchBjRcBA,UCXxvp_mme5drdvEchBjRcBA,ismail hammounou,Ugw7Y1K57AKU_c5-UuN4AaABAg,2,1518613018.0,You Are Really Good. The Best of The Best. What about scrapy ? Multiprocessing ?  Django ? You really deserve money for this Work. My respect ! chapeau fréro,none,,2020-07-07 23:38:16.105101,1.0
530,ng2o98k983k,http://www.youtube.com/channel/UCS0Huu2dxo9ZzjYC_2QO1oA,UCS0Huu2dxo9ZzjYC_2QO1oA,Geeky Programmer,UgwHvNBHPfg6_ul9QUl4AaABAg,63,1518448917.0,Best Python WebScraping video on YouTube! Seriously brother you deserve money for this masterpiece! Keep it up :'),none,,2020-07-07 23:38:16.105101,4.0
531,ng2o98k983k,http://www.youtube.com/channel/UCotDEfMqXn85QkRm0LRmGkw,UCotDEfMqXn85QkRm0LRmGkw,Failure Great,Ugx11DEFSRSXtrynNkN4AaABAg,155,1518399638.0,"you never fail to impress me with your teachings! I don't watch videos at all once, because I feel it's easier to learn watching by 'parts' — but again and again, I'm watching the videos and I feel 'wow.. this is really well done, I have to like this, it's the least I can do' and I realized I already liked before lol",none,,2020-07-07 23:38:16.105101,1.0
532,ng2o98k983k,http://www.youtube.com/channel/UCBS8gkd36q4bglJhuZp8yOA,UCBS8gkd36q4bglJhuZp8yOA,Its KirA,UgwIw-ec6hqlGuzhQG94AaABAg,0,1518088001.0,Web scraping from multiple websites?,none,,2020-07-07 23:38:16.105101,0.0
533,ng2o98k983k,http://www.youtube.com/channel/UC_APJPbfWEzF4a3h9Ye7ncQ,UC_APJPbfWEzF4a3h9Ye7ncQ,DonVTOL,Ugy8wwq0AhuSgTuXsYt4AaABAg,0,1518011030.0,How did you not get the UnicodeEncodeError?,none,,2020-07-07 23:38:16.105101,1.0
534,ng2o98k983k,http://www.youtube.com/channel/UCq5X8culCH25xmHR-BSS8_Q,UCq5X8culCH25xmHR-BSS8_Q,Geoff Groves,UgzGw2vGTqzHxwEdOGV4AaABAg,1,1517827796.0,"Great Corey! Something of note for new MAC / Py 3 users: If you have Python 3 installed, then you have to specify pip3 (pip3 install...)  in your terminal commands for all library installs (soup, requests, html5lib etc) as MAC OSx ships with PY 2.7.   Also, if you are building in Sublime as Corey does, you need to install a python 3 build package, otherwise Sublime will try to build it with Py 2.",none,,2020-07-07 23:38:16.105101,0.0
535,ng2o98k983k,http://www.youtube.com/channel/UCiq7nJWYHau4BnN5wUv3zsA,UCiq7nJWYHau4BnN5wUv3zsA,Kamal Sahoo,Ugx_6J_sPbYhDSSnCQB4AaABAg,1,1517435002.0,"Hi Corey.

Thanks a lot.

Traceback (most recent call last):
  File ""scrap.py"", line 22, in <module>
    summary = div.find('div', class_='headerAbNormal').p.text
AttributeError: 'NoneType' object has no attribute 'text'

How to fix the issue. Please guide me.",none,,2020-07-07 23:38:16.105101,2.0
536,ng2o98k983k,http://www.youtube.com/channel/UCYKNSZLOvxI_30yN--LurnA,UCYKNSZLOvxI_30yN--LurnA,Nimble Berto,Ugwoiw_ryFByHflbe054AaABAg,0,1517411226.0,What about pushing  info we scrape into an app?,none,,2020-07-07 23:38:16.105101,0.0
537,ng2o98k983k,http://www.youtube.com/channel/UC7EwRlbv6ssGDGYKl8Ed2qQ,UC7EwRlbv6ssGDGYKl8Ed2qQ,arabic Mitnich,Ugxc9InKlTSGup5luTR4AaABAg,0,1517331060.0,can u do mechanize,none,,2020-07-07 23:38:16.105101,0.0
538,ng2o98k983k,http://www.youtube.com/channel/UCTy5U8xLsHt7ZB0T9bn89Aw,UCTy5U8xLsHt7ZB0T9bn89Aw,DistortedV12,UgzMz03sMwMemTLVDtJ4AaABAg,0,1517240388.0,can't you use css selectors too?,none,,2020-07-07 23:38:16.106095,0.0
539,ng2o98k983k,http://www.youtube.com/channel/UCSGTGQczWTkNPosrWT3rjYQ,UCSGTGQczWTkNPosrWT3rjYQ,ming hao Tao,UgxiuYdm_qqpp6zUQ7V4AaABAg,0,1517188982.0,Awesome video! Thank you!,none,,2020-07-07 23:38:16.106095,0.0
540,ng2o98k983k,http://www.youtube.com/channel/UC2fsjWT-BqqMui-GClM6EBw,UC2fsjWT-BqqMui-GClM6EBw,Oriental Melodies 2010,UgyYQsm-RarBF446WR14AaABAg,4,1516841021.0,An awesome video. Your way of moving from one idea to another is amazing. I have been searching YouTube for years for a good tutorial video explaining web scrapping with BeautifulSoup and finally found it. Thanks a million!,none,,2020-07-07 23:38:16.106095,0.0
541,ng2o98k983k,http://www.youtube.com/channel/UCyADaqNwCXISkdwxbl2xfIw,UCyADaqNwCXISkdwxbl2xfIw,Dylan Duregger,UgymspOfhH9V8Usyzz94AaABAg,0,1516839645.0,"What do you do if you want to get data from a tag, but there are multiple of the same exact tag? 
for example:
<div id=""sortable"">
     <td align=""right"">27</td>
     <td align=""right"">30</td>
     <td align=""right"">19</td>
If i just want the second one how would I do that?
If i use soup.find(""div"", id=""sortable"") it only comes up with the first one? What would i do if i just want the middle or last one?",none,,2020-07-07 23:38:16.106095,2.0
542,ng2o98k983k,http://www.youtube.com/channel/UCtumVQcZbim7J-6jxT88wtg,UCtumVQcZbim7J-6jxT88wtg,Karishma Shaik,UgwM4t8uOm55e4Qvm7x4AaABAg,0,1516803783.0,Hi Corey ! Your videos are extremely useful. Could you please videos of django framework if any on YouTube. Thanks!!,none,,2020-07-07 23:38:16.106095,1.0
543,ng2o98k983k,http://www.youtube.com/channel/UC6jnjHaidfLrCjOPQK_Ah8g,UC6jnjHaidfLrCjOPQK_Ah8g,Dr. Yashveer Yadav,Ugzk6sR1NjotstbH2zd4AaABAg,0,1516721313.0,Very nice and explained wonderfully...,none,,2020-07-07 23:38:16.106095,0.0
544,ng2o98k983k,http://www.youtube.com/channel/UCm3lWFRlfe2ao4fcAaHde0Q,UCm3lWFRlfe2ao4fcAaHde0Q,corpknut80,UgxJ-jA2lQKfFsDX3714AaABAg,0,1516547904.0,"My f*** god, the underscore!! I have been trying for hours to use the class tag!! Thank you !!!!!!",none,,2020-07-07 23:38:16.106095,0.0
545,ng2o98k983k,http://www.youtube.com/channel/UC6IJXlYcOFVrwjHogm_Szpg,UC6IJXlYcOFVrwjHogm_Szpg,Dylan Duregger,Ugz_XOW0TojQAEgzh2J4AaABAg,0,1516543497.0,"Hey, I am having trouble with Sublimetext3 when I try to import requests. It keeps saying""no module names requests"". Anything i can to do fix it?",none,,2020-07-07 23:38:16.106095,2.0
546,ng2o98k983k,http://www.youtube.com/channel/UCkOdgaPQ64fxJT_XnrqyZ6Q,UCkOdgaPQ64fxJT_XnrqyZ6Q,kagome sakura,UgzCNw1tUj5oLrzi_8h4AaABAg,0,1516042175.0,"I am using PyCharm and and Python 2.7 I keep getting ""None"" back for the youtube links, any suggestions?",none,,2020-07-07 23:38:16.106095,0.0
547,ng2o98k983k,http://www.youtube.com/channel/UCfKuWWfdQ5Cq0J3EOw3Vcyw,UCfKuWWfdQ5Cq0J3EOw3Vcyw,Daniel Weikert,Ugz-ReyxMXMwjWIwcKJ4AaABAg,0,1515770752.0,Great work Corey. Your tutorials are really good. What if I need to scape an infinite scroll website with login data? I probably need to use scrapy or sth. similar. Can you do a tutorial for this as well? Thanks a lot,none,,2020-07-07 23:38:16.106095,2.0
548,ng2o98k983k,http://www.youtube.com/channel/UC9nQQnNOqZFfJu787gN8SyQ,UC9nQQnNOqZFfJu787gN8SyQ,Sujal Padhiyar,Ugzyi-P5ersiKTJM0c94AaABAg,0,1515735422.0,"Hello Corey, the way you explain & present commands and examples are very nice . Please upload some video related to data science like some libraries like numpy, pandas, skit-learn",none,,2020-07-07 23:38:16.106095,0.0
549,ng2o98k983k,http://www.youtube.com/channel/UCgBMW3792gvGGFBb1Dz0ANg,UCgBMW3792gvGGFBb1Dz0ANg,Sam P,UgzWt8C2t2RFOwUdGWZ4AaABAg,0,1515648595.0,This video is awesome! It was really helpful! You are really thorough and clear about explaining everything. Thank you!,none,,2020-07-07 23:38:16.106095,0.0
550,ng2o98k983k,http://www.youtube.com/channel/UC2Khdtu3IPF9z4zCxIzsk9A,UC2Khdtu3IPF9z4zCxIzsk9A,Matheus A. Fernandes,UgytkyG8MscZ2yBSHv54AaABAg,0,1515148868.0,Great video! Thanks!,none,,2020-07-07 23:38:16.106095,0.0
551,ng2o98k983k,http://www.youtube.com/channel/UCCvYQwPVQx3YNiUsQ-2Emsg,UCCvYQwPVQx3YNiUsQ-2Emsg,João Farinha,UgzdN-xgjvavm0CwiLF4AaABAg,0,1515097546.0,"Hi Corey. One question: When a create the CSV it's placing everything in one column. What am i doing wrong?  Btw Thank you very much for your help in the video, it was a really good explanation :)",none,,2020-07-07 23:38:16.106095,1.0
552,ng2o98k983k,http://www.youtube.com/channel/UCC5H0XHU1YSGsh2xd8kxj1g,UCC5H0XHU1YSGsh2xd8kxj1g,John Slater,Ugxe41WKX7FavsIIght4AaABAg,0,1515020656.0,A very fine video; well done.,none,,2020-07-07 23:38:16.107093,0.0
553,ng2o98k983k,http://www.youtube.com/channel/UCW6tL8IHsXSJEkyJZ1ziX-w,UCW6tL8IHsXSJEkyJZ1ziX-w,Apatten001,UgyMwvIWWa2iRwZE89l4AaABAg,0,1514577792.0,Very useful video!,none,,2020-07-07 23:38:16.107093,0.0
554,ng2o98k983k,http://www.youtube.com/channel/UCsYW_xyYZcmyjHsibgN85pQ,UCsYW_xyYZcmyjHsibgN85pQ,Ash Co,UgyKn_EaAAFY4soIyAl4AaABAg,1,1514559733.0,"I looked for videos for a straight week to no avail, until I finally found yours. This has been so helpful and exactly what I needed. Thank you so much!",none,,2020-07-07 23:38:16.107093,0.0
555,ng2o98k983k,http://www.youtube.com/channel/UC7EwRlbv6ssGDGYKl8Ed2qQ,UC7EwRlbv6ssGDGYKl8Ed2qQ,arabic Mitnich,Ugy0B3xyKg4lcswN3Hl4AaABAg,0,1514386708.0,can u help us with more network programming somthing big and from scratch,none,,2020-07-07 23:38:16.107093,0.0
556,ng2o98k983k,http://www.youtube.com/channel/UCY139Ne_tirhcG4DnPKu59A,UCY139Ne_tirhcG4DnPKu59A,Ahsin Shabbir,UgxappErqVwWtZJ19Jx4AaABAg,0,1514149541.0,"Very informative and clear tutorial. One thing I would like to add though is that the BeautifulSoup, requests, and lxml libraries can be added from the ""project interpreter"" in PyCharm. This is an alternative for those who are having issues with the ""pip install"" method of adding libraries.",none,,2020-07-07 23:38:16.107093,0.0
557,ng2o98k983k,http://www.youtube.com/channel/UCy8NqCaUypxSqm_VAZXTXEQ,UCy8NqCaUypxSqm_VAZXTXEQ,John Wood,UgwNS9094qN5T2_jV6x4AaABAg,0,1514035352.0,"Maybe this is more of a question about ajax calls, but I'm trying to scrap my school's class system (kind of like Canvas), take the assignments, and use the Todoist API to add them to my Todoist as tasks. The problem is obviously that the page is dynamically generated. Is there anything in particular that I should look for in order to get the call after?",none,,2020-07-07 23:38:16.107093,1.0
558,ng2o98k983k,http://www.youtube.com/channel/UCFa9N8wr9iaKoxRDaqJ6Wuw,UCFa9N8wr9iaKoxRDaqJ6Wuw,shankachu Coral,UgxIXUMalccs8DNv4Ep4AaABAg,0,1513674251.0,"Great video! I really enjoy it and learned a lot. What's your suggestion for the next step if I want to learn to scrap the whole website, not only a webpage?",none,,2020-07-07 23:38:16.107093,0.0
559,ng2o98k983k,http://www.youtube.com/channel/UC7xWdBtttht5EtH4m5dU7Kw,UC7xWdBtttht5EtH4m5dU7Kw,eduardo reis,Ugy8exEQhiAVQ91gXPZ4AaABAg,0,1513462158.0,Excelente video! Muito obrigado.,none,,2020-07-07 23:38:16.107093,0.0
560,ng2o98k983k,http://www.youtube.com/channel/UCM_0XIswX5DXr8EUS3i4wUQ,UCM_0XIswX5DXr8EUS3i4wUQ,Miguel Santos,Ugx-BN4noQdEwJu1V6h4AaABAg,1,1512923558.0,"Hey Corey! I'm new to Python and I really liked the video.
I have some few questions:
1. You said that if I want to scrape websites like Facebook, Twitter, etc., I need to use an API. So how do you use/implement an API to a Python Script?
2. Does adding a time-delay to my script good enough so that it wouldn't get blocked when scraping?
Thank you in advance!",none,,2020-07-07 23:38:16.107093,1.0
561,ng2o98k983k,http://www.youtube.com/channel/UCyZFKElOKLDWAPi3kItmsoQ,UCyZFKElOKLDWAPi3kItmsoQ,Vtorres Lopez,UgzC3LZ8_tG7yw9AB5B4AaABAg,0,1512736700.0,Thank you SR. for your great tutorials!!,none,,2020-07-07 23:38:16.107093,0.0
562,ng2o98k983k,http://www.youtube.com/channel/UCsfvLpnvn3S7hZ3ZuuUTOCA,UCsfvLpnvn3S7hZ3ZuuUTOCA,Martin Kaspar,Ugwv53wnoIQjBpjiOTJ4AaABAg,0,1512667454.0,hello - well it would be great if you show the code - that you develope - do you offer it at github somewhere !? that would be great-,none,,2020-07-07 23:38:16.107093,2.0
563,ng2o98k983k,http://www.youtube.com/channel/UCM5pHCao3DpbDbUYwz1hVrw,UCM5pHCao3DpbDbUYwz1hVrw,One Two,UgyIMuwrEnOf3iz-u7F4AaABAg,0,1512495794.0,When I run the code I get this: UnicodeEncodeError: 'ascii' codec can't encode character '\u2019' in position 231: ordinal not in range(128),none,,2020-07-07 23:38:16.107093,1.0
564,ng2o98k983k,http://www.youtube.com/channel/UCYOJL5j6clzJu0CwkPEc2ug,UCYOJL5j6clzJu0CwkPEc2ug,RiptorForever,UgwwznWxEnzhPXvTFvd4AaABAg,0,1512301385.0,Thank you for the lesson! From Pernambuco/Brasil,none,,2020-07-07 23:38:16.107093,0.0
565,ng2o98k983k,http://www.youtube.com/channel/UCz7k8I461QcbeTxudH3Bk5Q,UCz7k8I461QcbeTxudH3Bk5Q,Harold Thibault,UgwnYGVivohMJX7Av_R4AaABAg,0,1512116259.0,"That was a really good tutorial again !
Lots of interesting informations and advices indeed .",none,,2020-07-07 23:38:16.108088,0.0
566,ng2o98k983k,http://www.youtube.com/channel/UCJ-FcPUQk7dR2-yCPdgCXfQ,UCJ-FcPUQk7dR2-yCPdgCXfQ,Arif A,Ugw_0-IUEhvZlMwT6sx4AaABAg,0,1511702148.0,"This worked beautifully! Thank you. One question. Is there a way to limit, or define a boundary of text one can scrape from a website? For example, what if your website had thousands of blog posts, and I only wanted to scrape ten and not all? Is this possible?",none,,2020-07-07 23:38:16.108088,1.0
567,ng2o98k983k,http://www.youtube.com/channel/UCZYt0IFa5kA8y8u-CYH_Pbw,UCZYt0IFa5kA8y8u-CYH_Pbw,Ayush Choudhary,UgxVigpk3lLHHVauU9V4AaABAg,0,1511700818.0,How to scrape sites which require authentication?,none,,2020-07-07 23:38:16.108088,0.0
568,ng2o98k983k,http://www.youtube.com/channel/UCpN0scytHvH8nLPtmP3kWJA,UCpN0scytHvH8nLPtmP3kWJA,Tom Travolta,Ugzr160U4n5nf7fA10l4AaABAg,0,1511698855.0,"Hi Corey I am having this error after entering line 8 at timestamp 20:50 of the video:
""UnicodeEncodeError: 'ascii' codec can't encode character u'\xbb' in position 2678: ordinal not in range(128)""

I installed all the prerequisite packages and have used Python 2 and 3, nothing seems to work! Any ideas on how to fix this issue? Thanks!",none,,2020-07-07 23:38:16.108088,2.0
569,ng2o98k983k,http://www.youtube.com/channel/UCSNDHDGSl7tA79BaTckHUjQ,UCSNDHDGSl7tA79BaTckHUjQ,Escape the Matrix,UgwzFt-z_HZQJnmrTCd4AaABAg,0,1511651864.0,"Great tutorial, thanks!",none,,2020-07-07 23:38:16.108088,0.0
570,ng2o98k983k,http://www.youtube.com/channel/UC0ijs_zCuC6aKUJ3vt5rzaQ,UC0ijs_zCuC6aKUJ3vt5rzaQ,Mayukh Sarkar,Ugxxhjl-zT8BQlYe7AV4AaABAg,0,1511369829.0,"Hey Corey...awesome video man. Can you please do a video on python 3.6 async io functions. Everywhere I saw it, I didn't understand.",none,,2020-07-07 23:38:16.108088,0.0
571,ng2o98k983k,http://www.youtube.com/channel/UCjWhDRs-Ci5DHPVN_1ZxabQ,UCjWhDRs-Ci5DHPVN_1ZxabQ,anton gyurov,UgwMB7Lg24EjOOfSVmp4AaABAg,0,1511007170.0,great stuff!,none,,2020-07-07 23:38:16.108088,0.0
572,ng2o98k983k,http://www.youtube.com/channel/UC_2Cgpk3dl7tRDKAcwXQo0g,UC_2Cgpk3dl7tRDKAcwXQo0g,Sheldon Fourie,UgyrMxa6MFXzHYNqBDN4AaABAg,0,1511003688.0,how would we scrape images to a folder with a custom name from the site like h3 tag for the item and product code of the item and color tag - eg B300 productName Color.png and then @images in csv file link to that exact name,none,,2020-07-07 23:38:16.108088,0.0
573,ng2o98k983k,http://www.youtube.com/channel/UCISy2I4szB3Cnvm1lcISxxg,UCISy2I4szB3Cnvm1lcISxxg,Aditya Verma,UgxKgpqpxx-CgP_pd5B4AaABAg,0,1510930481.0,"@Corey Schafer, a video on Deep Copy vs Shallow Copy?",none,,2020-07-07 23:38:16.108088,0.0
574,ng2o98k983k,http://www.youtube.com/channel/UCWExkuo_FCTjKS-1yG_TsRg,UCWExkuo_FCTjKS-1yG_TsRg,H Pavan,Ugx3HLr-cgdZGkNS8494AaABAg,0,1510705047.0,Can u pls provide a tutorial on django ?,none,,2020-07-07 23:38:16.108088,0.0
575,ng2o98k983k,http://www.youtube.com/channel/UC8Dwq2uP8mangKwkGXouOQA,UC8Dwq2uP8mangKwkGXouOQA,Absar F,Ugx63mTaba_23uyIoFB4AaABAg,0,1510698146.0,Another great video..thanks Corey..pls do a tutorial (in your unique style :-) ) on debugging using “pdb”..many thanks..,none,,2020-07-07 23:38:16.108088,0.0
576,ng2o98k983k,http://www.youtube.com/channel/UCQI0cr2APNOsrop0DkU_7Og,UCQI0cr2APNOsrop0DkU_7Og,Rasstag,UgxTa5vdjsqHbjtA8LZ4AaABAg,0,1510691510.0,Excellent... to the point... question: would BeautifulSoup be the choice for scraping a web page with something like 'dynamic html'?,none,,2020-07-07 23:38:16.108088,1.0
577,ng2o98k983k,http://www.youtube.com/channel/UCqdVW3qL2PEuM1ASZdEgApA,UCqdVW3qL2PEuM1ASZdEgApA,D P,UgzC_c1NsrRlCEF_O-14AaABAg,8,1510573363.0,Corey your Python tutorials are great! You explain everything very well. It would be great if you could make a series of Django tutorials for beginners! Keep up the good work!,none,,2020-07-07 23:38:16.108088,0.0
578,ng2o98k983k,http://www.youtube.com/channel/UCdh9hyJjVVvStL4hPvbcVYw,UCdh9hyJjVVvStL4hPvbcVYw,General ZED,Ugw1qJPN6uSLvN_vmDl4AaABAg,0,1510497461.0,"Hello and thank you! Very good explanation.
I have a question if the site has some components loading from JS which module do you suggest for this situation?
I have used dryscrape but now is deprecated.",none,,2020-07-07 23:38:16.108088,0.0
579,ng2o98k983k,http://www.youtube.com/channel/UCPTbPy9Qgxs4c99lp3dn3qw,UCPTbPy9Qgxs4c99lp3dn3qw,Ivan Yakushchenko,Ugw2XnA8q6BTRQnjLkZ4AaABAg,1,1510469109.0,"As usually, great video, thanks a lot!",none,,2020-07-07 23:38:16.109088,0.0
580,ng2o98k983k,http://www.youtube.com/channel/UCdcLlI2kAwc9OqDVf4mbQyw,UCdcLlI2kAwc9OqDVf4mbQyw,ali sefidmouy,UgyYN6D5I0yackufy4V4AaABAg,0,1510420003.0,Hi Corey! thank you for this tutorial. Please start to make a tutorial for django.,none,,2020-07-07 23:38:16.109172,0.0
581,ng2o98k983k,http://www.youtube.com/channel/UCRlsKnxLgqtDyPm2bNiXsmA,UCRlsKnxLgqtDyPm2bNiXsmA,- wellington galvao,UgwbMLdwskB1GbQ_eG54AaABAg,0,1510327879.0,This class was very useful. Thank you Sir.,none,,2020-07-07 23:38:16.109172,0.0
582,ng2o98k983k,http://www.youtube.com/channel/UC0bkqrWNBKxGZi-4gIfaCpg,UC0bkqrWNBKxGZi-4gIfaCpg,PyMoondra,UgxRz8kpXESMd9HzlA54AaABAg,0,1510326870.0,Thanks Corey. Is this how most embedded videos work? They have an id within their source that you need to extract( and then recreate a new link)  I always have trouble dealing with javascript components with bs4.,none,,2020-07-07 23:38:16.109172,1.0
583,ng2o98k983k,http://www.youtube.com/channel/UCtu6Sw_UNlZtmwDdHKu-kjA,UCtu6Sw_UNlZtmwDdHKu-kjA,Caleb Njiiri,UgyNMMyLXDpiLJ1xEg94AaABAg,0,1510315174.0,"Hey thanks for the video, great content. One question what if i want to scrap data from a website that requires me to login.",none,,2020-07-07 23:38:16.109172,3.0
584,ng2o98k983k,http://www.youtube.com/channel/UC469LxiP-hGnk-DY3XjbLbA,UC469LxiP-hGnk-DY3XjbLbA,Venky's Moments,UgwoUFe7bhUbFD5_VBh4AaABAg,26,1510303928.0,Corey please add tutorials related with multithreading and multiprocessing in python.,none,,2020-07-07 23:38:16.109172,4.0
585,ng2o98k983k,http://www.youtube.com/channel/UCMsSPeibPjk84S68q2o7b3A,UCMsSPeibPjk84S68q2o7b3A,Gesuchter,UgzIFd9eA60AoD1i0vR4AaABAg,1,1510300764.0,Love your videos about libraries! Thanks for the great tutorial! <3,none,,2020-07-07 23:38:16.109172,0.0
586,ng2o98k983k,http://www.youtube.com/channel/UCSpYcsZp4Ozb9grcZuCIvCg,UCSpYcsZp4Ozb9grcZuCIvCg,sharkyze,UgyKGEXbW9_E6GcFUb94AaABAg,2,1510289462.0,Thank you so much for your videos. You can’t imagine how much I enjoy and learn for them. Thanks.,none,,2020-07-07 23:38:16.109172,0.0
587,ng2o98k983k,http://www.youtube.com/channel/UCnFJO-w-EI5wIOgnfvJgTdA,UCnFJO-w-EI5wIOgnfvJgTdA,Neno jay,UgwpjWB7ju2Wzp95pRR4AaABAg,0,1510286664.0,"i love it , thx for the info",none,,2020-07-07 23:38:16.109172,0.0
588,ng2o98k983k,http://www.youtube.com/channel/UCUz4jZ6y0PnI3T0SRTxbRLg,UCUz4jZ6y0PnI3T0SRTxbRLg,fconteEBdotcom,UgyHLVJfvVb34GD1hsd4AaABAg,0,1510269559.0,A very good introduction to BeautifulSoup. Thanks!,none,,2020-07-07 23:38:16.109172,0.0
589,ng2o98k983k,http://www.youtube.com/channel/UCYncGHWZY3HshPkjjmbFB2w,UCYncGHWZY3HshPkjjmbFB2w,Pie Apple,UgzCjvRqFA9XJFe8S9l4AaABAg,0,1510265078.0,"Hi, What is the font you are using? Thanks.",none,,2020-07-07 23:38:16.109172,0.0
590,ng2o98k983k,http://www.youtube.com/channel/UCGVpq7dplC5F0xNP-Jn2btw,UCGVpq7dplC5F0xNP-Jn2btw,Mohamed AbdElslam,UgzJ4sFK2Ag69gpdzBZ4AaABAg,0,1510254852.0,awesome,none,,2020-07-07 23:38:16.109172,0.0
591,ng2o98k983k,http://www.youtube.com/channel/UCTKHH5KJ-iKfIRBiGK53EAQ,UCTKHH5KJ-iKfIRBiGK53EAQ,Josus,UgwJPrEXlFHTdDI7Yll4AaABAg,1,1510230421.0,"Thanks a lot, man! Keep up the good work.",none,,2020-07-07 23:38:16.109172,0.0
592,ng2o98k983k,http://www.youtube.com/channel/UC2W0aQEPNpU6XrkFCYifRFQ,UC2W0aQEPNpU6XrkFCYifRFQ,Mark Jay,UgwhAryOeBs5vzTJcYh4AaABAg,25,1510200212.0,Another great video Corey! Keep them coming.,none,,2020-07-07 23:38:16.109172,0.0
593,ng2o98k983k,http://www.youtube.com/channel/UCYIxCkFIgjMLmwrJY9Ydy6g,UCYIxCkFIgjMLmwrJY9Ydy6g,Yunik Maharjan,UgxJtD7ckk79Nyey4GN4AaABAg,0,1510195543.0,please do a video on python setuptools,none,,2020-07-07 23:38:16.110177,0.0
594,ng2o98k983k,http://www.youtube.com/channel/UCjhOTge0dOC-Lpc63L-Y_1Q,UCjhOTge0dOC-Lpc63L-Y_1Q,Wolf Rage,UgwVVCB1qzwMInV8rih4AaABAg,4,1510187346.0,"Holy maaaaan, you are truly most amazing....
Big fan😊",none,,2020-07-07 23:38:16.110177,0.0
595,ng2o98k983k,http://www.youtube.com/channel/UCjhOTge0dOC-Lpc63L-Y_1Q,UCjhOTge0dOC-Lpc63L-Y_1Q,Wolf Rage,Ugx807QnlTzF6TMDzBd4AaABAg,0,1510186732.0,"I am glad you are still alive...
Great video, thank you very much..",none,,2020-07-07 23:38:16.110177,0.0
596,ng2o98k983k,http://www.youtube.com/channel/UCkNiUraoWuCaU9aenx8Ugdg,UCkNiUraoWuCaU9aenx8Ugdg,Darshan Mm,UgzbQChwVRFeJCfomw14AaABAg,0,1510184408.0,Excellent tutorial Corey. Thanks alot👍👍👍,none,,2020-07-07 23:38:16.110177,0.0
597,ng2o98k983k,http://www.youtube.com/channel/UCUZrSPzRe8bC39M9c9Q7Lnw,UCUZrSPzRe8bC39M9c9Q7Lnw,saxon plusminus,Ugy5VzUPUTAO0NGg_014AaABAg,0,1510171099.0,Starting Java soon,none,,2020-07-07 23:38:16.110177,0.0
598,ng2o98k983k,http://www.youtube.com/channel/UCmCA-jCoQ1Atxs_c_-q5AtA,UCmCA-jCoQ1Atxs_c_-q5AtA,Njul,UgwKeLL7SeVS7qs-99J4AaABAg,0,1510156617.0,"Very nice!
By the way, I think commenting on a video also ""helps"", if I recall correctly.",none,,2020-07-07 23:38:16.110177,0.0
599,ng2o98k983k,http://www.youtube.com/channel/UC7BmTSpLinRUJv0scK9LduA,UC7BmTSpLinRUJv0scK9LduA,Miscritz Brotherzz,Ugzhg8HEQIx0h1RYLxJ4AaABAg,2,1510156486.0,and yeah probably ur website will be RIPPED soon,none,,2020-07-07 23:38:16.110177,2.0
600,ng2o98k983k,http://www.youtube.com/channel/UCzzJp5OcwUl412dys8Onjxg,UCzzJp5OcwUl412dys8Onjxg,Hasan Fares,UgyQRY591zNDBvBPpit4AaABAg,13,1510154820.0,thanks was just taking an online class about it and couldn't understand until I got the notification you just uploaded wow you really are in sync with your followers,none,,2020-07-07 23:38:16.357107,0.0
601,ng2o98k983k,http://www.youtube.com/channel/UC7BmTSpLinRUJv0scK9LduA,UC7BmTSpLinRUJv0scK9LduA,Miscritz Brotherzz,UgwF-SVvPGGQMqV2tbl4AaABAg,8,1510151482.0,"awesome was just surfing the web about bs4 and u uploaded this
awesome 
awesome awesome thx",none,,2020-07-07 23:38:16.357107,0.0
602,,http://www.youtube.com/channel/UC3ObUafFMqN85pXd6jKtScg,UC3ObUafFMqN85pXd6jKtScg,WhizKiddo1569,UgxmgETkIcLp-LeyB_Z4AaABAg.99KVrNcOPT399Q2OnywuS0,0,1591118497.0,"This thread will help
https://stackoverflow.com/questions/49021589/how-do-i-fix-this-cp950-illegal-multibyte-sequence-unicodedecodeerror-when-rea",none,UgxmgETkIcLp-LeyB_Z4AaABAg,2020-07-07 23:38:16.864037,
603,,http://www.youtube.com/channel/UC5ezLx2XPPOQDHNGXqWVpmA,UC5ezLx2XPPOQDHNGXqWVpmA,jaddajaddajadda,UgyxcHOqy79GRnzRUMd4AaABAg.98uWK0G2zRG997nB-8pHkt,0,1590506015.0,@Naman kumar no problem I was getting same :),none,UgyxcHOqy79GRnzRUMd4AaABAg,2020-07-07 23:38:17.124175,
604,,http://www.youtube.com/channel/UCIY6ZTKcoUXtuoRsU613ZvA,UCIY6ZTKcoUXtuoRsU613ZvA,Naman kumar,UgyxcHOqy79GRnzRUMd4AaABAg.98uWK0G2zRG997ma_eHcAN,1,1590505709.0,@jaddajaddajadda thanku,none,UgyxcHOqy79GRnzRUMd4AaABAg,2020-07-07 23:38:17.124175,
605,,http://www.youtube.com/channel/UC5ezLx2XPPOQDHNGXqWVpmA,UC5ezLx2XPPOQDHNGXqWVpmA,jaddajaddajadda,UgyxcHOqy79GRnzRUMd4AaABAg.98uWK0G2zRG991b7_J9yiD,0,1590298369.0,BeautifulSoup - note capital letters,none,UgyxcHOqy79GRnzRUMd4AaABAg,2020-07-07 23:38:17.124175,
606,,http://www.youtube.com/channel/UC3ObUafFMqN85pXd6jKtScg,UC3ObUafFMqN85pXd6jKtScg,WhizKiddo1569,UgwCjVfc2VYEMpVKWQt4AaABAg.98Gumkksbqc99Q2Epw9LGK,0,1591118415.0,"This thread might help
https://stackoverflow.com/questions/38447738/beautifulsoup-html5lib-module-object-has-no-attribute-base",none,UgwCjVfc2VYEMpVKWQt4AaABAg,2020-07-07 23:38:17.402574,
607,,http://www.youtube.com/channel/UCGb9zl_bXHY98VmOTJqLs2A,UCGb9zl_bXHY98VmOTJqLs2A,Jingyi Wang,UgxHni7rCU8Wt82gnXN4AaABAg.97y8A1IXzUR97y8qK6dHxQ,0,1588001314.0,"By the way, the src I got is this (I used your website with the updated video) https://www.youtube.com/embed/z0gguhEmWiY?version=3&rel=1&fs=1&autohide=2&showsearch=0&showinfo=1&iv_load_policy=1&wmode=transparent. And when I click this link, it will show your video.",none,UgxHni7rCU8Wt82gnXN4AaABAg,2020-07-07 23:38:17.682253,
608,,http://www.youtube.com/channel/UCT6qL5CGjDS0wMpS1HC4AUg,UCT6qL5CGjDS0wMpS1HC4AUg,Ita Lutfiana,Ugx_tzjRo8OIXzEWUyN4AaABAg.97Nr8uKJLlb97VNRYt_GQm,0,1587002334.0,"I tried to practice this for extracting a Table of population in this site: https://en.wikipedia.org/wiki/Jakarta into Pandas dataframe but not successful, can you make the tutorial video ? Many thanks in advance",none,Ugx_tzjRo8OIXzEWUyN4AaABAg,2020-07-07 23:38:17.932561,
609,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxQVX-90OlzFr5TpCR4AaABAg.973JO2-cerP975hbVdrIX-,0,1586141019.0,No. I’m originally from West Virginia and now live in South Carolina,none,UgxQVX-90OlzFr5TpCR4AaABAg,2020-07-07 23:38:18.230379,
610,,http://www.youtube.com/channel/UCOEG2r1ss5rBHlmEYiROCqQ,UCOEG2r1ss5rBHlmEYiROCqQ,Rabbid Lnk,UgxsWb195MrStNg58iR4AaABAg.96q0s8vxq8H96xGzbt3Nce,0,1585824546.0,@varun canamedi Yup like same folder,none,UgxsWb195MrStNg58iR4AaABAg,2020-07-07 23:38:18.504142,
611,,http://www.youtube.com/channel/UCzvTKmy7mLrqzAKY7_iQEsg,UCzvTKmy7mLrqzAKY7_iQEsg,varun canamedi,UgxsWb195MrStNg58iR4AaABAg.96q0s8vxq8H96xG011ICsN,0,1585824033.0,@Rabbid Lnk you mean in the same file?,none,UgxsWb195MrStNg58iR4AaABAg,2020-07-07 23:38:18.504142,
612,,http://www.youtube.com/channel/UCOEG2r1ss5rBHlmEYiROCqQ,UCOEG2r1ss5rBHlmEYiROCqQ,Rabbid Lnk,UgxsWb195MrStNg58iR4AaABAg.96q0s8vxq8H96x3zsxE9mg,0,1585817733.0,make sure you have it saved in the same place as the code,none,UgxsWb195MrStNg58iR4AaABAg,2020-07-07 23:38:18.505139,
613,,http://www.youtube.com/channel/UCsU79KMFPtcI0AbTm_tjVfA,UCsU79KMFPtcI0AbTm_tjVfA,Mike Starr,Ugzv0wHFXC1slYcSTZR4AaABAg.94uOsaJm1qK959Z-NAAdxo,0,1581975230.0,@Corey Schafer Thanks for the reply. I appreciate your channel.,none,Ugzv0wHFXC1slYcSTZR4AaABAg,2020-07-07 23:38:18.826126,
614,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugzv0wHFXC1slYcSTZR4AaABAg.94uOsaJm1qK94yyW9D2emH,1,1581586477.0,"Hey there. That looks like it might be an issue with your PATH. I have two videos on that topic (One for Mac/Linux, one for Windows). If you watch the one for your operating system then it should give you a good idea of how to fix your problem. You can find those here...

Mac/Linux: https://youtu.be/PUIE7CPANfo
Windows: https://youtu.be/OdIHeg4jj2c",none,Ugzv0wHFXC1slYcSTZR4AaABAg,2020-07-07 23:38:18.826126,
615,,http://www.youtube.com/channel/UCP4oujcFp2EbXTuOAqWxZqQ,UCP4oujcFp2EbXTuOAqWxZqQ,Olutomilayo Dolapo,UgwIUVWJ5V-Ou7lw-PF4AaABAg.94UBfBwAIhb9A-VZyyZ9u5,1,1592375307.0,I don't know why I'm really happy about this 😂. Thank you Mr Corey,none,UgwIUVWJ5V-Ou7lw-PF4AaABAg,2020-07-07 23:38:19.151210,
616,,http://www.youtube.com/channel/UCCizjNTxM2kF40yz9w6mv5w,UCCizjNTxM2kF40yz9w6mv5w,Vaibhav Kumar,UgwIUVWJ5V-Ou7lw-PF4AaABAg.94UBfBwAIhb99qtc-Ra19i,1,1592052894.0,He deserves that 😀,none,UgwIUVWJ5V-Ou7lw-PF4AaABAg,2020-07-07 23:38:19.151210,
617,,http://www.youtube.com/channel/UCqDeCV5TBfakHFlqOETFDeQ,UCqDeCV5TBfakHFlqOETFDeQ,João Vítor,UgwIUVWJ5V-Ou7lw-PF4AaABAg.94UBfBwAIhb98wmgIiGOdQ,2,1590103102.0,"Yes, it was Alex Aklson from IBM",none,UgwIUVWJ5V-Ou7lw-PF4AaABAg,2020-07-07 23:38:19.151210,
618,,http://www.youtube.com/channel/UCR0OOadH8b4KfO1Ln9Zjffg,UCR0OOadH8b4KfO1Ln9Zjffg,Joaquín Requena,UgwIUVWJ5V-Ou7lw-PF4AaABAg.94UBfBwAIhb96WQ5uQRNca,6,1584889800.0,"Damn course, I'm getting whipped there",none,UgwIUVWJ5V-Ou7lw-PF4AaABAg,2020-07-07 23:38:19.151210,
619,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwIUVWJ5V-Ou7lw-PF4AaABAg.94UBfBwAIhb94WB_WVUHC5,49,1580587220.0,I didn't know that. Very cool! Thanks for letting me know!,none,UgwIUVWJ5V-Ou7lw-PF4AaABAg,2020-07-07 23:38:19.151210,
620,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxV34sFWzXSPTwK-j94AaABAg.94RIxR1ylRs94TJ2R_Nfxl,1,1580490472.0,I made a video on Requests-HTML which can scrape JavaScript generated values. If you search my channel you should be able to find it,none,UgxV34sFWzXSPTwK-j94AaABAg,2020-07-07 23:38:19.422913,
621,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwdE32tTP4_VUmWjOl4AaABAg.94PYocdLLgd94QsT_3CIhW,1,1580408905.0,You could pass in a full PATH for the place you would like to save the file if you're not sure where it's being stored.,none,UgwdE32tTP4_VUmWjOl4AaABAg,2020-07-07 23:38:19.715421,
622,,http://www.youtube.com/channel/UCY-qpLFKiRo0ZhT0YDDsUAw,UCY-qpLFKiRo0ZhT0YDDsUAw,Upender Dalai,UgylPdzTFfmwTr8TeP54AaABAg.93Y2l0nJtu-93ivMbzSr3n,0,1578900471.0,"In Python 3.x, you can use the end argument to the print() function to prevent a newline character from being printed:

print(""how I can prevent \n"", end="""")",none,UgylPdzTFfmwTr8TeP54AaABAg,2020-07-07 23:38:19.949353,
623,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwRW0wpVB2xFV0HE3N4AaABAg.93DR7xuiPLM93_BOSTr8cb,0,1578573855.0,Thanks!,none,UgwRW0wpVB2xFV0HE3N4AaABAg,2020-07-07 23:38:20.240097,
624,,http://www.youtube.com/channel/UCHg_QyXO6TANcgLUZl6OC0Q,UCHg_QyXO6TANcgLUZl6OC0Q,Ferreira N,Ugw7wlt4xTMFEvmJDtB4AaABAg.93AWJ23pdeb93AdW38xt7T,0,1577716706.0,"hmmmm, I see its not possible to loop even in Corey website, I wonder if there is a way to break through the ""Wp blocked embed video"" - yes, of course I am newbie :D",none,Ugw7wlt4xTMFEvmJDtB4AaABAg,2020-07-07 23:38:20.491004,
625,,http://www.youtube.com/channel/UC9JsHWR0gsLJbUCA5rUiGHw,UC9JsHWR0gsLJbUCA5rUiGHw,babu patil,UgyEiJZOBejrItqIwIF4AaABAg.92eaXubVkNZ935IBOY6gdA,0,1577537230.0,"This below code will work fine.


import requests
from bs4 import BeautifulSoup
source=requests.get('https://coreyms.com').text
soup=BeautifulSoup(source,'lxml')
#print(soup.prettify())
for article in soup.find_all('article'):
    #print(article.prettify())
    headline=article.h2.a.text
    print(headline)
    summary=article.find('div',class_='entry-content').p.text
    print(summary)
    if article.find('iframe'):
        vid_source=article.find('iframe')['src']
        #print(vid_source)
        vid_id=vid_source.split('/')[4]
        #print(vid_id)
        youtube_link=""https://youtube.com/watch?v={}"".format(vid_id)
        print(youtube_link)
    elif article.find('figure'):
        vid_source=article.find('figure').div.text
        vid_id=vid_source.split('/')[3]
        youtube_link=""https://youtube.com/watch?v={}"".format(vid_id)
        print(youtube_link)
    
    print()",none,UgyEiJZOBejrItqIwIF4AaABAg,2020-07-07 23:38:20.958191,
626,,http://www.youtube.com/channel/UC9JsHWR0gsLJbUCA5rUiGHw,UC9JsHWR0gsLJbUCA5rUiGHw,babu patil,UgyEiJZOBejrItqIwIF4AaABAg.92eaXubVkNZ934ja8ZCTCi,0,1577518567.0,"vid_src=article.find('iframe')['src'], i think this will work, website got changed. there is no class in <iframe> tag...",none,UgyEiJZOBejrItqIwIF4AaABAg,2020-07-07 23:38:20.958191,
627,,http://www.youtube.com/channel/UC9JsHWR0gsLJbUCA5rUiGHw,UC9JsHWR0gsLJbUCA5rUiGHw,babu patil,UgyXrQmFEZ8WVuol3_94AaABAg.92BlH3xRjGh934jv05ijh4,0,1577518738.0,"Thank you, i was stuck in that...",none,UgyXrQmFEZ8WVuol3_94AaABAg,2020-07-07 23:38:21.214596,
628,,http://www.youtube.com/channel/UC7sdicZwFCle9fMS61G0lEw,UC7sdicZwFCle9fMS61G0lEw,Tomer Harari,Ugx-_ECCLYbnekxqqM54AaABAg.927t3hQpL7i927xCItoiq_,0,1575478883.0,lol figured out my problem... my html file wasn't in current directory,none,Ugx-_ECCLYbnekxqqM54AaABAg,2020-07-07 23:38:21.672250,
629,,http://www.youtube.com/channel/UC7sdicZwFCle9fMS61G0lEw,UC7sdicZwFCle9fMS61G0lEw,Tomer Harari,Ugx-_ECCLYbnekxqqM54AaABAg.927t3hQpL7i927tdOlrD_c,0,1575477016.0,"Also when I run a simple print on my secondClass, my terminal returns ""None""",none,Ugx-_ECCLYbnekxqqM54AaABAg,2020-07-07 23:38:21.672250,
630,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugx6Mo7oBtP0FYhctLF4AaABAg.91QaFzPFoH-91WrbpMyqPF,0,1574167332.0,"Hey There. I use Sublime Text for my tutorials since it's pretty minimalist. I also use VSCode for my day-to-day coding. I made a video on how I set up my Python development environment in both of those editors. You can find those videos here:
Sublime Text - https://youtu.be/xFciV6Ew5r4
VSCode - https://youtu.be/06I63_p-2A4

Thanks!",none,Ugx6Mo7oBtP0FYhctLF4AaABAg,2020-07-07 23:38:22.096462,
631,,http://www.youtube.com/channel/UCa802vHqr1qLIXwBydK7ftg,UCa802vHqr1qLIXwBydK7ftg,Azmylle H,UgwhisL4S9R_D6G9FBp4AaABAg.9-LLWY1P2AY9-QIbu8GvA3,1,1569652164.0,please teach me if you manage to do that ^_^,none,UgwhisL4S9R_D6G9FBp4AaABAg,2020-07-07 23:38:22.493853,
632,,http://www.youtube.com/channel/UCPxuLHKNgcExXTOYkYFo9aw,UCPxuLHKNgcExXTOYkYFo9aw,Kaiju,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-SlNqTpNTA,0,1569734878.0,@Mohib Atom is just a text editor. If you use notepad or atom wouldn't make a difference.,none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.753149,
633,,http://www.youtube.com/channel/UCpuv-Qjgd4llAuYIJjQqSNA,UCpuv-Qjgd4llAuYIJjQqSNA,Mohib,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-S4ZkSRyYX,0,1569711907.0,@Kaiju i've tried that before. It still doesn't work. It works in a different editor(IDLE). it just doesn't work for atom,none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.753149,
634,,http://www.youtube.com/channel/UCPxuLHKNgcExXTOYkYFo9aw,UCPxuLHKNgcExXTOYkYFo9aw,Kaiju,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-S2O8Ev4sO,0,1569710764.0,"@Mohib go into powershell or cmd again and type ""python -m pip install requests""",none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.753149,
635,,http://www.youtube.com/channel/UCpuv-Qjgd4llAuYIJjQqSNA,UCpuv-Qjgd4llAuYIJjQqSNA,Mohib,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-S0Pt4VpyU,0,1569709729.0,"@Kaiju well guess I can't send the screen shot but point is it says 
Import error: No module names requests",none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
636,,http://www.youtube.com/channel/UCPxuLHKNgcExXTOYkYFo9aw,UCPxuLHKNgcExXTOYkYFo9aw,Kaiju,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-RGBe-He9n,0,1569684447.0,@Mohib alright,none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
637,,http://www.youtube.com/channel/UCpuv-Qjgd4llAuYIJjQqSNA,UCpuv-Qjgd4llAuYIJjQqSNA,Mohib,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-Qf_WVNnzw,0,1569664728.0,Kaiju no. I will send you a screen shot in like 40min once I’m on my laptop,none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
638,,http://www.youtube.com/channel/UCPxuLHKNgcExXTOYkYFo9aw,UCPxuLHKNgcExXTOYkYFo9aw,Kaiju,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-QfGi_UPzb,0,1569664566.0,@Mohib It still does not work?,none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
639,,http://www.youtube.com/channel/UCpuv-Qjgd4llAuYIJjQqSNA,UCpuv-Qjgd4llAuYIJjQqSNA,Mohib,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-Q_0qsX8Ed,0,1569661290.0,Kaiju yea requests* my bad,none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
640,,http://www.youtube.com/channel/UCPxuLHKNgcExXTOYkYFo9aw,UCPxuLHKNgcExXTOYkYFo9aw,Kaiju,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-QNx-TfGdP,2,1569654959.0,"@Mohib not request, but requests",none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
641,,http://www.youtube.com/channel/UCpuv-Qjgd4llAuYIJjQqSNA,UCpuv-Qjgd4llAuYIJjQqSNA,Mohib,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-PV2yBDplQ,1,1569625131.0,"Kaiju  bruh... I meant like the library. I have it installed and it works for IDLE, but won’t work in atom. It keeps giving me an error saying “module request doesn’t exist” or I think it said module not found",none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
642,,http://www.youtube.com/channel/UCPxuLHKNgcExXTOYkYFo9aw,UCPxuLHKNgcExXTOYkYFo9aw,Kaiju,UgwCEFMhga2QSTxCwzd4AaABAg.9-JSbJuWkJI9-PUiBNumul,1,1569624953.0,"Add ""import requests"" at the top of the document",none,UgwCEFMhga2QSTxCwzd4AaABAg,2020-07-07 23:38:22.754146,
643,,http://www.youtube.com/channel/UCgOSulpG81fTcIN_q3dZr6g,UCgOSulpG81fTcIN_q3dZr6g,Freem4nn,UgzglUjP7KcnEJjCJWt4AaABAg.8z25bevGDim8z3zjCv729Z,0,1566756057.0,it's in sublime,none,UgzglUjP7KcnEJjCJWt4AaABAg,2020-07-07 23:38:23.006775,
644,,http://www.youtube.com/channel/UC492v9o4iR8hHEmuFi_tK0w,UC492v9o4iR8hHEmuFi_tK0w,Bianca A. - There's art to data science,UgzPFCF2slzGA-u_Ct14AaABAg.8y_rUI6PBvb8zP8YXtefCz,0,1567465848.0,"Yes, you're on the right track. Create a Python list where each element is one of the pages you want to scrape, then loop through the list. 😊",none,UgzPFCF2slzGA-u_Ct14AaABAg,2020-07-07 23:38:23.269113,
645,,http://www.youtube.com/channel/UCru8rb2ubZ1YufD7JgCcSmw,UCru8rb2ubZ1YufD7JgCcSmw,Karim Diouf,UgyXPEPft9XRtlgBun54AaABAg.8yXIal_pXUj96OxaKSwXzC,0,1584639448.0,"Hey I am very new to Python..you probablu have already figured it out ..but:
1.create empy lists for pages, titles, summaries and links
2. Then
for i in range(1,18):
    url=(""https://coreyms.com/page/{}"".format(i))
    pages.append(url)
for content in pages:
    page=requests.get(content)
    soup=BeautifulSoup(page.text,""lxml"")
    for article in soup.find_all(""article""):
        summary=article.div.p.text
        summaries.append(summary)
        title=article.h2.a.text
        titles.append(title)

        try:
            link=""youtube.com/watch?v=""+article.div.span.iframe[""src""].split(""/"")[4].split(""?"")[0]
            
        except Exception as e:
            link= ""No Link""
        links.append(link)",none,UgyXPEPft9XRtlgBun54AaABAg,2020-07-07 23:38:23.497455,
646,,http://www.youtube.com/channel/UC5HgNxFWcINg81ynYZ39_mA,UC5HgNxFWcINg81ynYZ39_mA,Ali Mansour,Ugy_C9--zi0O_jlJuKR4AaABAg.8xk6tLHbV8t8xwid0Tn5sK,0,1564331174.0,@Corey Schafer Thank you so very much. Your suggestion worked beautifully. Best Regards.,none,Ugy_C9--zi0O_jlJuKR4AaABAg,2020-07-07 23:38:23.814597,
647,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugy_C9--zi0O_jlJuKR4AaABAg.8xk6tLHbV8t8xmB-6Ou6Ok,0,1563977469.0,"When you run find, it should return a list of everything it finds. You can parse that like any other list or access the exact div you want directly by specifying an index (i.e. results[1])",none,Ugy_C9--zi0O_jlJuKR4AaABAg,2020-07-07 23:38:23.814597,
648,,http://www.youtube.com/channel/UCEJ5xwhYdk-QxKf6O2r-WDg,UCEJ5xwhYdk-QxKf6O2r-WDg,jianfeng jian,UgyBd1JocIBOAG1tXi54AaABAg.8xRjlPAxq-A8xWopvAyfaT,0,1563428456.0,"@Deniz Utku Aktürk are you on windows platform? try  csv_file = open('coreyms.csv', 'w', newline='')",none,UgyBd1JocIBOAG1tXi54AaABAg,2020-07-07 23:38:24.110464,
649,,http://www.youtube.com/channel/UC4U7hkyW0dahbn9HgluttPw,UC4U7hkyW0dahbn9HgluttPw,Deniz Utku Aktürk,UgyBd1JocIBOAG1tXi54AaABAg.8xRjlPAxq-A8xRqwSx8uC9,0,1563261786.0,"the code is printing blank rows at every even line. at 2,4,6,8,10... the code is identical so why am i getting this kind of output? very grateful for any help. thank you.",none,UgyBd1JocIBOAG1tXi54AaABAg,2020-07-07 23:38:24.110464,
650,,http://www.youtube.com/channel/UC5elG7qIlmnriHgWrkqa6ow,UC5elG7qIlmnriHgWrkqa6ow,Justin Gossett,UgzwSukBdwvYmbcUowh4AaABAg.8xCbvqsROBo8xF0eRjtGGx,1,1562831198.0,the code is in the description.,none,UgzwSukBdwvYmbcUowh4AaABAg,2020-07-07 23:38:24.349988,
651,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwJFKEuacLD-l8aaiJ4AaABAg.8x8bsIbIfBI8x8zWpVIM59,1,1562628753.0,You can scrape the URL for the image and then download it using the Requests library. If you'd like to see how to download an image using Requests then you can watch my video on the library to see an example.,none,UgwJFKEuacLD-l8aaiJ4AaABAg,2020-07-07 23:38:24.668408,
652,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwaU2yYXu_J3BkgDMR4AaABAg.8v1IIw9VsyE8v1mmGfIO7Q,0,1558092223.0,Good to hear! Thanks for the comment.,none,UgwaU2yYXu_J3BkgDMR4AaABAg,2020-07-07 23:38:24.892956,
653,,http://www.youtube.com/channel/UCcWGUDNu0vfgxSw0ADG-Tnw,UCcWGUDNu0vfgxSw0ADG-Tnw,Mustafa Olomi,Ugzi0nH9w2twAviljLh4AaABAg.8ug-ILyv3Do8ukwVs-qxJR,0,1557493344.0,"I changed it a bit and added .content... soup = BeautifulSoup(source.content, 'lxml')",none,Ugzi0nH9w2twAviljLh4AaABAg,2020-07-07 23:38:25.139381,
654,,http://www.youtube.com/channel/UCZIzpTpvqNM_047ya4gMLkQ,UCZIzpTpvqNM_047ya4gMLkQ,Lucas Miguel,Ugxkn_Cka9xa-XSxoKV4AaABAg.8u6Lri4gE5_91I8VEKrmam,0,1573673390.0,Have you found out yet? I'm having a rough time trying to do this,none,Ugxkn_Cka9xa-XSxoKV4AaABAg,2020-07-07 23:38:25.447022,
655,,http://www.youtube.com/channel/UCZQuIYy4484RmyCaRaJV2ZA,UCZQuIYy4484RmyCaRaJV2ZA,Stewart Darke,Ugw3wExXSJ7f5KiT0hN4AaABAg.8u2y3iA8D-Z8u3idhSE46s,0,1556009681.0,"from bs4 import BeautifulSoup
import requests

page = requests.get(""https://www.bbc.co.uk/news"")

# We should see a <Response> 200 OK
# print(page)

soup = BeautifulSoup(page.content, ""html.parser"")

print(soup)",none,Ugw3wExXSJ7f5KiT0hN4AaABAg,2020-07-07 23:38:25.821000,
656,,http://www.youtube.com/channel/UCZQuIYy4484RmyCaRaJV2ZA,UCZQuIYy4484RmyCaRaJV2ZA,Stewart Darke,Ugw3wExXSJ7f5KiT0hN4AaABAg.8u2y3iA8D-Z8u3hA8h6Xy4,0,1556008906.0,"'html.parser' its asking for instead of 'lmxl'


think this is a python version diffrence i could be wrong so dont question me lool
im on Python 3.",none,Ugw3wExXSJ7f5KiT0hN4AaABAg,2020-07-07 23:38:25.821996,
657,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwCO-mXNJF8eGLVb2V4AaABAg.8u1ChV0bvSc8u1aruXr02L,1,1555938494.0,It depends on the webpage. If you need to crawl the page then you could scrape the links and then loop through those and capture the information similar to how we did here. For more advanced crawling you could use Scrapy or Selenium,none,UgwCO-mXNJF8eGLVb2V4AaABAg,2020-07-07 23:38:26.080233,
658,,http://www.youtube.com/channel/UCo8P9miyqy2Loo1eh5BMIGw,UCo8P9miyqy2Loo1eh5BMIGw,FaLL_Nemesis00,Ugzglxvm7gF477Ad1z54AaABAg.8t6akPNeDOw8tAP_rKa49j,0,1554086561.0,Corey Schafer thanks!,none,Ugzglxvm7gF477Ad1z54AaABAg,2020-07-07 23:38:26.845863,
659,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugzglxvm7gF477Ad1z54AaABAg.8t6akPNeDOw8t7LUtmUUrt,1,1553983752.0,You could access them like a list and grab the ones you want,none,Ugzglxvm7gF477Ad1z54AaABAg,2020-07-07 23:38:26.846464,
660,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugz6wy6Dw6PyviT-RwN4AaABAg.8stWvQEQCtF8suU59aQ2_J,1,1553518498.0,3,none,Ugz6wy6Dw6PyviT-RwN4AaABAg,2020-07-07 23:38:27.102054,
661,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugx43imEBtnQmfcNJ854AaABAg.8skkLstJ3NO8slAcJyi8R5,0,1553206302.0,Thanks!,none,Ugx43imEBtnQmfcNJ854AaABAg,2020-07-07 23:38:27.340101,
662,,http://www.youtube.com/channel/UC2S065GGZkCsZEAUBlVD1sQ,UC2S065GGZkCsZEAUBlVD1sQ,Jethro Cao,UgxpIYnBtP-tROlcxoZ4AaABAg.8sWUufWoRPN8s_Z8pt-T1l,0,1552816506.0,"@Corey Schafer Awesome, thanks for replying! Maybe you can even consider doing a video or two on it.",none,UgxpIYnBtP-tROlcxoZ4AaABAg,2020-07-07 23:38:27.615261,
663,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxpIYnBtP-tROlcxoZ4AaABAg.8sWUufWoRPN8sW_KzqnR-G,0,1552682912.0,"I like conda and I used to use it, but since I'm teaching videos I wanted to try to match the environment of most of my viewers as best as I could. That way when people see me do something in my video, they're likely using a very similar setup. I'll likely use Conda again once I start teaching Data Science",none,UgxpIYnBtP-tROlcxoZ4AaABAg,2020-07-07 23:38:27.615261,
664,,http://www.youtube.com/channel/UCDAOD3X4wJn8yGfj27tyidQ,UCDAOD3X4wJn8yGfj27tyidQ,Frank Conte,UgxxgSJPw8GKUT4oJaB4AaABAg.8ruFyLrFO9-8ruhlWHRIWb,0,1551378709.0,"@Corey Schafer  I get this error even though the output for the headline and the summary the  "" and the ""pbanner"" are fine.  
-> 16     headline = article.h2.text
     17     print(headline)
     18 

AttributeError: 'NoneType' object has no attribute 'text'",none,UgxxgSJPw8GKUT4oJaB4AaABAg,2020-07-07 23:38:27.943223,
665,,http://www.youtube.com/channel/UCDAOD3X4wJn8yGfj27tyidQ,UCDAOD3X4wJn8yGfj27tyidQ,Frank Conte,UgxxgSJPw8GKUT4oJaB4AaABAg.8ruFyLrFO9-8rufn_xiIH3,0,1551377677.0,Thanks I will give it a try.,none,UgxxgSJPw8GKUT4oJaB4AaABAg,2020-07-07 23:38:27.944287,
666,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxxgSJPw8GKUT4oJaB4AaABAg.8ruFyLrFO9-8ru_GxCqEjh,0,1551374256.0,"Oh I see. The reason I used split in my example was because I actually had to parse out a specific value from the URL. If you just want the URL then you can simply use your line that says:
buttonLink  = article.find('li', class_ = 'button big')['href']

If you print that buttonLink then it should be the URL for that button.",none,UgxxgSJPw8GKUT4oJaB4AaABAg,2020-07-07 23:38:27.944287,
667,,http://www.youtube.com/channel/UCDAOD3X4wJn8yGfj27tyidQ,UCDAOD3X4wJn8yGfj27tyidQ,Frank Conte,UgxxgSJPw8GKUT4oJaB4AaABAg.8ruFyLrFO9-8ruW-zhuOn8,0,1551372020.0,"try:
        buttonLink = article.find('li', class_ = 'button big')['href']
        #print (vid_src)

        buttonLink_id = buttonLink.split ('href')[4]
        
except Exception as e:
        weblink = None
    
     
print (weblink)

weblink is my attempt to scrape the url in in the 'button big' class. Thanks",none,UgxxgSJPw8GKUT4oJaB4AaABAg,2020-07-07 23:38:27.944287,
668,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxxgSJPw8GKUT4oJaB4AaABAg.8ruFyLrFO9-8ruKcBZ1-Hk,0,1551366050.0,"Can you include your line where you’re running split? I’d like to see exactly what you’re trying to split.

PS, thanks for being a Patron!",none,UgxxgSJPw8GKUT4oJaB4AaABAg,2020-07-07 23:38:27.944287,
669,,http://www.youtube.com/channel/UCwCse-_VuoIN21OYhMIMgmA,UCwCse-_VuoIN21OYhMIMgmA,Tarun Garg,UgxkrzwYdmBJiI4RmAl4AaABAg.8rpA_yXxbiM8rrOLhQVLqL,0,1551267340.0,"@Corey Schafer Yes.. may be i can send you code over email for review if you can share your email.. or let me paste here only..
............................

from bs4 import BeautifulSoup
import requests
import csv

source=requests.get('http://coreyms.com').text

soup=BeautifulSoup(source,'lxml')


csv_file=open('cms_scrape.csv','w')
csv_writer=csv.writer(csv_file)
csv_writer.writerow(['Headline','Summary','video_link'])

for article in soup.find_all('article'):
    
    heading=article.h2.a.text
    
    summary=article.find('div',class_='entry-content').p.text
    video=article.find('iframe',class_='youtube-player')['src']
    video_id=video.split('/')[4].split('?')[0]
    yt_link=f'http://www.youtube.com/watch?v={video_id}'
    csv_writer.writerow([heading,summary,yt_link])

     

csv_file.close()",none,UgxkrzwYdmBJiI4RmAl4AaABAg,2020-07-07 23:38:28.159438,
670,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxkrzwYdmBJiI4RmAl4AaABAg.8rpA_yXxbiM8rpE5d1wQtS,0,1551194857.0,Are you sure you're only writing to the file once within the loop?,none,UgxkrzwYdmBJiI4RmAl4AaABAg,2020-07-07 23:38:28.160387,
671,,http://www.youtube.com/channel/UCd-EiEQTiXnxK7LQd0lkbPw,UCd-EiEQTiXnxK7LQd0lkbPw,fern092,UgyMqah42v02inNcX594AaABAg.8rdVJkxVVjb91S3wu8aVjB,0,1574006548.0,"Your change has an error. With a correction, it is useful :
v_id = vid_src.split('?')[0]                                #Step 1

v_id = v_id.split('/')[len(v_id.split('/')) - 1]     #Step 2",none,UgyMqah42v02inNcX594AaABAg,2020-07-07 23:38:28.385079,
672,,http://www.youtube.com/channel/UCofTWxpGUUl_ArJa-UvGZIg,UCofTWxpGUUl_ArJa-UvGZIg,Jae Kim,Ugym5T-R7qDxKUkAuKF4AaABAg.8qQyR3ex2KR8r2aRX_Ehc5,0,1549529373.0,"Corey Schafer Sorry, i thought i had it installed but i didn’t. I feel foolish now but hey thank you so much for your video and kind reply.",none,Ugym5T-R7qDxKUkAuKF4AaABAg,2020-07-07 23:38:28.644894,
673,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugym5T-R7qDxKUkAuKF4AaABAg.8qQyR3ex2KR8qRIP_UggkI,0,1548210773.0,"Are you setting an attribute called ""requests""? I'm not sure what else could be causing that",none,Ugym5T-R7qDxKUkAuKF4AaABAg,2020-07-07 23:38:28.644894,
674,,http://www.youtube.com/channel/UCm5xfxPgvT7834i1jpo8Y-g,UCm5xfxPgvT7834i1jpo8Y-g,Ed Tix,Ugxj5MEadTFsWsFSesl4AaABAg.8pk_9FB4w5X8qmEowEUnqG,0,1548947089.0,Hey I struggled with this for a few hours and now I'm okay with that. Fortunately many sites use API'S as authentication backed. When you log in they send you back authentication key to your browser. Key is valid for about 15 minutes and browser refreshes it automatically but in this case periodic request is needed. With auth key you can get any internal data you want (of course if your user has access to it). This method requires json.,none,Ugxj5MEadTFsWsFSesl4AaABAg,2020-07-07 23:38:28.894733,
675,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugxj5MEadTFsWsFSesl4AaABAg.8pk_9FB4w5X8poSSiBRn9v,0,1546873864.0,"There are different options available here. If it's basic auth, then I believe you can use the requests library to login and follow the redirect. From there, you can grab the HTML and parse it. If it isn't basic auth then there are still ways, but you'd likely have to use a headless browser to do this. There are some available in Python, but it is an advanced topic. Perhaps I'll cover in the near future if I find the time.",none,Ugxj5MEadTFsWsFSesl4AaABAg,2020-07-07 23:38:28.894733,
676,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgzChzXahXyZgNoTSUd4AaABAg.8pkDYjLahT78poR_Vpa4jb,0,1546873404.0,"Hey there. That looks like it might be an issue with your PATH. I have two videos on that topic (One for Mac/Linux, one for Windows). If you watch the one for your operating system then it should give you a good idea of how to fix your problem. You can find those here...

Mac/Linux: https://youtu.be/PUIE7CPANfo
Windows: https://youtu.be/OdIHeg4jj2c",none,UgzChzXahXyZgNoTSUd4AaABAg,2020-07-07 23:38:29.149808,
677,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugy6H-9J3MeKdFk-dcl4AaABAg.8pbSSjg08zS8pck6Xb-Zkn,1,1546480991.0,It depends on the API. If you’re scraping Twitter or Youtube or Facebook then they’re all different. I’ll try to cover some of those APIs in the future.,none,Ugy6H-9J3MeKdFk-dcl4AaABAg,2020-07-07 23:38:29.407285,
678,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugyx_iU9UpRUdXmwHw54AaABAg.8pTw8sgrY3S8pUz39ogEjd,0,1546186838.0,"Yes, blocking by IP is one way they could prevent you from accessing their site if you are hitting them with too many requests.",none,Ugyx_iU9UpRUdXmwHw54AaABAg,2020-07-07 23:38:29.879067,
679,,http://www.youtube.com/channel/UC7qgoMJVYoXsfxrqBeK1YpA,UC7qgoMJVYoXsfxrqBeK1YpA,Harish S,UgwucF_Nwx-A0fLCwWB4AaABAg.8pJcYH_I4g18pLkmEZedfD,0,1545877361.0,@Corey Schafer Thank You so much!! It would be really helpful and your teachings are detailed and really helpful!! Thanks for all the teachings.,none,UgwucF_Nwx-A0fLCwWB4AaABAg,2020-07-07 23:38:30.205058,
680,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwucF_Nwx-A0fLCwWB4AaABAg.8pJcYH_I4g18pKBQsESQ5S,1,1545824749.0,You can write a loop and get a request per page. Perhaps I’ll try to put together a video on that in the future,none,UgwucF_Nwx-A0fLCwWB4AaABAg,2020-07-07 23:38:30.205058,
681,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugxsl1d45IVwsKBwvX94AaABAg.8pIyku2ffOy8pJ6ynYGarT,0,1545788859.0,If you can find anything specific about the divs then you could pick them out by the index of which divs you wanted from the list.,none,Ugxsl1d45IVwsKBwvX94AaABAg,2020-07-07 23:38:30.430326,
682,,http://www.youtube.com/channel/UCpBrWymKJb1mZJ7EfAvGYaw,UCpBrWymKJb1mZJ7EfAvGYaw,TmanD54,Ugx9Ygp_QV9MulskLa94AaABAg.8p6Iuwv5y908p6K7DE8eNr,0,1545359545.0,"nvm, got it , csv_file = open('cms_scrape.csv', 'w', newline='')

adding newline='' to the open prevented the extra spaces. Thanks.",none,Ugx9Ygp_QV9MulskLa94AaABAg,2020-07-07 23:38:30.686020,
683,,http://www.youtube.com/channel/UCThR2qiYucancZNv9FkTq-Q,UCThR2qiYucancZNv9FkTq-Q,Upasana Singh,UgyE0cmu0MOzjhoVZYB4AaABAg.8p-QHElcNE-8xN3OpQL7Lr,0,1563101070.0,Agree!,none,UgyE0cmu0MOzjhoVZYB4AaABAg,2020-07-07 23:38:30.942434,
684,,http://www.youtube.com/channel/UCNN2_TAIsdcaVXwWvMZ9Fmw,UCNN2_TAIsdcaVXwWvMZ9Fmw,Edy,Ugyz3QlpyyzkAIqnsZ94AaABAg.8omEg2ZmA-48osbF7bEZMG,0,1544865730.0,@Corey Schafer Thank you Corey! I already started to try another approach but I had to give up as it was not fetching correctly the data.After that I used your approach - to check in a while loop in each page the existence of the next page button and that worked beautiful.Thanks for your prompt support.,none,Ugyz3QlpyyzkAIqnsZ94AaABAg,2020-07-07 23:38:31.455339,
685,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugyz3QlpyyzkAIqnsZ94AaABAg.8omEg2ZmA-48omVPNcerW7,0,1544660817.0,"You could use the same parsing methods that we learned in this video to parse out the ""Next Page"" button. You could do a while loop where you check if that link is there and if it is then you can continue parsing. Every website is different, but that is the approach I would take for a lot of sites.",none,Ugyz3QlpyyzkAIqnsZ94AaABAg,2020-07-07 23:38:31.455962,
686,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwEAUUP-k3wCIP4pEd4AaABAg.8oT95nEYB3x8oUP7DRCRuh,0,1544019989.0,Thanks!,none,UgwEAUUP-k3wCIP4pEd4AaABAg,2020-07-07 23:38:31.677090,
687,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwWtWMajxFk4Z2m5D94AaABAg.8o4gvN9zcup8o5QZIceTOs,0,1543181882.0,This should work on Windows as well. What error are you getting?,none,UgwWtWMajxFk4Z2m5D94AaABAg,2020-07-07 23:38:31.966266,
688,,http://www.youtube.com/channel/UC9OZ3smunNTIEiYkMpU9KFg,UC9OZ3smunNTIEiYkMpU9KFg,Evo Wolf,UgzctxzUQeUUe4nzis14AaABAg.8m_C9GQAaBg8maTredXbHB,1,1539962388.0,"I figured it out a little bit ago, I just used it for the same problem correlating to anchor tags, I used...
.find_all('a')[2]['href'] at the end of my soup .find
and that seemed to work.",none,UgzctxzUQeUUe4nzis14AaABAg,2020-07-07 23:38:32.221011,
689,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgzctxzUQeUUe4nzis14AaABAg.8m_C9GQAaBg8maPoGu-1Mk,1,1539960263.0,"You should be able to simply get the index like you would in a list. Something like:
p = paragraphs[1]",none,UgzctxzUQeUUe4nzis14AaABAg,2020-07-07 23:38:32.221011,
690,,http://www.youtube.com/channel/UCnFhHhC-hKBYWNByp9QSscw,UCnFhHhC-hKBYWNByp9QSscw,Darryl Lobo,Ugxo36d4M-_2Ua6zYkl4AaABAg.8lJrcbW8R4A8lMTHx6AXr9,0,1537311287.0,"to clarify the emails lead to a link which opens an HTML page. I already got the scraping part from the HTML page, thanks to your video (much appreciated!!!). The thing I now need is to log into Gmail, then only search for the emails I want from a certain sender (my realtor), loop through these emails and fetch the HTML page from these links. Once I get these HTML pages I can use beautiful soup! I am thinking to do the prior steps using Gmail API (which I guess I have to google around and see how to use).",none,Ugxo36d4M-_2Ua6zYkl4AaABAg,2020-07-07 23:38:32.433586,
691,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugxo36d4M-_2Ua6zYkl4AaABAg.8lJrcbW8R4A8lLOAImsRmk,0,1537275049.0,There are probably better modules for that. You can use a module meant specifically for email to open your account and get those programmatically.,none,Ugxo36d4M-_2Ua6zYkl4AaABAg,2020-07-07 23:38:32.433586,
692,,http://www.youtube.com/channel/UCYzOzEcUnavZxsSlzboDbHw,UCYzOzEcUnavZxsSlzboDbHw,Robin Green,Ugx7daXD19QyNdk9yKh4AaABAg.8kz_VcAxDQH8kz_Y5wE3zk,0,1536509783.0,"full error: 

Traceback (most recent call last):
  File ""cms_scrape.py"", line 12, in <module>
    csv.writer.writerow(['headline', 'summary', 'yt_link'])
AttributeError: 'builtin_function_or_method' object has no attribute 'writerow'",none,Ugx7daXD19QyNdk9yKh4AaABAg,2020-07-07 23:38:32.697611,
693,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgzevaYguGG-O-Hh9Et4AaABAg.8kR3mRc4V8R8kSqR0OZ0kT,0,1535377787.0,You can use the request library to loop through different pages and scrape them one at a time,none,UgzevaYguGG-O-Hh9Et4AaABAg,2020-07-07 23:38:33.310373,
694,,http://www.youtube.com/channel/UCBzYI8oWCjeevic7G9NhmxA,UCBzYI8oWCjeevic7G9NhmxA,ablg 13,UgwLe0fsCg4UZMRTBfF4AaABAg.8jgDwgiTvCc8jhc03dMgtN,0,1533759614.0,Corey Schafer Thanks for that advice. Regards.,none,UgwLe0fsCg4UZMRTBfF4AaABAg,2020-07-07 23:38:33.524172,
695,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwLe0fsCg4UZMRTBfF4AaABAg.8jgDwgiTvCc8jgpAUymT9t,1,1533732961.0,"It depends on what you're doing. Scraping the data is really no different than downloading the source code from a browser and processing it; however, you can't simply use that data for your own uses or profit. Also, some websites don't like you scraping their data and instead provide APIs to get this data in a more efficient format (such as raw JSON). So be sure to read a website's terms to see if they have an API... otherwise they may try to block direct scraping attempts.",none,UgwLe0fsCg4UZMRTBfF4AaABAg,2020-07-07 23:38:33.524172,
696,,http://www.youtube.com/channel/UC0XpWllyVzLSJ-k0UE3QXSg,UC0XpWllyVzLSJ-k0UE3QXSg,Jan Struyf,UgyM1HJHbmiaUwHjLYJ4AaABAg.8i_iB6C5RVL8kXTqnZIFkT,0,1535533196.0,"That explanation is really top. So detailed, so clear, also your other video's top. It helps me to solve problems, which I could not otherwise. Also some basic Python __main__ , __init__ self, and classes etc info are so informative.
I suddenly had errors on ""self."", what the heck is going on. I wrote a lot of code but never had issues with ""self"". But after your videos on ""classes"" made it clear what the heck I was doing and doing wrong.",none,UgyM1HJHbmiaUwHjLYJ4AaABAg,2020-07-07 23:38:33.806984,
697,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgyM1HJHbmiaUwHjLYJ4AaABAg.8i_iB6C5RVL8iaCp61GoCy,0,1531363520.0,Thanks! That's awesome you're getting started so early. Glad to hear you found the videos helpful!,none,UgyM1HJHbmiaUwHjLYJ4AaABAg,2020-07-07 23:38:33.806984,
698,,http://www.youtube.com/channel/UCx5H9tIYNh5u_zWSrl8SOQw,UCx5H9tIYNh5u_zWSrl8SOQw,krishna narwani,UgyhBGIJbr1qIIMnaz94AaABAg.8iMwse70eyK8iNAExE27We,0,1530892405.0,well I'm done now I used html5lib well video was really good i learned a lot from this video,none,UgyhBGIJbr1qIIMnaz94AaABAg,2020-07-07 23:38:34.062810,
699,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxhlG-8ot1ulKnU1Qd4AaABAg.8hoEi7Bfl9h8hqUHmvbd1D,0,1529762063.0,It sounds like an issue with your Sublime Text Build System. I have a video on those build systems that might clear it up. If you search on my channel you will find it.,none,UgxhlG-8ot1ulKnU1Qd4AaABAg,2020-07-07 23:38:34.287813,
700,,http://www.youtube.com/channel/UCkbaU_KRCvEJfSEjD4Twozg,UCkbaU_KRCvEJfSEjD4Twozg,Mateus Ribeiro,Ugyr0FPhKDNuWbXKaH94AaABAg.8h7YYjrwrbM997-ELzrSlq,2,1590479304.0,"I actually enjoy much more watching his videos than watching normal movies, series...",none,Ugyr0FPhKDNuWbXKaH94AaABAg,2020-07-07 23:38:34.644444,
701,,http://www.youtube.com/channel/UCjkBPRBdA44PsIqDaq6fA7Q,UCjkBPRBdA44PsIqDaq6fA7Q,Paulo Esperon,Ugyr0FPhKDNuWbXKaH94AaABAg.8h7YYjrwrbM96rIcKRV_Yk,0,1585624077.0,I felt the same.,none,Ugyr0FPhKDNuWbXKaH94AaABAg,2020-07-07 23:38:34.644444,
702,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugyr0FPhKDNuWbXKaH94AaABAg.8h7YYjrwrbM8hIiUtTofLP,11,1528595630.0,Thanks!,none,Ugyr0FPhKDNuWbXKaH94AaABAg,2020-07-07 23:38:34.644444,
703,,http://www.youtube.com/channel/UCXDdfxaX74-1dvRDVEYFnWA,UCXDdfxaX74-1dvRDVEYFnWA,Father Hotdog,Ugy_xo6N4oZVE5eQwbR4AaABAg.8gojTyOGYK88hMAzexQg77,0,1528711758.0,Thank you so much Cory.  That really clears it up for me and thank you so much for all your hard work and awesome videos,none,Ugy_xo6N4oZVE5eQwbR4AaABAg,2020-07-07 23:38:34.914019,
704,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugy_xo6N4oZVE5eQwbR4AaABAg.8gojTyOGYK88hIUccy_Wmk,1,1528587837.0,"Copying and pasting would take far too much time if you needed to scrape a page on a regular basis. Its much easier to script this so that you can get the information you need on a consistent basis. And yes, some people use scraping in that way, but there are many uses of scraping besides what you listed above.",none,Ugy_xo6N4oZVE5eQwbR4AaABAg,2020-07-07 23:38:34.915016,
705,,http://www.youtube.com/channel/UClUJcyjCUsLo41CAxQo4rEw,UClUJcyjCUsLo41CAxQo4rEw,San Samman,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z796zw2yT6Nbe,0,1585913711.0,@Slobodan Tajisic LMAO,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255146,
706,,http://www.youtube.com/channel/UCNwYZHC3GmM0Xooy7cXWFfQ,UCNwYZHC3GmM0Xooy7cXWFfQ,Slobodan Tajisic,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z796zso4j6tl_,2,1585912008.0,@San Samman No way. They mess with some viruses.,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255146,
707,,http://www.youtube.com/channel/UClUJcyjCUsLo41CAxQo4rEw,UClUJcyjCUsLo41CAxQo4rEw,San Samman,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z795v1gCwqxCQ,1,1583601930.0,did you become a programer?,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255582,
708,,http://www.youtube.com/channel/UCVmLTu2l1mZsS6DJVAXkanw,UCVmLTu2l1mZsS6DJVAXkanw,collins dakurah,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z795WyaUjWogE,0,1582760926.0,@abby tried adding you but couldnt find your id this is mine {Kyefondeme},none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255582,
709,,http://www.youtube.com/channel/UCUxXHZV6u8K7uoTsQE3p9YQ,UCUxXHZV6u8K7uoTsQE3p9YQ,abby,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z795WdQhX-PX4,0,1582749827.0,@collins dakurahmy WeChat Id is bhisto33,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255582,
710,,http://www.youtube.com/channel/UCVmLTu2l1mZsS6DJVAXkanw,UCVmLTu2l1mZsS6DJVAXkanw,collins dakurah,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z795WCmDehBVh,0,1582735332.0,abby yeah. Am in Beijing tho.,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255582,
711,,http://www.youtube.com/channel/UCUxXHZV6u8K7uoTsQE3p9YQ,UCUxXHZV6u8K7uoTsQE3p9YQ,abby,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z795TbleadOPr,0,1582648295.0,"@collins dakurah hahaha broah lets link up,",none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255582,
712,,http://www.youtube.com/channel/UCVmLTu2l1mZsS6DJVAXkanw,UCVmLTu2l1mZsS6DJVAXkanw,collins dakurah,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z795Rko52gnxU,0,1582585925.0,@abby i think we in China knowledge seeker lol,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.255582,
713,,http://www.youtube.com/channel/UCUxXHZV6u8K7uoTsQE3p9YQ,UCUxXHZV6u8K7uoTsQE3p9YQ,abby,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z78tz8gC_cgGN,2,1555822531.0,"hey Einsteiincy , are you still using this youtube acc, im also from China,  would like some help with python",none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.256142,
714,,http://www.youtube.com/channel/UCUxXHZV6u8K7uoTsQE3p9YQ,UCUxXHZV6u8K7uoTsQE3p9YQ,abby,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z78tz8Mm1RM88,1,1555822363.0,@Corey Schafer hey you still using this acc for youtube?,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.256142,
715,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwtbFbxcMblKej5ln94AaABAg.8gf08LhR4Z78gglwiGsAaS,34,1527288816.0,Thanks!,none,UgwtbFbxcMblKej5ln94AaABAg,2020-07-07 23:38:35.256142,
716,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgyLMugP-7Sd7FepJ_x4AaABAg.8gT0pdx5jnP8gUdBxR2zdg,0,1526848023.0,Requests is what gets the data from the website and BeautifulSoup is the HTML parser,none,UgyLMugP-7Sd7FepJ_x4AaABAg,2020-07-07 23:38:35.477339,
717,,http://www.youtube.com/channel/UCItod6gn-jEDThkdktYeizQ,UCItod6gn-jEDThkdktYeizQ,Elvis Bicharri,UgzrT6mqaPRVcrWrGnl4AaABAg.8g9-XpkxPa_8g9Ufz1f4JI,0,1526138391.0,Thanks Corey that makes sense. Another question. Is it possible to use XPATH instead of the find() and find_all() methods to grab specific elements on a page? I am more conversant with XPATH and was wondering about that.,none,UgzrT6mqaPRVcrWrGnl4AaABAg,2020-07-07 23:38:35.709236,
718,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgzrT6mqaPRVcrWrGnl4AaABAg.8g9-XpkxPa_8g9CXzX6U2-,0,1526128880.0,"You could use the requests library to loop through those pages. It would really depend on the website and how they link their pages, but for my site you could add a loop that does a request to each of those pages and parses the information",none,UgzrT6mqaPRVcrWrGnl4AaABAg,2020-07-07 23:38:35.709236,
719,,http://www.youtube.com/channel/UChHq04qPaKvsSEOA8axa_nQ,UChHq04qPaKvsSEOA8axa_nQ,525gigidy,UgzBdDiqBsErcUkyV754AaABAg.8efE4iWk2cR8efNMKMW3qo,0,1522946881.0,I tried putting the code of what you did but edited straight into the python command shell and its popping up a error: SyntaxError: multiple statements found while compiling a single statement   right after beautiful soup on line 1,none,UgzBdDiqBsErcUkyV754AaABAg,2020-07-07 23:38:35.926010,
720,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgzBdDiqBsErcUkyV754AaABAg.8efE4iWk2cR8efIA5wyniS,0,1522944159.0,That is Sublime Text. It is a different program. That is the editor I use to program Python. I have a video on my channel how I set that editor up.,none,UgzBdDiqBsErcUkyV754AaABAg,2020-07-07 23:38:35.926010,
721,,http://www.youtube.com/channel/UCJ8feJZNtpPpEZiS4C-Dx_A,UCJ8feJZNtpPpEZiS4C-Dx_A,LIAM ENEUK,UgwRh2gQdEAXx8f0bMR4AaABAg.8eSu_zjHV4S8eT81jOk6sI,0,1522502640.0,"Thank you for the insight. I am a newbie to programming and am picking one as my first language. The only ""programming"" that I had experience of is PC LOGO (Donno if there is still anyone know what it is) and so I am picking a language by functions.",none,UgwRh2gQdEAXx8f0bMR4AaABAg,2020-07-07 23:38:36.170286,
722,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwRh2gQdEAXx8f0bMR4AaABAg.8eSu_zjHV4S8eT7adrzPSB,0,1522502410.0,"Sure. If you had a location file or database of a static website, then you could use a technique like this to compare previous output to new output. There may be certain parts of the page constantly changing (like advertisements) but you could parse those out so that you're only comparing the content you want.",none,UgwRh2gQdEAXx8f0bMR4AaABAg,2020-07-07 23:38:36.170286,
723,,http://www.youtube.com/channel/UC0XpWllyVzLSJ-k0UE3QXSg,UC0XpWllyVzLSJ-k0UE3QXSg,Jan Struyf,Ugw7TJ0iAFeIbDC_bDZ4AaABAg.8dOyuePoKVj8kXSjqeZEI_,0,1535532615.0,Vorst,none,Ugw7TJ0iAFeIbDC_bDZ4AaABAg,2020-07-07 23:38:36.417472,
724,,http://www.youtube.com/channel/UCu36Lw5rJktG1WKMV2gDSJw,UCu36Lw5rJktG1WKMV2gDSJw,Henri Cattoire,UgyeuoIO_sXdSBHtMMl4AaABAg.8dHCbKZwokB8gVbyFBGxM5,0,1526880932.0,"You need to install it using pip install bs4 or if you're using python 3.6, you should argue pip3 install bs4",none,UgyeuoIO_sXdSBHtMMl4AaABAg,2020-07-07 23:38:36.629138,
725,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgzpR3wsCP3UdEomafp4AaABAg.8czKfvL9ogF8cznTNABZ_M,0,1519337215.0,You can use the format method to fill in placeholders. I have a video on string formatting on my channel if you have never used that method before.,none,UgzpR3wsCP3UdEomafp4AaABAg,2020-07-07 23:38:36.890457,
726,,http://www.youtube.com/channel/UC1Z8jn3UzqzL3rLUOvXmH-A,UC1Z8jn3UzqzL3rLUOvXmH-A,Arjun Gaihre,UgzpR3wsCP3UdEomafp4AaABAg.8czKfvL9ogF8czeLqvR7jf,0,1519332435.0,"I am using Python 3.5.2, so what would be the solution for this version?",none,UgzpR3wsCP3UdEomafp4AaABAg,2020-07-07 23:38:36.890457,
727,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgzpR3wsCP3UdEomafp4AaABAg.8czKfvL9ogF8czcy_VzMoY,0,1519331712.0,"Are you using Python 3.6 or higher? If so, are you sure you have the “f” before the string. It needs that there in order to know to format the string.",none,UgzpR3wsCP3UdEomafp4AaABAg,2020-07-07 23:38:36.890457,
728,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugwc-UOeblMT1D0a0r94AaABAg.8cubuGFvZ2H8cunaBPXdi0,0,1519169507.0,"Your Atom might not be using the Python interpreter you think it is. If you run these lines:
import sys
print(sys.executable)

Then it will show you the Python interpreter that is running your code. That is where you need to install BeautifulSoup.",none,Ugwc-UOeblMT1D0a0r94AaABAg,2020-07-07 23:38:37.118106,
729,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugw7Y1K57AKU_c5-UuN4AaABAg.8ceDAKVV0Qz8ceaUBF2JQs,5,1518625763.0,Thanks. I'll be doing videos on all of those topics in the near future. At the beginning of March I will be in a position to commit a LOT more time to these videos and pump them out more quickly. Stay tuned.,none,Ugw7Y1K57AKU_c5-UuN4AaABAg,2020-07-07 23:38:37.334985,
730,,http://www.youtube.com/channel/UCcHfS4twn-ZI-aKwLfX9o-Q,UCcHfS4twn-ZI-aKwLfX9o-Q,Rift,UgwHvNBHPfg6_ul9QUl4AaABAg.8c_KARmebu799XCapJNOLh,1,1591358727.0,@j w ye ok i was just confused. lmao,none,UgwHvNBHPfg6_ul9QUl4AaABAg,2020-07-07 23:38:37.592086,
731,,http://www.youtube.com/channel/UCg4icDmUWgc8k8oDU8KFjNQ,UCg4icDmUWgc8k8oDU8KFjNQ,j w,UgwHvNBHPfg6_ul9QUl4AaABAg.8c_KARmebu799X9dhrYeNx,1,1591357178.0,"@Rift was trying to be funny, but jesss i failed hard.",none,UgwHvNBHPfg6_ul9QUl4AaABAg,2020-07-07 23:38:37.593053,
732,,http://www.youtube.com/channel/UCcHfS4twn-ZI-aKwLfX9o-Q,UCcHfS4twn-ZI-aKwLfX9o-Q,Rift,UgwHvNBHPfg6_ul9QUl4AaABAg.8c_KARmebu799X7_FmjILw,0,1591356093.0,@j w what the heck,none,UgwHvNBHPfg6_ul9QUl4AaABAg,2020-07-07 23:38:37.593053,
733,,http://www.youtube.com/channel/UCg4icDmUWgc8k8oDU8KFjNQ,UCg4icDmUWgc8k8oDU8KFjNQ,j w,UgwHvNBHPfg6_ul9QUl4AaABAg.8c_KARmebu794Vbpt441lO,0,1580567955.0,pls no charge. me no have money,none,UgwHvNBHPfg6_ul9QUl4AaABAg,2020-07-07 23:38:37.593053,
734,,http://www.youtube.com/channel/UC6aBFgrdW1BdieMYkXZOMAQ,UC6aBFgrdW1BdieMYkXZOMAQ,Cosmic Dark Matter,Ugx11DEFSRSXtrynNkN4AaABAg.8cYrB0DNOK28tZ6dH3XM-u,3,1554915488.0,@Failer Great...Agreed. Corey Schafer is very good at explaining things. A lot of professors can't do that....,none,Ugx11DEFSRSXtrynNkN4AaABAg,2020-07-07 23:38:37.834227,
735,,http://www.youtube.com/channel/UC_APJPbfWEzF4a3h9Ye7ncQ,UC_APJPbfWEzF4a3h9Ye7ncQ,DonVTOL,Ugy8wwq0AhuSgTuXsYt4AaABAg.8cNGyQ40Pav8cNHShsN0d4,0,1518011286.0,Using Sublime Text 3 and Python 3.6,none,Ugy8wwq0AhuSgTuXsYt4AaABAg,2020-07-07 23:38:38.074509,
736,,http://www.youtube.com/channel/UC0fMVguVuhkszYLugc61Zxg,UC0fMVguVuhkszYLugc61Zxg,morgengabe1,Ugx_6J_sPbYhDSSnCQB4AaABAg.8c66HbiCM1S8wcxZeU1dtD,0,1561520431.0,"Hey, I solved it (with help from stackoverflow).
My script had two problems. The first was that I hadn't changed the ""find()"" to ""find_all()"" in the for loop.
The second problem was that not all instances of article, had the  tags we were looking for (ie 'h2',  'a', 'p', etc.) so the returned result would be ""None"" and we cannot check None for content because there is nothing.
The solution was to include an if statement such as the following:


if article.find('div', class_='entry-content') is not None:
     blah
     blah
     blah




That should do it! Pulled up 10 (headline,summary,link) for me

feel free to let me know if that doesn't work",none,Ugx_6J_sPbYhDSSnCQB4AaABAg,2020-07-07 23:38:38.291192,
737,,http://www.youtube.com/channel/UC0fMVguVuhkszYLugc61Zxg,UC0fMVguVuhkszYLugc61Zxg,morgengabe1,Ugx_6J_sPbYhDSSnCQB4AaABAg.8c66HbiCM1S8wcsPSpgmgE,0,1561517726.0,"Please, do let me know if you were able to solve the problem! Struggling myself right now. (I was about twenty minutes too early, just got up to the right part of the video and I'm baffled now lol)

So the problem is that sub-object p of the of the div object  has no type. I couldn't tell you why that's the case though, but if you remove "".text"" you should get enough of what you're looking for (assuming there is a <div><p></p></div> somewhere in the page).",none,Ugx_6J_sPbYhDSSnCQB4AaABAg,2020-07-07 23:38:38.291192,
738,,http://www.youtube.com/channel/UCyADaqNwCXISkdwxbl2xfIw,UCyADaqNwCXISkdwxbl2xfIw,Dylan Duregger,UgymspOfhH9V8Usyzz94AaABAg.8bpMjAGc48k8bt3ZCOsZP8,0,1516963811.0,"Sorry to bother you again, but what would that look like? if a variable data is = to soup.td would it be data[1]?",none,UgymspOfhH9V8Usyzz94AaABAg,2020-07-07 23:38:38.508335,
739,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgymspOfhH9V8Usyzz94AaABAg.8bpMjAGc48k8bqouM1EQyd,0,1516888495.0,You could do a findall and then use the index of the value you want.,none,UgymspOfhH9V8Usyzz94AaABAg,2020-07-07 23:38:38.508335,
740,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgwM4t8uOm55e4Qvm7x4AaABAg.8boIKSnUULI8boMdNAHDaY,0,1516806043.0,"Yes, I am trying to work on a series of web framework tutorials right now. It is taking some time since it is a larger subject, but I hope to have those out around March timeframe? There will be other tutorials released before these as well.",none,UgwM4t8uOm55e4Qvm7x4AaABAg,2020-07-07 23:38:38.721468,
741,,http://www.youtube.com/channel/UC0S7i0W_tqeNQOQTseOLN5g,UC0S7i0W_tqeNQOQTseOLN5g,李永胜,Ugz_XOW0TojQAEgzh2J4AaABAg.8bgXsCjgXtl8epBemSSkyS,0,1523276293.0,"seems your installation was not correctly. sometimes, it may be that you run 'pip install requests', but your python is running in python3, so you may need run 'pip3 install requests'",none,Ugz_XOW0TojQAEgzh2J4AaABAg,2020-07-07 23:38:38.939138,
742,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugz_XOW0TojQAEgzh2J4AaABAg.8bgXsCjgXtl8bgcyWbNRa0,0,1516546694.0,"If you know you have requests installed and it's still not working, then you likely have an issue with your Build System. If you watch my video on Sublime Text Build Systems then it should answer any questions you have on getting it properly set up.",none,Ugz_XOW0TojQAEgzh2J4AaABAg,2020-07-07 23:38:38.939172,
743,,http://www.youtube.com/channel/UCfKuWWfdQ5Cq0J3EOw3Vcyw,UCfKuWWfdQ5Cq0J3EOw3Vcyw,Daniel Weikert,Ugz-ReyxMXMwjWIwcKJ4AaABAg.8bKVyz27SBi8bMBbIu2IKD,0,1515827180.0,Highly appreciated. Thank you Corey,none,Ugz-ReyxMXMwjWIwcKJ4AaABAg,2020-07-07 23:38:39.169997,
744,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugz-ReyxMXMwjWIwcKJ4AaABAg.8bKVyz27SBi8bKgCn_eG1A,3,1515776640.0,"Yes, I plan on doing a video on dynamic websites sometime in the near future.",none,Ugz-ReyxMXMwjWIwcKJ4AaABAg,2020-07-07 23:38:39.169997,
745,,http://www.youtube.com/channel/UC0S7i0W_tqeNQOQTseOLN5g,UC0S7i0W_tqeNQOQTseOLN5g,李永胜,UgzdN-xgjvavm0CwiLF4AaABAg.8b0RwcBjwQn8epC3DFtmSL,0,1523276501.0,"you may need first creat a csv writer object, then apply writerow  or writerows method to it(for list like this [['name','age'],['john',28]])",none,UgzdN-xgjvavm0CwiLF4AaABAg,2020-07-07 23:38:39.486450,
746,,http://www.youtube.com/channel/UCy8NqCaUypxSqm_VAZXTXEQ,UCy8NqCaUypxSqm_VAZXTXEQ,John Wood,UgwNS9094qN5T2_jV6x4AaABAg.8aWmyBWE1CH8aWoy_oGcOB,0,1514036403.0,"http://toddhayton.com/2015/03/11/scraping-ajax-pages-with-python/

I think will help with what I'm looking for. As always, thank you for the great videos!",none,UgwNS9094qN5T2_jV6x4AaABAg,2020-07-07 23:38:39.719308,
747,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugx-BN4noQdEwJu1V6h4AaABAg.8a-eODpRYZI8a1xghkxLov,0,1513000788.0,"Hey there. If you watch my latest video on working with JSON data, I show an example of pulling down JSON from a Yahoo API and using that in a Python script. And yes, sometimes a time delay works fine but sometimes scripts are blocked based on their header information. You can customize the header information to make it look like your script is coming from a browser instead.",none,Ugx-BN4noQdEwJu1V6h4AaABAg,2020-07-07 23:38:39.935669,
748,,http://www.youtube.com/channel/UCsfvLpnvn3S7hZ3ZuuUTOCA,UCsfvLpnvn3S7hZ3ZuuUTOCA,Martin Kaspar,Ugwv53wnoIQjBpjiOTJ4AaABAg.8_t0uXdgoK38a4golpwiIg,0,1513092604.0,many thanks !!!!,none,Ugwv53wnoIQjBpjiOTJ4AaABAg,2020-07-07 23:38:40.159422,
749,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugwv53wnoIQjBpjiOTJ4AaABAg.8_t0uXdgoK38_tR72Yw9ch,1,1512681196.0,"The code snippets from my videos are mostly here:
https://github.com/CoreyMSchafer/code_snippets",none,Ugwv53wnoIQjBpjiOTJ4AaABAg,2020-07-07 23:38:40.159511,
750,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgyIMuwrEnOf3iz-u7F4AaABAg.8_nuUr73jnS8_oMhtm5F6i,0,1512511113.0,"You can use "".encode('utf-8')"" on the text, or you can set your Sublime Text encoding to UTF-8. There are instructions here:
https://stackoverflow.com/questions/39576308/printing-utf-8-in-python-3-using-sublime-text-3",none,UgyIMuwrEnOf3iz-u7F4AaABAg,2020-07-07 23:38:40.395783,
751,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugw_0-IUEhvZlMwT6sx4AaABAg.8_RFjELSAe38_Rqum7jmjM,0,1511722165.0,You could keep a result count and break out of the loop whenever you want. Or you could use list slicing to only loop through the first 10. If you watch my video on list slicing then I show in detail how to do this. Hope that helps.,none,Ugw_0-IUEhvZlMwT6sx4AaABAg,2020-07-07 23:38:40.669695,
752,,http://www.youtube.com/channel/UCpN0scytHvH8nLPtmP3kWJA,UCpN0scytHvH8nLPtmP3kWJA,Tom Travolta,Ugzr160U4n5nf7fA10l4AaABAg.8_R9SHnPNF08_Sf2vo-Nos,0,1511749503.0,Fixed it! Thanks Arif! :D,none,Ugzr160U4n5nf7fA10l4AaABAg,2020-07-07 23:38:40.884142,
753,,http://www.youtube.com/channel/UCJ-FcPUQk7dR2-yCPdgCXfQ,UCJ-FcPUQk7dR2-yCPdgCXfQ,Arif A,Ugzr160U4n5nf7fA10l4AaABAg.8_R9SHnPNF08_RvMn1UVAy,1,1511724500.0,"I had the same error. I was able to resolve this with the help of Google, by inserting 

encoding='utf-8')

right after the comma next to 'w' in csv_file = open",none,Ugzr160U4n5nf7fA10l4AaABAg,2020-07-07 23:38:40.884142,
754,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxTa5vdjsqHbjtA8LZ4AaABAg.8Zy85MAiRWk8ZyR0HRLoIc,1,1510701429.0,"I plan on doing a video on dynamic webpages in the near future. I wouldn't use BeautifulSoup for this. I believe you can do this with a headless browser, depending on the content you're looking for. If you Google for Python and dynamic content then you will see many different options. I'm still not sure exactly which technology I will use. Will likely do multiple videos covering several.",none,UgxTa5vdjsqHbjtA8LZ4AaABAg,2020-07-07 23:38:41.132255,
755,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgxRz8kpXESMd9HzlA54AaABAg.8ZnGad4u6Bv8ZnPgHRTLaP,0,1510331634.0,"I haven't done much work with embedded videos outside of YouTube's, so I'm not entirely sure. This is how I've always worked with YouTube's embedded videos though. Sorry I can't give a more clear answer.",none,UgxRz8kpXESMd9HzlA54AaABAg,2020-07-07 23:38:41.370138,
756,,http://www.youtube.com/channel/UCtu6Sw_UNlZtmwDdHKu-kjA,UCtu6Sw_UNlZtmwDdHKu-kjA,Caleb Njiiri,UgyNMMyLXDpiLJ1xEg94AaABAg.8ZmvHv6xkMM8Zn8zXm8jZV,0,1510322879.0,I’ll definitely keep an eye out for that. Thanks,none,UgyNMMyLXDpiLJ1xEg94AaABAg,2020-07-07 23:38:41.602109,
757,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgyNMMyLXDpiLJ1xEg94AaABAg.8ZmvHv6xkMM8Zmy2h31VXg,1,1510316622.0,"Long story short, you can look into frameworks like Selenium or Mechanize to navigate through the browser and then scrape pages you'd like. This also goes for scraping dynamic content.",none,UgyNMMyLXDpiLJ1xEg94AaABAg,2020-07-07 23:38:41.602506,
758,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,UgyNMMyLXDpiLJ1xEg94AaABAg.8ZmvHv6xkMM8ZmxxlmBX4U,1,1510316573.0,It really depends on what kind of login. I will be doing some more advanced scraping videos in the future where I go over different strategies for situations like this. It's a bit too long of an answer for a single comment.,none,UgyNMMyLXDpiLJ1xEg94AaABAg,2020-07-07 23:38:41.602506,
759,,http://www.youtube.com/channel/UCluxCRUWnOmPc2ezWSmGNJQ,UCluxCRUWnOmPc2ezWSmGNJQ,Jeff Hunter,UgwoUFe7bhUbFD5_VBh4AaABAg.8Zm_q6a_XZY8q4X--FAbff,0,1547446668.0,"sure heres your tutorial 

import threading 
shit = threading.Thread(target=shittingFunc) 
shit.start()",none,UgwoUFe7bhUbFD5_VBh4AaABAg,2020-07-07 23:38:41.913426,
760,,http://www.youtube.com/channel/UCmLqtVM4D43tVkr6Dm7D4nA,UCmLqtVM4D43tVkr6Dm7D4nA,War of Wars,UgwoUFe7bhUbFD5_VBh4AaABAg.8Zm_q6a_XZY8jEZZ7xG4iz,0,1532751171.0,did u have nipple alert app installed,none,UgwoUFe7bhUbFD5_VBh4AaABAg,2020-07-07 23:38:41.913426,
761,,http://www.youtube.com/channel/UChbx568g-nB2Bkaj0Lzc-ZA,UChbx568g-nB2Bkaj0Lzc-ZA,Mojo Tech,UgwoUFe7bhUbFD5_VBh4AaABAg.8Zm_q6a_XZY8hY2KmjL8lL,0,1529109873.0,Venkatesh K great suggestion,none,UgwoUFe7bhUbFD5_VBh4AaABAg,2020-07-07 23:38:41.913426,
762,,http://www.youtube.com/channel/UCbSkFCcFIHVxvNqXzdEpvuQ,UCbSkFCcFIHVxvNqXzdEpvuQ,Creampie,UgwoUFe7bhUbFD5_VBh4AaABAg.8Zm_q6a_XZY8aqWGOG-0F3,2,1514730715.0,Venkatesh K this gets my nipples hard^,none,UgwoUFe7bhUbFD5_VBh4AaABAg,2020-07-07 23:38:41.913426,
763,,http://www.youtube.com/channel/UCAroV9vpw1jLA3D3xQAkq7Q,UCAroV9vpw1jLA3D3xQAkq7Q,Ali Raxa,Ugzhg8HEQIx0h1RYLxJ4AaABAg.8ZiBbt6AB5i8ZiEPhzI3ZK,2,1510157951.0,"hahahahah I like your reply, especially this line: ""But if someone wanted to do that then likely they wouldn't have needed my tutorial to do so to begin with.""",none,Ugzhg8HEQIx0h1RYLxJ4AaABAg,2020-07-07 23:38:42.174227,
764,,http://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g,UCCezIgC97PvUuR4_gbFUs5g,Corey Schafer,Ugzhg8HEQIx0h1RYLxJ4AaABAg.8ZiBbt6AB5i8ZiCY2zA2pq,14,1510156971.0,"Haha... possibly. Hopefully not too much. A few people practicing here and there won't hurt, but hopefully no nefarious scripts come of this. But if someone wanted to do that then likely they wouldn't have needed my tutorial to do so to begin with.",none,Ugzhg8HEQIx0h1RYLxJ4AaABAg,2020-07-07 23:38:42.174227,
